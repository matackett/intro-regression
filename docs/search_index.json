[
["index.html", "Intro Regression Welcome to Intro Regression!", " Intro Regression Maria Tackett Latest update: 2020-08-23 Welcome to Intro Regression! The content in this book was originally developed for STA 210: Regression Analysis at Duke University.The computing aspects of the assignments are written using the tidyverse syntax in R; however, the assignments can be adapted to fit the computing language of your choice. All of the files are available in the Intro Regression GitHub repo. This book is under development and will be periodically upated with new material. Please email me (maria.tackett@duke.edu) if you have any questions, feedback, or suggestions. I would also love to hear about your experience if you use any of the content in your course. License This work is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. "],
["getstarted.html", "Chapter 1 Getting Started 1.1 How to use this book 1.2 Review: Intro Statistics and R", " Chapter 1 Getting Started 1.1 How to use this book Each chapter of this book is a topic that may be covered in an intermediate-level regression analysis course. The topics are arranged based on the way they were taught in STA 210: Regression Analysis; however, the assignments do not have to be used in the order they are presented. Feel free to use the text and adapt it to fit the needs of your course. Each chapter includes several sections of assignments and supplmental notes about the mathematical details. Each section begins with one of the codes below to help you determine the type of assignment or note in that section: COMP: These assignments focus on the computing skills needed to conduct regression analysis. They were originally designed to be completed in groups in a weekly lab/discussion session; however, they can be also be used for homework assignments or in-class work days. Because the emphasis is computing, they include a lot of step-by-step instruction. IN-CLASS: Assignments to be completed as short in-class activities. Most of the code is already written, so students mostly run the code and interpret the output. Students may also need to fill in short lines of code. HW: Focus on putting together conceptual knowledge and computing skills. Most homework assingments include two parts: (1) Concepts &amp; Computations - guided short-answer exercises that focus on conceptual knowledge and short computational tasks, (2) Data Analysis - open-ended question where students perform a complete regression analysis and write results as a narrative. NOTES: Supplemental notes providing more mathematical details. To fully understand the notes, the reader should be familiar with basic concepts in linear algebra. 1.2 Review: Intro Statistics and R The primary audience for this text is students who have completed an introductory statistics course. It is assumed that students are familiar with the concept of statitical inference. This text is also written assuming students have had some exposure to R and the tidyverse syntax. (There is one “Intro to R” assingment included; however, this assignment is not a comprehensive introduction to R.) The following are suggested texts to review statistical concepts and computing: OpenIntro Statistics Modern Dive R for Data Science "],
["slr.html", "Chapter 2 Simple Linear Regression 2.1 Getting started 2.2 Foundation 2.3 Inference 2.4 Prediction 2.5 Checking conditions 2.6 Partioning variability 2.7 Derivation for slope and intercept", " Chapter 2 Simple Linear Regression 2.1 Getting started Putting text here 2.2 Foundation Putting text here 2.3 Inference Putting text here 2.4 Prediction 2.5 Checking conditions 2.6 Partioning variability 2.7 Derivation for slope and intercept This document contains the mathematical details for deriving the least-squares estimates for slope (\\(\\beta_1\\)) and intercept (\\(\\beta_0\\)). We obtain the estimates, \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_0\\) by finding the values that minimize the sum of squared residuals (). \\[\\begin{equation} \\label{ssr} SSR = \\sum\\limits_{i=1}^{n}[y_i - \\hat{y}_i]^2 = [y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i)]^2 = [y_i - (\\hat{\\beta}_0 - \\hat{\\beta}_1 x_i]^2 \\end{equation}\\] Recall that we can find the values of \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_0\\) that minimize () by taking the partial derivatives of () and setting them to 0. Thus, the values of \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_0\\) that minimize the respective partial derivative also minimize the sum of squared residuals. The partial derivatives are \\[\\begin{equation} \\label{par-deriv} \\begin{aligned} &amp;\\frac{\\partial \\text{SSR}}{\\partial \\hat{\\beta}_1} = -2 \\sum\\limits_{i=1}^{n}x_i(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i) \\\\[10pt] &amp;\\frac{\\partial \\text{SSR}}{\\partial \\hat{\\beta}_0} = -2 \\sum\\limits_{i=1}^{n}(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i) \\end{aligned} \\end{equation}\\] Let’s begin by deriving \\(\\hat{\\beta}_0\\). \\[\\begin{equation} \\label{est-beta0} \\begin{aligned} \\frac{\\partial \\text{SSR}}{\\partial \\hat{\\beta}_0} &amp;= -2 \\sum\\limits_{i=1}^{n}(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i) = 0 \\\\[10pt] &amp;\\Rightarrow -\\sum\\limits_{i=1}^{n}(y_i + \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i) = 0 \\\\[10pt] &amp;\\Rightarrow - \\sum\\limits_{i=1}^{n}y_i + n\\hat{\\beta}_0 + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i = 0 \\\\[10pt] &amp;\\Rightarrow n\\hat{\\beta}_0 = \\sum\\limits_{i=1}^{n}y_i - \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i \\\\[10pt] &amp;\\Rightarrow \\hat{\\beta}_0 = \\frac{1}{n}\\Big(\\sum\\limits_{i=1}^{n}y_i - \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i\\Big)\\\\[10pt] &amp;\\Rightarrow \\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x} \\\\[10pt] \\end{aligned} \\end{equation}\\] Now, we can derive \\(\\hat{\\beta}_1\\) using the \\(\\hat{\\beta}_0\\) we just derived \\[\\begin{equation} \\label{est-beta1-pt1} \\begin{aligned} &amp;\\frac{\\partial \\text{SSR}}{\\partial \\hat{\\beta}_1} = -2 \\sum\\limits_{i=1}^{n}x_i(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i) = 0 \\\\[10pt] &amp;\\Rightarrow -\\sum\\limits_{i=1}^{n}x_iy_i + \\hat{\\beta}_0\\sum\\limits_{i=1}^{n}x_i + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 = 0 \\\\[10pt] \\text{(Fill in }\\hat{\\beta}_0\\text{)}&amp;\\Rightarrow -\\sum\\limits_{i=1}^{n}x_iy_i + (\\bar{y} - \\hat{\\beta}_1\\bar{x})\\sum\\limits_{i=1}^{n}x_i + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 = 0 \\\\[10pt] &amp;\\Rightarrow (\\bar{y} - \\hat{\\beta}_1\\bar{x})\\sum\\limits_{i=1}^{n}x_i + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 = \\sum\\limits_{i=1}^{n}x_iy_i \\\\[10pt] &amp;\\Rightarrow \\bar{y}\\sum\\limits_{i=1}^{n}x_i - \\hat{\\beta}_1\\bar{x}\\sum\\limits_{i=1}^{n}x_i + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 = \\sum\\limits_{i=1}^{n}x_iy_i \\\\[10pt] &amp;\\Rightarrow n\\bar{y}\\bar{x} - \\hat{\\beta}_1n\\bar{x}^2 + \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 = \\sum\\limits_{i=1}^{n}x_iy_i \\\\[10pt] &amp;\\Rightarrow \\hat{\\beta}_1\\sum\\limits_{i=1}^{n}x_i^2 - \\hat{\\beta}_1n\\bar{x}^2 = \\sum\\limits_{i=1}^{n}x_iy_i - n\\bar{y}\\bar{x} \\\\[10pt] &amp;\\Rightarrow \\hat{\\beta}_1\\Big(\\sum\\limits_{i=1}^{n}x_i^2 -n\\bar{x}^2\\Big) = \\sum\\limits_{i=1}^{n}x_iy_i - n\\bar{y}\\bar{x} \\\\[10pt] &amp;\\hat{\\beta}_1 = \\frac{\\sum\\limits_{i=1}^{n}x_iy_i - n\\bar{y}\\bar{x}}{\\sum\\limits_{i=1}^{n}x_i^2 -n\\bar{x}^2} \\end{aligned} \\end{equation}\\] To write \\(\\hat{\\beta}_1\\) in a form that’s more recognizable, we will use the following: \\[\\begin{equation} \\label{cov} \\sum x_iy_i - n\\bar{y}\\bar{x} = \\sum(x - \\bar{x})(y - \\bar{y}) = (n-1)\\text{Cov}(x,y) \\end{equation}\\] \\[\\begin{equation} \\label{var_x} \\sum x_i^2 - n\\bar{x}^2 - \\sum(x - \\bar{x})^2 = (n-1)s_x^2 \\end{equation}\\] where \\(\\text{Cov}(x,y)\\) is the covariance of \\(x\\) and \\(y\\), and \\(s_x^2\\) is the sample variance of \\(x\\) (\\(s_x\\) is the sample standard deviation). Thus, applying () and (), we have \\[\\begin{equation} \\label{est-beta1-pt2} \\begin{aligned} \\hat{\\beta}_1 &amp;= \\frac{\\sum\\limits_{i=1}^{n}x_iy_i - n\\bar{y}\\bar{x}}{\\sum\\limits_{i=1}^{n}x_i^2 -n\\bar{x}^2} \\\\[10pt] &amp;= \\frac{\\sum\\limits_{i=1}^{n}(x-\\bar{x})(y-\\bar{y})}{\\sum\\limits_{i=1}^{n}(x-\\bar{x})^2}\\\\[10pt] &amp;= \\frac{(n-1)\\text{Cov}(x,y)}{(n-1)s_x^2}\\\\[10pt] &amp;= \\frac{\\text{Cov}(x,y)}{s_x^2} \\end{aligned} \\end{equation}\\] The correlation between \\(x\\) and \\(y\\) is \\(r = \\frac{\\text{Cov}(x,y)}{s_x s_y}\\). Thus, \\(\\text{Cov}(x,y) = r s_xs_y\\). Plugging this into (), we have \\[\\begin{equation} \\hat{\\beta}_1 = \\frac{\\text{Cov}(x,y)}{s_x^2} = r\\frac{s_ys_x}{s_x^2} = r\\frac{s_y}{s_x} \\end{equation}\\] "],
["anova.html", "Chapter 3 Analysis of Variance", " Chapter 3 Analysis of Variance "],
["mlr.html", "Chapter 4 Multiple Linear Regression", " Chapter 4 Multiple Linear Regression "],
["select.html", "Chapter 5 Model Selection", " Chapter 5 Model Selection "],
["logistic.html", "Chapter 6 Logistic Regression", " Chapter 6 Logistic Regression "],
["multinom-logistic.html", "Chapter 7 Multinomial Logistic Regression", " Chapter 7 Multinomial Logistic Regression "],
["special.html", "Chapter 8 Special Topics", " Chapter 8 Special Topics "],
["datasets.html", "Chapter 9 Data Sets", " Chapter 9 Data Sets Below is a list of the datasets used in this book. More details about each dataset are coming soon. advertising.csv airbnb_basic.csv airbnb_details.csv beer.csv bikeshare.csv evals_mod.csv fivethirtyeight-recent-grads.R framingham.csv gss2016.csv KingCountyHouses.csv movies.csv recent-grads.csv sesame.csv sis.csv spotify.csv test_songs.csv "]
]
