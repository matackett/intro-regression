<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>2.6 Details about Model Diagnostics | Intro to Regression Analysis</title>
  <meta name="description" content="This document contains lab assignments and other materials for an intermediate-level regression course.">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="2.6 Details about Model Diagnostics | Intro to Regression Analysis" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This document contains lab assignments and other materials for an intermediate-level regression course." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="2.6 Details about Model Diagnostics | Intro to Regression Analysis" />
  
  <meta name="twitter:description" content="This document contains lab assignments and other materials for an intermediate-level regression course." />
  

<meta name="author" content="Maria Tackett">


<meta name="date" content="2019-05-14">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="2-5-log-transformations-in-linear-regression.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />










<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Broadening Your Statistical Horizons</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Beginning of the Book</a></li>
<li class="chapter" data-level="2" data-path="2-mlr.html"><a href="2-mlr.html"><i class="fa fa-check"></i><b>2</b> Multiple Linear Regression</a><ul>
<li class="chapter" data-level="2.1" data-path="2-1-multiple-linear-regression.html"><a href="2-1-multiple-linear-regression.html"><i class="fa fa-check"></i><b>2.1</b> Multiple Linear Regression</a><ul>
<li class="chapter" data-level="2.1.1" data-path="2-1-multiple-linear-regression.html"><a href="2-1-multiple-linear-regression.html#packages"><i class="fa fa-check"></i><b>2.1.1</b> Packages</a></li>
<li class="chapter" data-level="2.1.2" data-path="2-1-multiple-linear-regression.html"><a href="2-1-multiple-linear-regression.html#data"><i class="fa fa-check"></i><b>2.1.2</b> Data</a></li>
<li class="chapter" data-level="2.1.3" data-path="2-1-multiple-linear-regression.html"><a href="2-1-multiple-linear-regression.html#exercises"><i class="fa fa-check"></i><b>2.1.3</b> Exercises</a></li>
<li class="chapter" data-level="2.1.4" data-path="2-1-multiple-linear-regression.html"><a href="2-1-multiple-linear-regression.html#acknowledgement"><i class="fa fa-check"></i><b>2.1.4</b> Acknowledgement</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="2-2-data-wrangling-multiple-linear-regression.html"><a href="2-2-data-wrangling-multiple-linear-regression.html"><i class="fa fa-check"></i><b>2.2</b> Data Wrangling &amp; Multiple Linear Regression</a><ul>
<li class="chapter" data-level="2.2.1" data-path="2-2-data-wrangling-multiple-linear-regression.html"><a href="2-2-data-wrangling-multiple-linear-regression.html#packages-1"><i class="fa fa-check"></i><b>2.2.1</b> Packages</a></li>
<li class="chapter" data-level="2.2.2" data-path="2-2-data-wrangling-multiple-linear-regression.html"><a href="2-2-data-wrangling-multiple-linear-regression.html#data-1"><i class="fa fa-check"></i><b>2.2.2</b> Data</a></li>
<li class="chapter" data-level="2.2.3" data-path="2-2-data-wrangling-multiple-linear-regression.html"><a href="2-2-data-wrangling-multiple-linear-regression.html#exercises-1"><i class="fa fa-check"></i><b>2.2.3</b> Exercises</a></li>
<li class="chapter" data-level="2.2.4" data-path="2-2-data-wrangling-multiple-linear-regression.html"><a href="2-2-data-wrangling-multiple-linear-regression.html#acknowledgement-1"><i class="fa fa-check"></i><b>2.2.4</b> Acknowledgement</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="2-3-analyzing-wages.html"><a href="2-3-analyzing-wages.html"><i class="fa fa-check"></i><b>2.3</b> Analyzing Wages</a><ul>
<li class="chapter" data-level="2.3.1" data-path="2-3-analyzing-wages.html"><a href="2-3-analyzing-wages.html#initial-model"><i class="fa fa-check"></i><b>2.3.1</b> Initial model</a></li>
<li class="chapter" data-level="2.3.2" data-path="2-3-analyzing-wages.html"><a href="2-3-analyzing-wages.html#model-with-mean-centered-variables"><i class="fa fa-check"></i><b>2.3.2</b> Model with mean-centered variables</a></li>
<li class="chapter" data-level="2.3.3" data-path="2-3-analyzing-wages.html"><a href="2-3-analyzing-wages.html#model-with-indicator-variables"><i class="fa fa-check"></i><b>2.3.3</b> Model with indicator variables</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="2-4-matrix-form-of-linear-regression.html"><a href="2-4-matrix-form-of-linear-regression.html"><i class="fa fa-check"></i><b>2.4</b> Matrix Form of Linear Regression</a><ul>
<li class="chapter" data-level="2.4.1" data-path="2-4-matrix-form-of-linear-regression.html"><a href="2-4-matrix-form-of-linear-regression.html#introduction"><i class="fa fa-check"></i><b>2.4.1</b> Introduction</a></li>
<li class="chapter" data-level="2.4.2" data-path="2-4-matrix-form-of-linear-regression.html"><a href="2-4-matrix-form-of-linear-regression.html#matrix-form-for-the-regression-model"><i class="fa fa-check"></i><b>2.4.2</b> Matrix Form for the Regression Model</a></li>
<li class="chapter" data-level="2.4.3" data-path="2-4-matrix-form-of-linear-regression.html"><a href="2-4-matrix-form-of-linear-regression.html#estimating-the-coefficients"><i class="fa fa-check"></i><b>2.4.3</b> Estimating the Coefficients</a></li>
<li class="chapter" data-level="2.4.4" data-path="2-4-matrix-form-of-linear-regression.html"><a href="2-4-matrix-form-of-linear-regression.html#variance-covariance-matrix-of-the-coefficients"><i class="fa fa-check"></i><b>2.4.4</b> Variance-covariance matrix of the coefficients</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="2-5-log-transformations-in-linear-regression.html"><a href="2-5-log-transformations-in-linear-regression.html"><i class="fa fa-check"></i><b>2.5</b> Log Transformations in Linear Regression</a><ul>
<li class="chapter" data-level="2.5.1" data-path="2-5-log-transformations-in-linear-regression.html"><a href="2-5-log-transformations-in-linear-regression.html#log-transformation-on-the-response-variable"><i class="fa fa-check"></i><b>2.5.1</b> Log-transformation on the response variable</a></li>
<li class="chapter" data-level="2.5.2" data-path="2-5-log-transformations-in-linear-regression.html"><a href="2-5-log-transformations-in-linear-regression.html#log-transformation-on-the-predictor-variable"><i class="fa fa-check"></i><b>2.5.2</b> Log-transformation on the predictor variable</a></li>
<li class="chapter" data-level="2.5.3" data-path="2-5-log-transformations-in-linear-regression.html"><a href="2-5-log-transformations-in-linear-regression.html#log-transformation-on-the-the-response-and-predictor-variable"><i class="fa fa-check"></i><b>2.5.3</b> Log-transformation on the the response and predictor variable</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="2-6-details-about-model-diagnostics.html"><a href="2-6-details-about-model-diagnostics.html"><i class="fa fa-check"></i><b>2.6</b> Details about Model Diagnostics</a><ul>
<li class="chapter" data-level="2.6.1" data-path="2-6-details-about-model-diagnostics.html"><a href="2-6-details-about-model-diagnostics.html#introduction-1"><i class="fa fa-check"></i><b>2.6.1</b> Introduction</a></li>
<li class="chapter" data-level="2.6.2" data-path="2-6-details-about-model-diagnostics.html"><a href="2-6-details-about-model-diagnostics.html#matrix-form-for-the-regression-model-1"><i class="fa fa-check"></i><b>2.6.2</b> Matrix Form for the Regression Model</a></li>
<li class="chapter" data-level="2.6.3" data-path="2-6-details-about-model-diagnostics.html"><a href="2-6-details-about-model-diagnostics.html#hat-matrix-leverage"><i class="fa fa-check"></i><b>2.6.3</b> Hat Matrix &amp; Leverage</a></li>
<li class="chapter" data-level="2.6.4" data-path="2-6-details-about-model-diagnostics.html"><a href="2-6-details-about-model-diagnostics.html#standardized-residuals"><i class="fa fa-check"></i><b>2.6.4</b> Standardized Residuals</a></li>
<li class="chapter" data-level="2.6.5" data-path="2-6-details-about-model-diagnostics.html"><a href="2-6-details-about-model-diagnostics.html#cooks-distance"><i class="fa fa-check"></i><b>2.6.5</b> Cookâ€™s Distance</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Intro to Regression Analysis</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="details-about-model-diagnostics" class="section level2">
<h2><span class="header-section-number">2.6</span> Details about Model Diagnostics</h2>
<p>This document discusses some of the mathematical details of the model diagnostics - leverage, standardized residuals, and Cookâ€™s distance. We assume the reader knowledge of the matrix form for multiple linear regression.Please see <a href="https://github.com/STA210-Sp19/supplemental-notes/blob/master/regression-basics-matrix.pdf">Matrix Form of Linear Regression</a> for a review.</p>
<div id="introduction-1" class="section level3">
<h3><span class="header-section-number">2.6.1</span> Introduction</h3>
<p>Suppose we have <span class="math inline">\(n\)</span> observations. Let the <span class="math inline">\(i^{th}\)</span> be <span class="math inline">\((x_{i1}, \ldots, x_{ip}, y_i)\)</span>, such that <span class="math inline">\(x_{i1}, \ldots, x_{ip}\)</span> are the explanatory variables (predictors) and <span class="math inline">\(y_i\)</span> is the response variable. We assume the data can be modeled using the least-squares regression model, such that the mean response for a given combination of explanatory variables follows the form in ().</p>
<span class="math display">\[\begin{equation}
\label{basic_model}
y = \beta_0 + \beta_1 x_1 + \dots + \beta_p x_p 
\end{equation}\]</span>
<p>We can write the response for the <span class="math inline">\(i^{th}\)</span> observation as shown in ()</p>
<span class="math display">\[\begin{equation}
\label{ind_response}
y_i = \beta_0 + \beta_1 x_{i1} + \dots + \beta_p x_{ip} + \epsilon_i 
\end{equation}\]</span>
<p>such that <span class="math inline">\(\epsilon_i\)</span> is the amount <span class="math inline">\(y_i\)</span> deviates from <span class="math inline">\(\mu\{y|x_{i1}, \ldots, x_{ip}\}\)</span>, the mean response for a given combination of explanatory variables. We assume each <span class="math inline">\(\epsilon_i \sim N(0,\sigma^2)\)</span>, where <span class="math inline">\(\sigma^2\)</span> is a constant variance for the distribution of the response <span class="math inline">\(y\)</span> for any combination of explanatory variables <span class="math inline">\(x_1, \ldots, x_p\)</span>.</p>
</div>
<div id="matrix-form-for-the-regression-model-1" class="section level3">
<h3><span class="header-section-number">2.6.2</span> Matrix Form for the Regression Model</h3>
<p>We can represent the () and () using matrix notation. Let</p>
<span class="math display">\[\begin{equation}
\label{matrix notation}
\mathbf{Y} = \begin{bmatrix}y_1 \\ y_2 \\ \vdots \\y_n\end{bmatrix} 
\hspace{15mm}
\mathbf{X} = \begin{bmatrix}x_{11} &amp; x_{12} &amp; \dots &amp; x_{1p} \\
x_{21} &amp; x_{22} &amp; \dots &amp; x_{2p} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots\\
x_{n1} &amp; x_{n2} &amp; \dots &amp; x_{np} \end{bmatrix}
\hspace{15mm}
\boldsymbol{\beta}= \begin{bmatrix}\beta_0 \\ \beta_1 \\ \vdots \\ \beta_p \end{bmatrix} 
\hspace{15mm}
\boldsymbol{\epsilon}= \begin{bmatrix}\epsilon_1 \\ \epsilon_2 \\ \vdots \\ \epsilon_n \end{bmatrix}
\end{equation}\]</span>
<p>Thus,</p>
<p><span class="math display">\[\mathbf{Y} = \mathbf{X}\boldsymbol{\beta} + \mathbf{\epsilon}\]</span></p>
<p>Therefore the estimated response for a given combination of explanatory variables and the associated residuals can be written as</p>
<span class="math display">\[\begin{equation}
\label{matrix_mean}
\hat{\mathbf{Y}} = \mathbf{X}\hat{\boldsymbol{\beta}} \hspace{10mm} \mathbf{e} = \mathbf{Y} - \mathbf{X}\hat{\boldsymbol{\beta}}
\end{equation}\]</span>
</div>
<div id="hat-matrix-leverage" class="section level3">
<h3><span class="header-section-number">2.6.3</span> Hat Matrix &amp; Leverage</h3>
<p>Recall from the notes <a href="https://github.com/STA210-Sp19/supplemental-notes/blob/master/regression-basics-matrix.pdf"><strong>Matrix Form of Linear Regression</strong></a> that <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> can be written as the following:</p>
<span class="math display">\[\begin{equation}
\label{beta-hat}
\hat{\boldsymbol{\beta}} = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{Y}
\end{equation}\]</span>
<p>Combining () and (), we can write <span class="math inline">\(\hat{\mathbf{Y}}\)</span> as the following:</p>
<span class="math display">\[\begin{equation}
\label{y-hat}
\begin{aligned}
\hat{\mathbf{Y}} &amp;= \mathbf{X}\hat{\boldsymbol{\beta}} \\[10pt]
&amp;= \mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{Y}\\
\end{aligned}
\end{equation}\]</span>
<p>We define the <strong>hat matrix</strong> as an <span class="math inline">\(n \times n\)</span> matrix of the form <span class="math inline">\(\mathbf{H} = \mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\)</span>. Thus () becomes</p>
<span class="math display">\[\begin{equation}
\label{y-hat-matrix}
\hat{\mathbf{Y}} = \mathbf{H}\mathbf{Y}
\end{equation}\]</span>
<p>The diagonal elements of the hat matrix are a measure of how far the predictor variables of each observation are from the means of the predictor variables. For example, <span class="math inline">\(h_{ii}\)</span> is a measure of how far the values of the predictor variables for the <span class="math inline">\(i^{th}\)</span> observation, <span class="math inline">\(x_{i1}, x_{i2}, \ldots, x_{ip}\)</span>, are from the mean values of the predictor variables, <span class="math inline">\(\bar{x}_1, \bar{x}_2, \ldots, \bar{x}_p\)</span>. In the case of simple linear regression, the <span class="math inline">\(i^{th}\)</span> diagonal, <span class="math inline">\(h_{ii}\)</span>, can be written as</p>
<p><span class="math display">\[h_{ii} =  \frac{1}{n} + \frac{(x_i - \bar{x})^2}{\sum_{j=1}^{n}(x_j-\bar{x})^2}\]</span></p>
<p>We call these diagonal elements, the <strong>leverage</strong> of each observation.</p>
<p>The diagonal elements of the hat matrix have the following properties:</p>
<ul>
<li><span class="math inline">\(0 \leq h_ii \leq 1\)</span></li>
<li><span class="math inline">\(\sum\limits_{i=1}^{n} h_{ii} = p+1\)</span>, where <span class="math inline">\(p\)</span> is the number of predictor variables in the model.</li>
<li>The mean hat value is <span class="math inline">\(\bar{h} = \frac{\sum\limits_{i=1}^{n} h_{ii}}{n} = \frac{p+1}{n}\)</span>.</li>
</ul>
<p>Using these properties, we consider a point to have <strong>high leverage</strong> if it has a leverage value that is more than 2 times the average. In other words, observations with leverage greater than <span class="math inline">\(\frac{2(p+1)}{n}\)</span> are considered to be <strong>high leverage</strong> points, i.e.Â outliers in the predictor variables. We are interested in flagging high leverage points, because they may have an influence on the regression coefficients.</p>
<p>When there are high leverage points in the data, the regression line will tend towards those points; therefore, one property of high leverage points is that they tend to have small residuals. We will show this by rewriting the residuals from () using ().</p>
<span class="math display">\[\begin{equation}
\label{resid-hat}
\begin{aligned}
\mathbf{e} &amp;= \mathbf{Y} - \hat{\mathbf{Y}} \\[10pt]
&amp; = \mathbf{Y} - \mathbf{H}\mathbf{Y} \\[10pt]
&amp;= (1-\mathbf{H})\mathbf{Y}
\end{aligned}
\end{equation}\]</span>
<p>Note that the identity matrix and hat matrix are <strong>idempotent</strong>, i.e. <span class="math inline">\(\mathbf{I}\mathbf{I} = \mathbf{I}\)</span>, <span class="math inline">\(\mathbf{H}\mathbf{H} = \mathbf{H}\)</span>. Thus, <span class="math inline">\((\mathbf{I} - \mathbf{H}\)</span> is also idempotent. These matrices are also symmetric. Using these properties and (), we have that the variance-covariance matrix of the residuals <span class="math inline">\(\boldsymbol{e}\)</span>, is</p>
<span class="math display">\[\begin{equation}
\label{resid-var}
\begin{aligned}
Var(\mathbf{e}) &amp;= \mathbf{e}\mathbf{e}^T \\[10pt]
&amp;=  (1-\mathbf{H})Var(\mathbf{Y})^T(1-\mathbf{H})^T \\[10pt]
&amp;= (1-\mathbf{H})\hat{\sigma}^2(1-\mathbf{H})^T  \\[10pt]
&amp;= \hat{\sigma}^2(1-\mathbf{H})(1-\mathbf{H})  \\[10pt]
&amp;= \hat{\sigma}^2(1-\mathbf{H})
\end{aligned}
\end{equation}\]</span>
<p>where <span class="math inline">\(\hat{\sigma}^2 = \frac{\sum_{i=1}^{n}e_i^2}{n-p-1}\)</span> is the estimated regression variance. Thus, the variance of the <span class="math inline">\(i^{th}\)</span> residual is <span class="math inline">\(Var(e_i) = \hat{\sigma}^2(1-h_{ii})\)</span>. Therefore, the higher the leverage, the smaller the variance of the residual. Because the expected value of the residuals is 0, we conclude that points with high leverage tend to have smaller residuals than points with lower leverage.</p>
</div>
<div id="standardized-residuals" class="section level3">
<h3><span class="header-section-number">2.6.4</span> Standardized Residuals</h3>
<p>In general, we standardize a value by shifting by the expected value and rescaling by the standard deviation (or standard error). Thus, the <span class="math inline">\(i^{th}\)</span> standardized residual takes the form</p>
<p><span class="math display">\[std.res_i = \frac{e_i - E(e_i)}{SE(e_i)}\]</span></p>
<p>The expected value of the residuals is 0, i.e. <span class="math inline">\(E(e_i) = 0\)</span>. From (), the standard error of the residual is <span class="math inline">\(SE(e_i) = \hat{\sigma}\sqrt{1-h_{ii}}\)</span>. Therefore,</p>
<span class="math display">\[\begin{equation}
\label{std.resid.}
std.res_i = \frac{e_i}{\hat{\sigma}\sqrt{1-h_{ii}}}
\end{equation}\]</span>
</div>
<div id="cooks-distance" class="section level3">
<h3><span class="header-section-number">2.6.5</span> Cookâ€™s Distance</h3>
<p>Cookâ€™s distance is a measure of how much each observation influences the model coefficients, and thus the predicted values. The Cookâ€™s distance for the <span class="math inline">\(i^{th}\)</span> observation can be written as</p>
<span class="math display">\[\begin{equation}
\label{cooksd}
D_i = \frac{(\hat{\mathbf{Y}} -\hat{\mathbf{Y}}_{(i)})^T(\hat{\mathbf{Y}} -\hat{\mathbf{Y}}_{(i)})}{(p+1)\hat{\sigma}}
\end{equation}\]</span>
<p>where <span class="math inline">\(\hat{\mathbf{Y}}_{(i)}\)</span> is the vector of predicted values from the model fitted when the <span class="math inline">\(i^{th}\)</span> observation is deleted. Cookâ€™s Distance can be calculated without deleting observations one at a time, since () below is mathematically equivalent to ().</p>
<span class="math display">\[\begin{equation}
\label{cooksd-v2}
D_i = \frac{1}{p+1}std.res_i^2\Bigg[\frac{h_{ii}}{(1-h_{ii})}\Bigg] = \frac{e_i^2}{(p+1)\hat{\sigma}^2(1-h_{ii})}\Bigg[\frac{h_{ii}}{(1-h_{ii})}\Bigg]
\end{equation}\]</span>

</div>
</div>
<!-- </div> -->
            </section>

          </div>
        </div>
      </div>
<a href="2-5-log-transformations-in-linear-regression.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": ["intro-regression.pdf", "intro-regression.epub"],
"toc": {
"collapse": "section",
"toc_depth": 1,
"scroll_highlight": true
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
