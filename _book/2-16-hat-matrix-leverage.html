<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>2.16 Hat Matrix &amp; Leverage | Intro to Regression Analysis</title>
  <meta name="description" content="This document contains lab assignments and other materials for an intermediate-level regression course.">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="2.16 Hat Matrix &amp; Leverage | Intro to Regression Analysis" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This document contains lab assignments and other materials for an intermediate-level regression course." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="2.16 Hat Matrix &amp; Leverage | Intro to Regression Analysis" />
  
  <meta name="twitter:description" content="This document contains lab assignments and other materials for an intermediate-level regression course." />
  

<meta name="author" content="Maria Tackett">


<meta name="date" content="2019-05-14">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="2-15-matrix-form-for-the-regression-model-1.html">
<link rel="next" href="2-17-standardized-residuals.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />










<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Broadening Your Statistical Horizons</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Beginning of the Book</a></li>
<li class="chapter" data-level="2" data-path="2-mlr.html"><a href="2-mlr.html"><i class="fa fa-check"></i><b>2</b> Multiple Linear Regression</a><ul>
<li class="chapter" data-level="2.1" data-path="2-1-multiple-linear-regression.html"><a href="2-1-multiple-linear-regression.html"><i class="fa fa-check"></i><b>2.1</b> Multiple Linear Regression</a><ul>
<li class="chapter" data-level="2.1.1" data-path="2-1-multiple-linear-regression.html"><a href="2-1-multiple-linear-regression.html#packages"><i class="fa fa-check"></i><b>2.1.1</b> Packages</a></li>
<li class="chapter" data-level="2.1.2" data-path="2-1-multiple-linear-regression.html"><a href="2-1-multiple-linear-regression.html#data"><i class="fa fa-check"></i><b>2.1.2</b> Data</a></li>
<li class="chapter" data-level="2.1.3" data-path="2-1-multiple-linear-regression.html"><a href="2-1-multiple-linear-regression.html#exercises"><i class="fa fa-check"></i><b>2.1.3</b> Exercises</a></li>
<li class="chapter" data-level="2.1.4" data-path="2-1-multiple-linear-regression.html"><a href="2-1-multiple-linear-regression.html#acknowledgement"><i class="fa fa-check"></i><b>2.1.4</b> Acknowledgement</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="2-2-data-wrangling-multiple-linear-regression.html"><a href="2-2-data-wrangling-multiple-linear-regression.html"><i class="fa fa-check"></i><b>2.2</b> Data Wrangling &amp; Multiple Linear Regression</a><ul>
<li class="chapter" data-level="2.2.1" data-path="2-2-data-wrangling-multiple-linear-regression.html"><a href="2-2-data-wrangling-multiple-linear-regression.html#packages-1"><i class="fa fa-check"></i><b>2.2.1</b> Packages</a></li>
<li class="chapter" data-level="2.2.2" data-path="2-2-data-wrangling-multiple-linear-regression.html"><a href="2-2-data-wrangling-multiple-linear-regression.html#data-1"><i class="fa fa-check"></i><b>2.2.2</b> Data</a></li>
<li class="chapter" data-level="2.2.3" data-path="2-2-data-wrangling-multiple-linear-regression.html"><a href="2-2-data-wrangling-multiple-linear-regression.html#exercises-1"><i class="fa fa-check"></i><b>2.2.3</b> Exercises</a></li>
<li class="chapter" data-level="2.2.4" data-path="2-2-data-wrangling-multiple-linear-regression.html"><a href="2-2-data-wrangling-multiple-linear-regression.html#acknowledgement-1"><i class="fa fa-check"></i><b>2.2.4</b> Acknowledgement</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="2-3-analyzing-wages.html"><a href="2-3-analyzing-wages.html"><i class="fa fa-check"></i><b>2.3</b> Analyzing Wages</a><ul>
<li class="chapter" data-level="2.3.1" data-path="2-3-analyzing-wages.html"><a href="2-3-analyzing-wages.html#initial-model"><i class="fa fa-check"></i><b>2.3.1</b> Initial model</a></li>
<li class="chapter" data-level="2.3.2" data-path="2-3-analyzing-wages.html"><a href="2-3-analyzing-wages.html#model-with-mean-centered-variables"><i class="fa fa-check"></i><b>2.3.2</b> Model with mean-centered variables</a></li>
<li class="chapter" data-level="2.3.3" data-path="2-3-analyzing-wages.html"><a href="2-3-analyzing-wages.html#model-with-indicator-variables"><i class="fa fa-check"></i><b>2.3.3</b> Model with indicator variables</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="2-4-matrix-form-of-linear-regression.html"><a href="2-4-matrix-form-of-linear-regression.html"><i class="fa fa-check"></i><b>2.4</b> Matrix Form of Linear Regression</a></li>
<li class="chapter" data-level="2.5" data-path="2-5-introduction.html"><a href="2-5-introduction.html"><i class="fa fa-check"></i><b>2.5</b> Introduction</a></li>
<li class="chapter" data-level="2.6" data-path="2-6-matrix-form-for-the-regression-model.html"><a href="2-6-matrix-form-for-the-regression-model.html"><i class="fa fa-check"></i><b>2.6</b> Matrix Form for the Regression Model</a></li>
<li class="chapter" data-level="2.7" data-path="2-7-estimating-the-coefficients.html"><a href="2-7-estimating-the-coefficients.html"><i class="fa fa-check"></i><b>2.7</b> Estimating the Coefficients</a></li>
<li class="chapter" data-level="2.8" data-path="2-8-variance-covariance-matrix-of-the-coefficients.html"><a href="2-8-variance-covariance-matrix-of-the-coefficients.html"><i class="fa fa-check"></i><b>2.8</b> Variance-covariance matrix of the coefficients</a></li>
<li class="chapter" data-level="2.9" data-path="2-9-log-transformations-in-linear-regression.html"><a href="2-9-log-transformations-in-linear-regression.html"><i class="fa fa-check"></i><b>2.9</b> Log Transformations in Linear Regression</a></li>
<li class="chapter" data-level="2.10" data-path="2-10-log-transformation-on-the-response-variable.html"><a href="2-10-log-transformation-on-the-response-variable.html"><i class="fa fa-check"></i><b>2.10</b> Log-transformation on the response variable</a><ul>
<li class="chapter" data-level="2.10.1" data-path="2-10-log-transformation-on-the-response-variable.html"><a href="2-10-log-transformation-on-the-response-variable.html#mean-median-and-log-transformations"><i class="fa fa-check"></i><b>2.10.1</b> Mean, Median, and Log Transformations</a></li>
<li class="chapter" data-level="2.10.2" data-path="2-10-log-transformation-on-the-response-variable.html"><a href="2-10-log-transformation-on-the-response-variable.html#interpretaton-of-model-coefficients"><i class="fa fa-check"></i><b>2.10.2</b> Interpretaton of model coefficients</a></li>
</ul></li>
<li class="chapter" data-level="2.11" data-path="2-11-log-transformation-on-the-predictor-variable.html"><a href="2-11-log-transformation-on-the-predictor-variable.html"><i class="fa fa-check"></i><b>2.11</b> Log-transformation on the predictor variable</a></li>
<li class="chapter" data-level="2.12" data-path="2-12-log-transformation-on-the-the-response-and-predictor-variable.html"><a href="2-12-log-transformation-on-the-the-response-and-predictor-variable.html"><i class="fa fa-check"></i><b>2.12</b> Log-transformation on the the response and predictor variable</a></li>
<li class="chapter" data-level="2.13" data-path="2-13-details-about-model-diagnostics.html"><a href="2-13-details-about-model-diagnostics.html"><i class="fa fa-check"></i><b>2.13</b> Details about Model Diagnostics</a></li>
<li class="chapter" data-level="2.14" data-path="2-14-introduction-1.html"><a href="2-14-introduction-1.html"><i class="fa fa-check"></i><b>2.14</b> Introduction</a></li>
<li class="chapter" data-level="2.15" data-path="2-15-matrix-form-for-the-regression-model-1.html"><a href="2-15-matrix-form-for-the-regression-model-1.html"><i class="fa fa-check"></i><b>2.15</b> Matrix Form for the Regression Model</a></li>
<li class="chapter" data-level="2.16" data-path="2-16-hat-matrix-leverage.html"><a href="2-16-hat-matrix-leverage.html"><i class="fa fa-check"></i><b>2.16</b> Hat Matrix &amp; Leverage</a></li>
<li class="chapter" data-level="2.17" data-path="2-17-standardized-residuals.html"><a href="2-17-standardized-residuals.html"><i class="fa fa-check"></i><b>2.17</b> Standardized Residuals</a></li>
<li class="chapter" data-level="2.18" data-path="2-18-cooks-distance.html"><a href="2-18-cooks-distance.html"><i class="fa fa-check"></i><b>2.18</b> Cook’s Distance</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Intro to Regression Analysis</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="hat-matrix-leverage" class="section level2">
<h2><span class="header-section-number">2.16</span> Hat Matrix &amp; Leverage</h2>
<p>Recall from the notes <a href="https://github.com/STA210-Sp19/supplemental-notes/blob/master/regression-basics-matrix.pdf"><strong>Matrix Form of Linear Regression</strong></a> that <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> can be written as the following:</p>
<span class="math display">\[\begin{equation}
\label{beta-hat}
\hat{\boldsymbol{\beta}} = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{Y}
\end{equation}\]</span>
<p>Combining () and (), we can write <span class="math inline">\(\hat{\mathbf{Y}}\)</span> as the following:</p>
<span class="math display">\[\begin{equation}
\label{y-hat}
\begin{aligned}
\hat{\mathbf{Y}} &amp;= \mathbf{X}\hat{\boldsymbol{\beta}} \\[10pt]
&amp;= \mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{Y}\\
\end{aligned}
\end{equation}\]</span>
<p>We define the <strong>hat matrix</strong> as an <span class="math inline">\(n \times n\)</span> matrix of the form <span class="math inline">\(\mathbf{H} = \mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\)</span>. Thus () becomes</p>
<span class="math display">\[\begin{equation}
\label{y-hat-matrix}
\hat{\mathbf{Y}} = \mathbf{H}\mathbf{Y}
\end{equation}\]</span>
<p>The diagonal elements of the hat matrix are a measure of how far the predictor variables of each observation are from the means of the predictor variables. For example, <span class="math inline">\(h_{ii}\)</span> is a measure of how far the values of the predictor variables for the <span class="math inline">\(i^{th}\)</span> observation, <span class="math inline">\(x_{i1}, x_{i2}, \ldots, x_{ip}\)</span>, are from the mean values of the predictor variables, <span class="math inline">\(\bar{x}_1, \bar{x}_2, \ldots, \bar{x}_p\)</span>. In the case of simple linear regression, the <span class="math inline">\(i^{th}\)</span> diagonal, <span class="math inline">\(h_{ii}\)</span>, can be written as</p>
<p><span class="math display">\[h_{ii} =  \frac{1}{n} + \frac{(x_i - \bar{x})^2}{\sum_{j=1}^{n}(x_j-\bar{x})^2}\]</span></p>
<p>We call these diagonal elements, the <strong>leverage</strong> of each observation.</p>
<p>The diagonal elements of the hat matrix have the following properties:</p>
<ul>
<li><span class="math inline">\(0 \leq h_ii \leq 1\)</span></li>
<li><span class="math inline">\(\sum\limits_{i=1}^{n} h_{ii} = p+1\)</span>, where <span class="math inline">\(p\)</span> is the number of predictor variables in the model.</li>
<li>The mean hat value is <span class="math inline">\(\bar{h} = \frac{\sum\limits_{i=1}^{n} h_{ii}}{n} = \frac{p+1}{n}\)</span>.</li>
</ul>
<p>Using these properties, we consider a point to have <strong>high leverage</strong> if it has a leverage value that is more than 2 times the average. In other words, observations with leverage greater than <span class="math inline">\(\frac{2(p+1)}{n}\)</span> are considered to be <strong>high leverage</strong> points, i.e. outliers in the predictor variables. We are interested in flagging high leverage points, because they may have an influence on the regression coefficients.</p>
<p>When there are high leverage points in the data, the regression line will tend towards those points; therefore, one property of high leverage points is that they tend to have small residuals. We will show this by rewriting the residuals from () using ().</p>
<span class="math display">\[\begin{equation}
\label{resid-hat}
\begin{aligned}
\mathbf{e} &amp;= \mathbf{Y} - \hat{\mathbf{Y}} \\[10pt]
&amp; = \mathbf{Y} - \mathbf{H}\mathbf{Y} \\[10pt]
&amp;= (1-\mathbf{H})\mathbf{Y}
\end{aligned}
\end{equation}\]</span>
<p>Note that the identity matrix and hat matrix are <strong>idempotent</strong>, i.e. <span class="math inline">\(\mathbf{I}\mathbf{I} = \mathbf{I}\)</span>, <span class="math inline">\(\mathbf{H}\mathbf{H} = \mathbf{H}\)</span>. Thus, <span class="math inline">\((\mathbf{I} - \mathbf{H}\)</span> is also idempotent. These matrices are also symmetric. Using these properties and (), we have that the variance-covariance matrix of the residuals <span class="math inline">\(\boldsymbol{e}\)</span>, is</p>
<span class="math display">\[\begin{equation}
\label{resid-var}
\begin{aligned}
Var(\mathbf{e}) &amp;= \mathbf{e}\mathbf{e}^T \\[10pt]
&amp;=  (1-\mathbf{H})Var(\mathbf{Y})^T(1-\mathbf{H})^T \\[10pt]
&amp;= (1-\mathbf{H})\hat{\sigma}^2(1-\mathbf{H})^T  \\[10pt]
&amp;= \hat{\sigma}^2(1-\mathbf{H})(1-\mathbf{H})  \\[10pt]
&amp;= \hat{\sigma}^2(1-\mathbf{H})
\end{aligned}
\end{equation}\]</span>
<p>where <span class="math inline">\(\hat{\sigma}^2 = \frac{\sum_{i=1}^{n}e_i^2}{n-p-1}\)</span> is the estimated regression variance. Thus, the variance of the <span class="math inline">\(i^{th}\)</span> residual is <span class="math inline">\(Var(e_i) = \hat{\sigma}^2(1-h_{ii})\)</span>. Therefore, the higher the leverage, the smaller the variance of the residual. Because the expected value of the residuals is 0, we conclude that points with high leverage tend to have smaller residuals than points with lower leverage.</p>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="2-15-matrix-form-for-the-regression-model-1.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="2-17-standardized-residuals.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": ["intro-regression.pdf", "intro-regression.epub"],
"toc": {
"collapse": "section",
"toc_depth": 1,
"scroll_highlight": true
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
