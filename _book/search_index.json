[
["index.html", "Intro Regression Welcome to Intro Regression!", " Intro Regression Maria Tackett 2019-05-15 Welcome to Intro Regression! The content in this book was originally developed for STA 210: Regression Analysis at Duke University.The computing aspects of the assignments are written using the tidyverse syntax in R; however, the assignments can be adapted to fit the computing language of your choice. All of the files are available in the Intro Regression GitHub repo. This book is under development and will be periodically upated with new material. Please email me (maria.tackett@duke.edu) if you have any questions, feedback, or suggestions. I would also love to hear about your experience if you use any of the content in your course. License This work is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. "],
["1-getstarted.html", " 1 Getting Started 1.1 How to use this book 1.2 Intro Statistics Review", " 1 Getting Started 1.1 How to use this book Each chapter of this book is a topic that may be covered in an intermediate-level regression analysis course. The topics are arranged based on the way they were taught in STA 210: Regression Analysis; however, the assignments do not have to be used in the order they are presented. Feel free to use the text and adapt it to fit the needs of your course. Each chapter includes several sections of assignments and supplmental notes about the mathematical details. Each section begins with one of the codes below to help you determine the type of assignment or note in that section: COMP: These assignments focus on the computing skills needed to conduct regression analysis. They were originally designed to be completed in groups in a weekly lab/discussion session; however, they can be also be used for homework assignments or in-class work days. Because the emphasis is computing, they include a lot of step-by-step instruction. IN-CLASS: Assignments to be completed as short in-class activities. Most of the code is already written, so students mostly run the code and interpret the output. Students may also need to fill in short lines of code. HW: Focus on putting together conceptual knowledge and computing skills. Most homework assingments include two parts: (1) Concepts &amp; Computations - guided short-answer exercises that focus on conceptual knowledge and short computational tasks, (2) Data Analysis - open-ended question where students perform a complete regression analysis and write results as a narrative. NOTES: Supplemental notes providing more mathematical details. To fully understand the notes, the reader should be familiar with basic concepts in linear algebra. 1.2 Intro Statistics Review The primary audience for this text is students who have completed an introductory statistics course. It is assumed that students are familiar with the concept of statitical inference. This text is also written assuming students have had some exposure to R and the tidyverse syntax. (There is one “Intro to R” assingment included; however, this assignment is not a comprehensive introduction to R.) The following are suggested texts to review statistical concepts and computing: OpenIntro Statistics Modern Dive R for Data Science "],
["2-instr.html", " 2 Notes for Instructors 2.1 Setting up your course 2.2 Making Assignments 2.3 More Information 2.4 References", " 2 Notes for Instructors 2.1 Setting up your course 2.1.1 GitHub GitHub can be used as a course management platform that is a more flexible alternative traditional platforms such as Blackboard and Sakai (Cetinkaya-Rudel and Rundel 2018). It can be used as the central place to share course content and for students to submit assignments. It is structured to promote collaboration, so it is good for courses that include a lot of group assignments. It also gives students practice establishing a workflow that is more representative of what is currently being used in industry. Before setting up your course, create an account on GitHub.com if you do not already have one. Happy Git with R has useful guidelines for making a GitHub username. To set up your course organization in GitHub: Create a new organization in GitHub. To keep things simple, you can name the organization based on the course number and semester. For example, the organization name for my most recent Regression Analysis course was STA210-Sp19. Apply for the GitHub Education benefits to obtain free private repositories for your course organization. By default any repositories created in the course organization will be private and only visible and accessible to the student (or group of students) whose assignment it is and the instructor. This is to comply with Family Educational Rights and Privacy Act (FERPA). Have students create a GitHub username and send it to you. You can share the guidelines from Happy Git with R to help students create usernames that are applicable beyond their time in your course that can be shared with future employers. I typically have students provide their GitHub username as part of the “Getting to Know You” survey at the beginning of the semester. It doesn’t take long to create a GitHub username, so this could also be done as an activity on one of the first days of class. Add students to the GitHub course organization using the ghclass R package. The section “Adding students and creating teams” on the ghclass reference site gives step-by-step instructions for adding new members to the course organization. If students will be working in teams, you can add the teams to the GitHub course organization using by following the steps on ghclass. Any assignment repo for a group assignment will be visible and accessbile to all the members of the team and instructor. You’re done! Now you’re ready to create your first assignment, which is described in the sections below. 2.1.2 RStudio Cloud RStudio Cloud is a great Go to rstudio.cloud and log in or create a new acount. I used the option to log in using my GitHub credentials to keep things simple. Click to create a “New Space” and follow the prompts to create a private workspace for your course. By default, the workspace only holds 10 members and 25 projects. If you need more space for course, submit a request by emailing support@rstudio.com. In the course workspace, go to the “Members” menu and click options. Change the access to “Shared” to create a sharing link that can be distributed to your students. Anyone who clicks the link will automatically be added to your course workspace. Students can log in to RStudio Cloud using their Github credentials. After the first few weeks of class, you can change the access to the course space to “Invitation required”. At that point you would need to send an invitation to anyone else wanting to join the course workspace. See the RStudio Cloud guide for more details about adding members and specific member roles. I typically use the following roles in my course: Primary instructor: Admin role that can manage membership, view, edit and manage all projects in the workspace Secondary instructors / Teaching assistants: Moderator role that can view, edit and manage all projects in the workspace Students: Contributor role (default) that can create, edit and manage their own projects. You’re done! Now you’re ready to create your first project in RStudio Cloud. 2.2 Making Assignments 2.2.1 GitHub 2.2.2 RStudio Cloud 2.3 More Information Watch Tech Talk: Frictionless onboarding to data science with RStudio Cloud for more information about using RStudio Cloud in your classroom. Read Happy Git with R for more information about using Git, Github and RStudio. 2.4 References Çetinkaya-Rudel, M., &amp; Rundel, C. (2018). Infrastructure and Tools for Teaching Computing Throughout the Statistical Curriculum. The American Statistician, 72, 58-65. Happy Git with R: https://happygitwithr.com/ ghclass: https://rundel.github.io/ghclass/articles/ghclass.html Get Started with RStudio Cloud: https://rstudio.cloud/learn/guide#space-members References "],
["3-slr.html", " 3 Simple Linear Regression 3.1 Computing: College Admissions 3.2 In-Class Exercise: Advertising Analysis 3.3 In-Class Exercise: Beer Data Analysis", " 3 Simple Linear Regression 3.1 Computing: College Admissions The primary goal of today’s lab is to give you practice with some of the tools you will need to conduct regression analysis using R. An additional goal for today is for you to be introduced to your teams and practice collaborating using GitHub and RStudio. 3.1.1 Packages We will use the following packages in today’s lab. library(tidyverse) library(skimr) library(broom) library(rcfss) 3.1.2 Data In today’s lab, we will analyze the scorecard dataset from the rcfss package. This dataset contains information about 1849 colleges obtained from the Department of Education’s College Scorecard. Load the rcfss library into the global R environment and type ?scorecard in the console to learn more about the dataset and variable definitions. Today’s analysis will focus on the following variables: type Type of college (Public, Private - nonprofit, Private - for profit) cost The average annual cost of attendance, including tuition and feeds, books and supplies, and living expenses, minus the average grant/scholarship aid admrate Undergraduate admissions rate (from 0 - 100%) 3.1.3 Exercises 3.1.3.1 Exploratory Data Analysis Plot a histogram to examine the distribution of admrate. What is the shape of the distribution? To better understand the distribution of admrate, we would like calculate measures of center and spread of the distribution. Fill in the code below to use the skim function to calculate summary statistics for admrate. Report the appropriate measures of center (mean or median) and spread (standard deviation or IQR) based on the shape of the distribution from Exercise 1. scorecard %&gt;% select(admrate) %&gt;% skim() Plot the distribution of cost and calculate the appropriate summary statistics. Describe the distribution of cost (shape, center, and spread) using the plot and appropriate summary statistics. One nice feature of the skim function is that it provides information about the number of observations that are missing values of the variable. How many observations have missing values of admrate? How many observations have missing values of cost? Later in the semester, we will techniques to deal with missing values in the data. For now, however, we will only include complete observations for the remainder of this analysis. We can use the filter function to select only the rows that values for both cost and admrate. Fill in the code below to create a new dataset called scorecard_new that only includes observations with values for both admrate and cost. __________ &lt;- scorecard %&gt;% filter(!is.na(admrate),________) # Learn more about the `filter` function in [Section 5.2 of R for Data Science] (https://r4ds.had.co.nz/transform.html#filter-rows-with-filter) You will use scorecard_new for the rest of the lab. Create a scatterplot to display the relationship between cost (response variable) and admrate (explanatory variable). Use the scatterplot to describe the relationship between the two variables. The data contains information about the type of college, and we would like to incorporate this information into the scatterplot. One way to do this is to use a different color marker for each type of college. Fill in the code below the scatterplot from the previous exercise with the marker colors based on the variable type. Describe two new observations from this scatterplot that you didn’t see in the previous plot. ggplot(data=scorecard_new, mapping=aes(x=admrate, y=cost, color=type)) + _____________________ 3.1.3.2 Simple Linear Regression Fit a regression model to describe the relationship between a college’s admission rate and cost. Use the tidy function to display the model. Interpret the slope in the context of the problem. Does the intercept have a meaningful interpretation? If so, write the interpretation in the context of the problem. Otherwise, explain why the interpretation is not meaningful. While the tidy function is used to display the model, we can obtain a one-row summary of the model using the glance function. Use the glance function to get a summary of the model fit in the previous exercise. See the documentation for glance for the syntax and a list of values output from the function. What is the value of \\(R^2\\)? Interpret this value in the context of the problem. Do you think this is a “good” value of \\(R^2\\)? Explain. What is the value of \\(\\hat{\\sigma}\\), the residual standard error. What is the 95% confidence interval for the coefficient of admrate, i.e. the slope? Interpret the interval in the context of the data. We want to test the following hypotheses about the population slope \\(\\beta_1\\): \\[H_0: \\beta_1 = 0 \\hspace{5mm} \\text{versus} \\hspace{5mm} H_a: \\beta_1 \\neq 0\\] State what the null and alternative hypotheses mean in terms of the linear relationship between admrate and cost. Consider the confidence interval from Exercise 13 and the hypotheses in Exercise 14. Is the confidence interval consistent with the null or alternative hypothesis? Briefly explain. 3.2 In-Class Exercise: Advertising Analysis In this mini analysis, we will work with the Advertising data used in Chapters 2 and 3 of Introduction to Statistical Learning. 3.2.1 Data and packages We start with loading the packages we’ll use. library(readr) library(tidyverse) library(skimr) library(broom) advertising &lt;- read_csv(&quot;data/advertising.csv&quot;) We will analyze the advertising and sales data for 200 markets. The variables we’ll use are tv: total spending on TV advertising (in $thousands) radio: total spending on radio advertising (in $thousands) newspaper: total spending on newspaper advertising (in $thousands) sales: total sales (in $millions) 3.2.2 Analysis We’ll begin the analysis by getting quick view of the data: glimpse(advertising) Next, we can calculate summary statistics for each of the variables in the data set. advertising %&gt;% skim() What type of advertising has the smallest median spending? What type of advertising has the largest variation in spending? Describe the shape of the distribution of sales. We are most interested in understanding how advertising spending affect sales. One way to quantify the relationship between the variables is by calculating the correlation matrix. advertising %&gt;% cor() What is the correlation between radio and sales? Interpret this value. What type of advertising has the strongest linear relationship with sales? Below are visualizations of sales versus each explanatory variable. advertising %&gt;% ggplot(mapping = aes(x=tv,y=sales)) + geom_point(alpha=0.7) + geom_smooth(method=&quot;lm&quot;,se=FALSE,color=&quot;blue&quot;) + labs(title = &quot;Sales vs. TV Advertising&quot;, x= &quot;TV Advertising (in $thousands)&quot;, y=&quot;_____&quot;) #fill in the Y axis label advertising %&gt;% ggplot(mapping = aes(x=radio,y=sales)) + geom_point(alpha=0.7) + geom_smooth(method=&quot;lm&quot;,se=FALSE,color=&quot;red&quot;) + labs(title = &quot;Sales vs. TV Advertising&quot;, x= &quot;Radio Advertising (in $thousands)&quot;, y=&quot;Sales (in $millions)&quot;) advertising %&gt;% ggplot(mapping = aes(x=newspaper,y=sales)) + geom_point(alpha=0.7) + geom_smooth(method=&quot;lm&quot;,se=FALSE,color=&quot;purple&quot;) + labs(title = &quot;Sales vs. Newspaper Advertising&quot;, x= &quot;Newspaper Advertising (in $thousands)&quot;, y=&quot;Sales (in $millions)&quot;) Since tv appears to have the strongest linear relationship with sales, let’s calculate a simple linear regression model using these two variables. ad_model &lt;- lm(sales ~ tv, data=advertising) ad_model Write the model equation. Interpret the intercept in the context of the problem. Interpret the slope in the context of the problem. 3.3 In-Class Exercise: Beer Data Analysis library(tidyverse) library(readr) library(broom) beer &lt;- read_csv(&quot;data/beer.csv&quot;) In this analysis, we will analyze the relationship between the amount of alcohol (PercentAlcohol) and the caloric content (CaloriesPer12Oz) in domestic beers. Let PercentAlcohol be the predictor variable and CaloriesPer12Oz the response variable. Due to limited class time, we will not do the exploratory data analysis in this example. In practice, however, you should always start with the exploratory data analysis. You can add your answers to this R Markdown document. 1. Calculate a regression model to describe the relationship between PercentAlcohol and CaloriesPer12Oz. Display the model output. model &lt;- lm(CaloriesPer12Oz ~ PercentAlcohol, data=beer) model %&gt;% tidy(conf.int=TRUE) 2. Does it make sense to interpret the intercept? Why or why not? There are non-alocoholic beers, so it is possible to have a meaningful interpretation of the intercept. In our data, however, there are very few beers with less than 3% alcoholic content, so it would not be wise to interpret the intercept. It is not safe to assume the same relationship between PercentAlcohol and CaloriesPer12Oz hold for beers with 0% alcohol; this would be extrapolation. 3. Interpret the 95% confidence interval for the slope in the context of the data. We are 95% confident that the interval (26.557, 30.620) contains the true population slope for PercentAlcohol. This means we are 95% confident that for every 1% increase in alcohol content, the number of calories (per 12 oz) is expected to increase between 26.557 and 30.620 calories. 4. Find the critical value, \\(t^*\\), used to calculate the 95% confidence interval. The code below is a guide; uncomment and complete the lines of code to calculate and display the critical value. n &lt;- nrow(beer) df &lt;- n-2 crit_val &lt;- qt(0.975,df) The critical value used to calculate the 95% confident interval for the slope is ______. 5. Interpret the test statistic in the context of the data The estimated slope of 28.577 is 27.78 standard errors above the hypothesized mean of 0, assuming there is no linear relationship between percent alcohol and calories in domestic beers. 6. How was the p-value calculated? Fill in the code below to calculate the p-value. The code below is a guide; uncomment and complete the lines of code to calculate and display the p-value. test_statistic &lt;- 27.778990 prob &lt;- 1 - pt(abs(test_statistic),df) p_value &lt;- 2 * prob The p-value is _______. Given there is no linear relationship between PercentAlcohol and CaloriesPer12Oz, the probability of obtaining a test statistic with magnitude ________ or more extreme is _______. 7. Fill in the code below to calculate the predicted calories and corresponding 90% interval for a single beer with alcohol content of 4.3%. x0 &lt;- data.frame(PercentAlcohol=4.3) predict.lm(model,x0,interval=&quot;prediction&quot;,conf.level=0.9) 8. Fill in the code below to calculate the predicted calories and corresponding 90% interval for the subset of beers with alcohol content of 4.3%. x0 &lt;- data.frame(PercentAlcohol=4.3) predict.lm(model,x0,interval=&quot;confidence&quot;,conf.level= 0.9) "],
["4-anova.html", " 4 Analysis of Variance 4.1 ANOVA", " 4 Analysis of Variance 4.1 ANOVA The goal of this lab is to use Analysis of Variance (ANOVA) to compare means in multiple groups. Additionally, you will be introduced to new R functions used for wrangling and summarizing data. 4.1.1 Packages We will use the following packages in today’s lab. 4.1.2 Data In today’s lab, we will analyze the diamonds dataset from the ggplot2 package. Type ?diamonds in the console to see a dictionary of the variables in the data set. This analysis will focus on the relationship between a diamond’s carat weight and its color. Before starting the exercises, take a moment to read more about the diamond attributes on the Gemological Institute of America webpage: https://www.gia.edu/diamond-quality-factor. 4.1.3 Exercises The diamonds dataset contains the price and other characteristics for over 50,000 diamonds price from $326 to $18823. In this lab, we will analyze the subset of diamonds that are priced $1200 or less. Create a dataframe called diamonds_low that is the subset of diamonds priced $1200 or less. How many observations are in diamonds_low? When using Analysis of Variance (ANOVA) to compare group means, it is ideal to have approximately the same number of observations in each group. Therefore, we will combine the worst two color groups, I and J, and create a new color category called “I/J”. Since color is an ordinal (&lt;ord&gt;) variable, we need to use the recode_factor function in the dplyr package to create the new category. Use the count function before and after making the new color category to ensure the recoding worked as expected. We begin by plotting the relationship between color and carat. As a group, brainstorm ways to plot the relationship between the two variables, then make one of the plots. Be sure to include informative axes labels and an informative title. Fill in the code below to calculate the mean and variance of carat at each level of color. Based on the plots and summary statistics, does there appear to be a relationship between carat weight the color of diamonds? In other words, does there appear to be a significant difference in the mean carat weight across colors? When using ANOVA to compare means across groups, we make the following assumptions (note how similar they are to the assumptions for regression): Normality: The distribution of \\(y\\) is approximately normal within each category of \\(x\\) - in the \\(k^{th}\\) category, \\(y \\sim (\\mu_k, \\sigma^2)\\) . If the sample size is large, ANOVA is robust to some departures from Normality. Independence: All observations are independent from one another, i.e. one observation does not affect another. Constant Variance: The distribution of \\(y\\) within each category of \\(x\\) has a common variance, \\(\\sigma^2\\). One way to assess if variances are sufficiently equal is to look at the ratio of the maximum group variance to the minimum group variance. If this ratio is less than 2, then we can conclude the variances are approximately equal. This isn’t an exact threshold, but rather a commonly used guideline. Note: There are formal tests for equal variance that are outside the scope of this class. Are the assumptions for ANOVA met? Comment on each assumption using the summary statistics and/or plots from previous exercises to support your conclusion. You may also calculate any additional summary statistics or make additional plots as needed. Regardless of your answer to Excerise 4, We will proceed with the analysis in the remainder of this lab as if the assumptions are met. Use the code below to calculate the ANOVA table. The tidy function from the broom package is used to put the ANOVA output in a dataframe, and with the kable function from the knitr package, you can display the results in an easy-to-read table. Use the ANOVA table to calculate the total mean square, i.e. the sample variance of carat. Show your calculations. You can put the calculations in a code chunk to use R like a calculator. What is \\(\\hat{\\sigma}^2\\), the estimated variance of carat within each level of color. We can use ANOVA to test if the true mean value of carat is equal for all levels of color, i.e. \\[ H_0: \\mu_1 = \\mu_2 = \\dots = \\mu_6\\] State the alternative hypothesis is the context of the data. Based on the ANOVA table, what is your conclusion from the test of the hypotheses in the previous question? State the conclusion in the context of the data. Use the code below to plot a 95% confidence interval for the mean carat weight at each level of color. Calculate the value of sigma by filling in the estimated variance from Exercise 7. The formula for the confidence interval for the mean of group \\(k\\) is \\[\\bar{y}_k \\pm t^* \\frac{\\hat{\\sigma}}{\\sqrt{n_k}}\\] For what color level is the mean carat weight the most different from all the others? Based on this analysis, describe the relationship between the color and the mean carat weight in diamonds that cost $1200 or less. Refer to the diamond documentation to recall what the color scale means. You’re done! Commit all remaining changes, use the commit message “Done with Lab 3!”, and push. Before you wrap up the assignment, make sure the .Rmd, .html, and .md documents are all updated on your GitHub repo. "],
["5-mlr.html", " 5 Multiple Linear Regression 5.1 Multiple Linear Regression 5.2 Data Wrangling &amp; Multiple Linear Regression 5.3 Analyzing Wages 5.4 NOTES: Matrix Form of Linear Regression 5.5 NOTES: Log Transformations 5.6 NOTES: Model Diagnostics", " 5 Multiple Linear Regression 5.1 Multiple Linear Regression The goal of this lab is to use multiple linear regression to understand the variation in the selling price of houses in King County, Washington. You will also gain practice using special predictors, such as categorical predictors and interaction effects, in the model, and you will be introduced to variable transformations. 5.1.1 Packages We will use the following packages in today’s lab. 5.1.2 Data The for today’s lab contains the price and other characteristics of over 20,000 houses sold in King County, Washington (the county that includes Seattle). The dataset includes the following variables: price: selling price of the house date: date house was sold, measured in days since January 1, 2014 bedrooms: number of bedrooms bathrooms: number of bathrooms sqft: interior square footage floors: number of floors waterfront: 1 if the house has a view of the waterfront, 0 otherwise yr_built: year the house was built yr_renovated: 0 if the house was never renovated, the year the house was renovated if else 5.1.3 Exercises Use data visualization and summary statistics to examine the distribution of bedrooms. What is the maximum value? Does this value make sense? If not, what is this an indication of, i.e. how did this value get recorded in the data? Briefly explain. We want to remove observations that have extreme values for bedrooms, i.e. those with values for bedrooms above the 95th percentile in the data. What is the 95th percentile for bedrooms? Use the summarise function to help you calculate this value. Fill in the code below to filter the data so that the extreme observations are removed. How many observations are in the updated dataset? We will use this dataset for the remainder of the analysis. Fit a regression model using square feet to explain variation in the price. Plot the residuals versus the predicted values. Based on this plot, what regression assumption appears to be violated? Briefly explain. Plot the histogram and Normal QQ-plot of the residuals. Based on these plots, what regression assumption appears to be violated? Briefly explain. One way to deal with violations in regression assumptions is to transform the response variable and use that transformed variable when fitting the regression model. (We will talk about this in class next week). Some common transformations used in regression are the natural log (log(y)), the square root (sqrt(y)), and the reciprocal (1/y). Each transformation is applied to the response variable price, and the distributions of the transformed data are shown below. Which transformation should we use to fix the violations of the model assumptions observed in the previous exercise? Briefly explain your choice. Add the variable logprice, the log-transformed version of price, to the data frame. Fit a regression model with logprice as the response and sqft as the predictor variable. Create the residuals plots (residuals vs. predicted, histogram of residuals, Normal QQ-plot). Briefly comment on whether or not using the transformed variable improved on the model assumptions. Though we can explain about 48% of the variation in a house prices by the square footage, we would like to incorporate some of the other available house characteristics in the model. Before fitting the model, use the code below to add the variablefloorsCat that is the categorical version of the variable floors. Discuss with your group why it may make sense to treat floors as categorical, even though it represents a count. Use the count function to see the number of observations at each level of floorsCat. What is the most common number of floors? Use the code below to calculate the mean-centered versions of the variables sqft, bedrooms, and bathrooms and add them to the data frame. It is not appropriate to calculate the mean-centered version of the variable waterfront. Briefly explain why it isn’t. Fit a regression model with logprice as the response variable, and the mean-centered variables from the previous exercise along with waterfront and floorsCat as the predictor variables. Display the model output. What is the baseline level for the variable floorsCat? Interpret the intercept of the model in the context of the data. Write the interpretation in terms of the price. What is the intercept of the model for the subset of houses with 3 floors that are not on the waterfront? Write the intercept in terms of the log(price). We would like to consider potential interactions for the model. A significant interaction occurs when the relationship of a predictor variable with the response depends on the value of another predictor variable. Fill in the code below to plot the relationship between logprice and bedrooms by waterfront. Based on this plot, do you think there is a significant interaction effect between bedrooms and waterfront? In other words, do you think the relationship between the logprice and the number of bedrooms differs based on whether or not a house is on the waterfront? Briefly explain. We will talk more about interaction effects in Monday’s lecture. In HW 03, you explore potential interaction effects using this housing data. You’re done! Commit all remaining changes, use the commit message “Done with Lab 4!”, and push. Before you wrap up the assignment, make sure the .Rmd and .md documents are updated in your GitHub repo. There is a 10% penalty if the .Rmd file has to be knitted to display graphs, i.e. the graphs are not showing in the .md file on GitHub. 5.1.4 Acknowledgement The data used in this lab was obtained from https://github.com/proback/BYSH. 5.2 Data Wrangling &amp; Multiple Linear Regression When doing statistical analyses in practice, there is often a lot of time spent on cleaning and preparing the data. The goal of today’s lab is to practice cleaning messy data, so it can be used in a regression analysis. You will also practice interpreting the results from a regression model that has numeric and categorical predictors and a log-transformed response variable. 5.2.1 Packages We will use the following packages in today’s lab. 5.2.2 Data Today’s data is about Airbnb listings in Asheville, NC. The data was obtained from http://insideairbnb.com/; it was originally scraped from airbnb.com. You can see a visualization of some of the data used in today’s lab at http://insideairbnb.com/asheville/. We will use the following variables in this lab: price: Cost per night (in U.S. dollars) cleaning_fee: Cleaning fee (in U.S. dollars) property_type: Type of dwelling (House, Apartment, etc.) room_type: Entire home/apt (guests have entire place to themselves) Private room (Guests have private room to sleep, all other rooms shared) Shared room (Guests sleep in room shared with others) number_of_reviews: Total number of reviews for the listing review_scores_rating: Average review score (0 - 100) 5.2.3 Exercises 5.2.3.1 Data wrangling We would like to use variables from both the basic_info and details data frames in this analysis. Both dataframes have the variable id that uniquely identifies each Airbnb listing. Because we need data from basic_info and details, we only want to include observations that are in both the basic_info and details datasets. Therefore, we will use an inner_join to combine the two data sets. (Note: Both data frames include a variable called id that uniquely identifies each Airbnb listing. R will use this variable to join the two data frames.) How many observations are in airbnb? How many variables? Some Airbnb rentals have cleaning fees, and we want to include the cleaning fee when we calculate the total rental cost. Use the code below to see how the data in the column cleaning_fee is currently stored in the airbnb data frame. The column cleaning_fee currently contains what type of data? Why do you think the data is stored this way even though cleaning_fee is a quantitative variable? Since cleaning_fee is a quantitative variable, we need to make sure it is stored as numeric data in the dataframe. To do so, we will first use the extract function in tidyr package to create a column of cleaning fees that don’t have the dollar sign. Then, we will use the as.numeric() function to make the extracted data the numeric data type double. Use the typeof function to confirm that cleaning_fee is now stored as a double data type. Use the skim function to view a summary of the cleaning_fee data. How many observations have missing values for cleaning_fee? What do you think is the most likely reason for the missing observations of cleaning_fee? In other words, what does a missing value of cleaning_fee indicate? Fill in the code below to impute the missing values of cleaning_fee with an appropriate numeric value. Then use the skim function to confirm that there are no longer missing values of cleaning_fee. This is an example of data that is missing not at random, since there is a specific pattern/explanation to the misisng data. We will talk more about dealing with missing data later in the semester. Next, we look at the variable property_type. Use the count function to determine how many categories are in the variable property and the frequency of each category. What are the top 4 most common property types? These make up what proportion of the observations? Since an overwhelming majority of the observations in the data are one of the top 4 property types, we would like to create a simplified version of the proprety_type variable that has 5 categories: House, Apartment, Guest suite, Bungalow, and Other. Fill in the code below to create prop_type_simp. Use the code below to check that prop_type_simp was correctly made. Airbnb is most commonly used for travel purposes, i.e. as an alternative to traditional hotels. We only want to include Airbnb listings in our regression analysis that are intended for travel purposes. What are the 5 most common values for the variable minimum_nights? Which value in the top 5 stands out? What is the likely intended purpose for Airbnb listings with this seemingly unusual value for minimum_nights? Filter the airbnb data so that it only includes observations with minimum_nights &lt;= 3. You will use this filtered dataset for the remainder of the lab. 5.2.3.2 Regression Analysis For the response variable, will use the cost to stay at an Airbnb location for 3 nights. Create a new variable called price_3_nights that uses price and cleaning_fee to calculate the total cost to stay at the Airbnb property for 3 nights. Be sure to add this variable to your dataframe. Use histograms to examine the distributions of price_3_nights and log(price_3_nights). Based on the histograms, which variable should you use for the regression model? Briefly explain. Use this variable as the response for the remainder of the lab. Fit a regression model called model1 with the response variable from the previous question and the following predictor variables: prop_type_simp, number_of_reviews, and review_scores_rating. Display the model output. Interpret the coefficient review_scores_rating in terms of price_3_nights. Interpret the coefficient of prop_type_simpGuest suite in terms of price_3_nights. We want to determine if room_type is a significant predictor of the cost for 3 nights, given everything else in the model. Fit a regression model called model2 that includes all of the predictor variables in model1 and room_type. Display the model output. Use the code below to conduct a Nested F test to determine if room_type is a significant predictor of the minimum cost. What is your conclusion from the Nested F test? Suppose you are planning to visit Asheville over spring break, and you want to stay in an Airbnb. You find an Airbnb that is an apartment with a private room, has 10 reviews, and an average rating of 90. Use model2 to predict the total cost to stay at this Airbnb for 3 nights. Include the appropriate 95% interval with your prediction. Report the prediction and interval in terms of price_3_nights. You’re done! Commit all remaining changes, use the commit message “Done with Lab 5!”, and push. Before you wrap up the assignment, make sure the .Rmd and .md documents are updated in your GitHub repo. There is a 10% penalty if the .Rmd file has to be knitted to display graphs, i.e. the graphs are not showing in the .md file on GitHub. 5.2.4 Acknowledgement The data from this lab is from insideairbnb.com 5.3 Analyzing Wages 5.3.1 Initial model 5.3.2 Model with mean-centered variables Calculate the regression model using the mean-centered variables. How did the model change? 5.3.3 Model with indicator variables Use the code below to create a categorical variable for Educ. Calculate the regression model using EducCat instead of Educ. 5.4 NOTES: Matrix Form of Linear Regression This document provides the details for the matrix form of multiple linear regression. We assume the reader has familiarity with some matrix alegbra. Please see Chapter 1 of An Introduction to Statistical Learning for a brief review of matrix algebra. 5.4.1 Introduction Suppose we have \\(n\\) observations. Let the \\(i^{th}\\) be \\((x_{i1}, \\ldots, x_{ip}, y_i)\\), such that \\(x_{i1}, \\ldots, x_{ip}\\) are the explanatory variables (predictors) and \\(y_i\\) is the response variable. We assume the data can be modeled using the least-squares regression model, such that the mean response for a given combination of explanatory variables follows the form in (). \\[\\begin{equation} \\label{basic_model} y = \\beta_0 + \\beta_1 x_1 + \\dots + \\beta_p x_p \\end{equation}\\] We can write the response for the \\(i^{th}\\) observation as shown in () \\[\\begin{equation} \\label{ind_response} y_i = \\beta_0 + \\beta_1 x_{i1} + \\dots + \\beta_p x_{ip} + \\epsilon_i \\end{equation}\\] such that \\(\\epsilon_i\\) is the amount \\(y_i\\) deviates from \\(\\mu\\{y|x_{i1}, \\ldots, x_{ip}\\}\\), the mean response for a given combination of explanatory variables. We assume each \\(\\epsilon_i \\sim N(0,\\sigma^2)\\), where \\(\\sigma^2\\) is a constant variance for the distribution of the response \\(y\\) for any combination of explanatory variables \\(x_1, \\ldots, x_p\\). 5.4.2 Matrix Form for the Regression Model We can represent the () and () using matrix notation. Let \\[\\begin{equation} \\label{matrix notation} \\mathbf{Y} = \\begin{bmatrix}y_1 \\\\ y_2 \\\\ \\vdots \\\\y_n\\end{bmatrix} \\hspace{15mm} \\mathbf{X} = \\begin{bmatrix}x_{11} &amp; x_{12} &amp; \\dots &amp; x_{1p} \\\\ x_{21} &amp; x_{22} &amp; \\dots &amp; x_{2p} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots\\\\ x_{n1} &amp; x_{n2} &amp; \\dots &amp; x_{np} \\end{bmatrix} \\hspace{15mm} \\boldsymbol{\\beta}= \\begin{bmatrix}\\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_p \\end{bmatrix} \\hspace{15mm} \\boldsymbol{\\epsilon}= \\begin{bmatrix}\\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{bmatrix} \\end{equation}\\] Thus, \\[\\mathbf{Y} = \\mathbf{X}\\boldsymbol{\\beta} + \\mathbf{\\epsilon}\\] Therefore the estimated response for a given combination of explanatory variables and the associated residuals can be written as \\[\\begin{equation} \\label{matrix_mean} \\hat{\\mathbf{Y}} = \\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\hspace{10mm} \\mathbf{e} = \\mathbf{Y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\end{equation}\\] 5.4.3 Estimating the Coefficients The least-squares model is the one that minimizes the sum of the squared residuals. Therefore, we want to find the coefficients, \\(\\hat{\\boldsymbol{\\beta}}\\) that minimizes \\[\\begin{equation} \\label{sum_sq_resid} \\sum\\limits_{i=1}^{n} e_{i}^2 = \\mathbf{e}^T\\mathbf{e} = (\\mathbf{Y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}})^T(\\mathbf{Y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}}) \\end{equation}\\] where \\(\\mathbf{e}^T\\), the transpose of the matrix \\(\\mathbf{e}\\). \\[\\begin{equation} \\label{model_equation} (\\mathbf{Y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}})^T(\\mathbf{Y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}}) = (\\mathbf{Y}^T\\mathbf{Y} - \\mathbf{Y}^T \\mathbf{X}\\hat{\\boldsymbol{\\beta}} - (\\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{X}^T\\mathbf{Y} + \\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{X}^T\\mathbf{X} \\hat{\\boldsymbol{\\beta}}) \\end{equation}\\] Note that \\((\\mathbf{Y^T}\\mathbf{X}\\hat{\\boldsymbol{\\beta}})^T = \\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{X}^T\\mathbf{Y}\\). Since these are both constants (i.e. \\(1\\times 1\\) vectors), \\(\\mathbf{Y^T}\\mathbf{X}\\hat{\\boldsymbol{\\beta}} = \\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{X}^T\\mathbf{Y}\\). Thus, () becomes \\[\\begin{equation} \\mathbf{Y}^T\\mathbf{Y} - 2 \\mathbf{X}^T\\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{Y} + \\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{X}^T\\mathbf{X} \\hat{\\boldsymbol{\\beta}} \\end{equation}\\] Since we want to find the \\(\\hat{\\boldsymbol{\\beta}}\\) that minimizes (), will find the value of \\(\\hat{\\boldsymbol{\\beta}}\\) such that the derivative with respect to \\(\\hat{\\boldsymbol{\\beta}}\\) is equal to 0. \\[\\begin{equation} \\begin{aligned} \\frac{\\partial \\mathbf{e}^T\\mathbf{e}}{\\partial \\hat{\\boldsymbol{\\beta}}} &amp; = \\frac{\\partial}{\\partial \\hat{\\boldsymbol{\\beta}}}(\\mathbf{Y}^T\\mathbf{Y} - 2 \\mathbf{X}^T\\hat{\\boldsymbol{\\beta}}{}^T\\mathbf{Y} + \\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}}) = 0 \\\\[10pt] &amp;\\Rightarrow - 2 \\mathbf{X}^T\\mathbf{Y} + 2 \\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}} = 0 \\\\[10pt] &amp; \\Rightarrow 2 \\mathbf{X}^T\\mathbf{Y} = 2 \\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\\\[10pt] &amp; \\Rightarrow \\mathbf{X}^T\\mathbf{Y} = \\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\\\[10pt] &amp; \\Rightarrow (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\\\[10pt] &amp; \\Rightarrow (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y} = \\mathbf{I}\\hat{\\boldsymbol{\\beta}} \\end{aligned} \\end{equation}\\] Thus, the estimate of the model coefficients is \\(\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}\\). 5.4.4 Variance-covariance matrix of the coefficients We will use two properties to derive the form of the variance-covarinace matrix of the coefficients: \\(E[\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^T] = \\sigma^2I\\) \\(\\hat{\\boldsymbol{\\beta}} = \\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1}\\epsilon\\) First, we will show that \\(E[\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^T] = \\sigma^2I\\) \\[\\begin{equation} \\label{expected_error} \\begin{aligned} E[\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^T] &amp;= E \\begin{bmatrix}\\epsilon_1 &amp; \\epsilon_2 &amp; \\dots &amp; \\epsilon_n \\end{bmatrix}\\begin{bmatrix}\\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{bmatrix} \\\\[10pt] &amp; = E \\begin{bmatrix} \\epsilon_1^2 &amp; \\epsilon_1 \\epsilon_2 &amp; \\dots &amp; \\epsilon_1 \\epsilon_n \\\\ \\epsilon_2 \\epsilon_1 &amp; \\epsilon_2^2 &amp; \\dots &amp; \\epsilon_2 \\epsilon_n \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\epsilon_n \\epsilon_1 &amp; \\epsilon_n \\epsilon_2 &amp; \\dots &amp; \\epsilon_n^2 \\end{bmatrix} \\\\[10pt] &amp; = \\begin{bmatrix} E[\\epsilon_1^2] &amp; E[\\epsilon_1 \\epsilon_2] &amp; \\dots &amp; E[\\epsilon_1 \\epsilon_n] \\\\ E[\\epsilon_2 \\epsilon_1] &amp; E[\\epsilon_2^2] &amp; \\dots &amp; E[\\epsilon_2 \\epsilon_n] \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ E[\\epsilon_n \\epsilon_1] &amp; E[\\epsilon_n \\epsilon_2] &amp; \\dots &amp; E[\\epsilon_n^2] \\end{bmatrix} \\end{aligned} \\end{equation}\\] Recall, the regression assumption that the errors \\(\\epsilon_i&#39;s\\) are Normally distributed with mean 0 and variance \\(\\sigma^2\\). Thus, \\(E(\\epsilon_i^2) = Var(\\epsilon_i) = \\sigma^2\\) for all \\(i\\). Additionally, recall the regression assumption that the errors are uncorrelated, i.e. \\(E(\\epsilon_i \\epsilon_j) = Cov(\\epsilon_i, \\epsilon_j) = 0\\) for all \\(i,j\\). Using these assumptions, we can write () as \\[\\begin{equation} E[\\mathbf{\\epsilon}\\mathbf{\\epsilon}^T] = \\begin{bmatrix} \\sigma^2 &amp; 0 &amp; \\dots &amp; 0 \\\\ 0 &amp; \\sigma^2 &amp; \\dots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\dots &amp; \\sigma^2 \\end{bmatrix} = \\sigma^2 \\mathbf{I} \\end{equation}\\] where \\(\\mathbf{I}\\) is the \\(n \\times n\\) identity matrix. Next, we show that \\(\\hat{\\boldsymbol{\\beta}} = \\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1}\\epsilon\\). Recall that the \\(\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}\\) and \\(\\mathbf{Y} = \\mathbf{X}\\mathbf{\\beta} + \\mathbf{\\epsilon}\\). Then, \\[\\begin{equation} \\begin{aligned} \\hat{\\boldsymbol{\\beta}} &amp;= (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y} \\\\[10pt] &amp;= (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T(\\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}) \\\\[10pt] &amp;= \\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T \\mathbf{\\epsilon} \\\\ \\end{aligned} \\end{equation}\\] Using these two properties, we derive the form of the variance-covariance matrix for the coefficients. Note that the covariance matrix is \\(E[(\\hat{\\boldsymbol{\\beta}} - \\boldsymbol{\\beta})(\\hat{\\boldsymbol{\\beta}} - \\boldsymbol{\\beta})^T]\\) \\[\\begin{equation} \\begin{aligned} E[(\\hat{\\boldsymbol{\\beta}} - \\boldsymbol{\\beta})(\\hat{\\boldsymbol{\\beta}} - \\boldsymbol{\\beta})^T] &amp;= E[(\\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T \\boldsymbol{\\epsilon} - \\boldsymbol{\\beta})(\\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T \\boldsymbol{\\epsilon} - \\boldsymbol{\\beta})^T]\\\\[10pt] &amp; = E[(\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T \\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^T\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}] \\\\[10pt] &amp; = (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T E[\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^T]\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\\\[10pt] &amp; = (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T (\\sigma^2\\mathbf{I})\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\\\ &amp;= \\sigma^2\\mathbf{I}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\\\[10pt] &amp; = \\sigma^2\\mathbf{I}(\\mathbf{X}^T\\mathbf{X})^{-1}\\\\[10pt] &amp; = \\sigma^2(\\mathbf{X}^T\\mathbf{X})^{-1} \\\\ \\end{aligned} \\end{equation}\\] 5.5 NOTES: Log Transformations This document provides details about the model interpretion when the predictor and/or response variables are log-transformed. For simplicity, we will discuss transformations for the simple linear regression model: \\[\\begin{equation} \\label{orig} y = \\beta_0 + \\beta_1 x \\end{equation}\\] All results and interpretations can be easily extended to transformations in multiple regression models. Note: log refers to the natural logarithm. 5.5.1 Log-transformation on the response variable Suppose we fit a linear regression model with \\(\\log(y)\\), the log-transformed \\(y\\), as the response variable. Under this model, we assume a linear relationship exists between \\(x\\) and \\(\\log(y)\\), such that \\(\\log(y) \\sim N(\\beta_0 + \\beta_1 x, \\sigma^2)\\) for some \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\sigma^2\\). In other words, we can model the relationship between \\(x\\) and \\(\\log(y)\\) using the model in (). \\[\\begin{equation} \\label{log-y} \\log(y) = \\beta_0 + \\beta_1 x \\end{equation}\\] If we interpret the model in terms of \\(\\log(y)\\), then we can use the usual interpretations for slope and intercept. When reporting results, however, it is best to give all interpretations in terms of the original response variable \\(y\\), since interpretations using log-transformed variables are often more difficult to truly understand. In order to get back on the original scale, we need to use the exponential function (also known as the anti-log), \\(\\exp\\{x\\} = e^x\\). Therefore, we use the model in () for interpretations and predictions, we will use () to state our conclusions in terms of \\(y\\). \\[\\begin{equation} \\label{exp-y} \\begin{aligned} &amp;\\exp\\{\\log(y)\\} = \\exp\\{\\beta_0 + \\beta_1 x\\} \\\\[10pt] \\Rightarrow &amp;y = \\exp\\{\\beta_0 + \\beta_1 x\\} \\\\[10pt] \\Rightarrow &amp;y = \\exp\\{\\beta_0\\}\\exp\\{\\beta_1 x\\} \\end{aligned} \\end{equation}\\] In order to interpret the slope and intercept, we need to first understand the relationship between the mean, median and log transformations. 5.5.1.1 Mean, Median, and Log Transformations Suppose we have a dataset y that contains the following observations: If we log-transform the values of y then calculate the mean and median, we have If we calculate the mean and median of y, then log-transform the mean and median, we have This is a simple illustration to show \\(\\text{Mean}[{\\log(y)}] \\neq \\log[\\text{Mean}(y)]\\) - the mean and log are not commutable \\(\\text{Median}[\\log(y)] = \\log[\\text{Median}(y)]\\) - the median and log are commutable 5.5.1.2 Interpretaton of model coefficients Using (), the mean \\(\\log(y)\\) for any given value of \\(x\\) is \\(\\beta_0 + \\beta_1 x\\); however, this does not indicate that the mean of \\(y = \\exp\\{\\beta_0 + \\beta_1 x\\}\\) (see previous section). From the assumptions of linear regression, we assume that for any given value of \\(x\\), the distribution of \\(\\log(y)\\) is Normal, and therefore symmetric. Thus the median of \\(\\log(y)\\) is equal to the mean of \\(\\log(y)\\), i.e \\(\\text{Median}(\\log(y)) = \\beta_0 + \\beta_1 x\\). Since the log and the median are commutable, \\(\\text{Median}(\\log(y)) = \\beta_0 + \\beta_1 x \\Rightarrow \\text{Median}(y) = \\exp\\{\\beta_0 + \\beta_1 x\\}\\). Thus, when we log-transform the response variable, the interpretation of the intercept and slope are in terms of the effect on the median of \\(y\\). Intercept: The intercept is expected median of \\(y\\) when the predictor variable equals 0. Therefore, when \\(x=0\\), \\[\\begin{equation} \\begin{aligned} &amp;\\log(y) = \\beta_0 + \\beta_1 \\times 0 = \\beta_0 \\\\[10pt] \\Rightarrow &amp;y = \\exp\\{\\beta_0\\} \\end{aligned} \\end{equation}\\] Interpretation: When \\(x=0\\), the median of \\(y\\) is expected to be \\(\\exp\\{\\beta_0\\}\\). Slope: The slope is the expected change in the median of \\(y\\) when \\(x\\) increases by 1 unit. The change in the median of \\(y\\) is \\[\\begin{equation} \\exp\\{[\\beta_0 + \\beta_1 (x+1)] - [\\beta_0 + \\beta_1 x]\\} = \\frac{\\exp\\{\\beta_0 + \\beta_1 (x+1)\\}}{\\exp\\{\\beta_0 + \\beta_1 x\\}} = \\frac{\\exp\\{\\beta_0\\}\\exp\\{\\beta_1 x\\}\\exp\\{\\beta_1\\}}{\\exp\\{\\beta_0\\}\\exp\\{\\beta_1 x\\}} = \\exp\\{\\beta_1\\} \\end{equation}\\] Thus, the median of \\(y\\) for \\(x+1\\) is \\(\\exp\\{\\beta_1\\}\\) times the median of \\(y\\) for \\(x\\). Interpretation: When \\(x\\) increases by one unit, the median of \\(y\\) is expected to multiply by a factor of \\(\\exp\\{\\beta_1\\}\\). 5.5.2 Log-transformation on the predictor variable Suppose we fit a linear regression model with \\(\\log(x)\\), the log-transformed \\(x\\), as the predictor variable. Under this model, we assume a linear relationship exists between \\(\\log(x)\\) and \\(y\\), such that \\(y \\sim N(\\beta_0 + \\beta_1 \\log(x), \\sigma^2)\\) for some \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\sigma^2\\). In other words, we can model the relationship between \\(\\log(x)\\) and \\(y\\) using the model in (). \\[\\begin{equation} \\label{log-x} y = \\beta_0 + \\beta_1 \\log(x) \\end{equation}\\] Intercept: The intercept is the mean of \\(y\\) when \\(\\log(x) = 0\\), i.e. \\(x = 1\\). Interpretation: When \\(x = 1\\) \\((\\log(x) = 0)\\), the mean of \\(y\\) is expected to be \\(\\beta_0\\). Slope: The slope is interpreted in terms of the change in the mean of \\(y\\) when \\(x\\) is mutliplied by a factor of \\(C\\), since \\(\\log(Cx) = \\log(x) + \\log(C)\\). Thus, when \\(x\\) is multiplied by a factor of \\(C\\), the change in the mean of \\(y\\) is \\[\\begin{equation} \\begin{aligned} [\\beta_0 + \\beta_1 \\log(Cx)] - [\\beta_0 + \\beta_1 \\log(x)] &amp;= \\beta_1 [\\log(Cx) - \\log(x)] \\\\[10pt] &amp; = \\beta_1[\\log(C) + \\log(x) - \\log(x)] \\\\[10pt] &amp; = \\beta_1 \\log(C) \\end{aligned} \\end{equation}\\] Thus the mean of \\(y\\) changes by \\(\\beta_1 \\log(C)\\) units. Interpretation: When \\(x\\) is multiplied by a factor of \\(C\\), the mean of \\(y\\) is expected to change by \\(\\beta_1 \\log(C)\\) units. For example, if \\(x\\) is doubled, then the mean of \\(y\\) is expected to change by \\(\\beta_1 \\log(2)\\) units. 5.5.3 Log-transformation on the the response and predictor variable Suppose we fit a linear regression model with \\(\\log(x)\\), the log-transformed \\(x\\), as the predictor variable and \\(\\log(y)\\), the log-transformed \\(y\\), as the response variable. Under this model, we assume a linear relationship exists between \\(\\log(x)\\) and \\(\\log(y)\\), such that \\(\\log(y) \\sim N(\\beta_0 + \\beta_1 \\log(x), \\sigma^2)\\) for some \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\sigma^2\\). In other words, we can model the relationship between \\(\\log(x)\\) and \\(\\log(y)\\) using the model in (). \\[\\begin{equation} \\label{log-x-y} \\log(y) = \\beta_0 + \\beta_1 \\log(x) \\end{equation}\\] Because the response variable is log-transformed, the interpretations on the original scale will be in terms of the median of \\(y\\) (see the section on the log-transformed response variable for more detail). Intercept: The intercept is the mean of \\(y\\) when \\(\\log(x) = 0\\), i.e. \\(x = 1\\). Therefore, when \\(\\log(x) = 0\\), \\[\\begin{equation} \\begin{aligned} &amp;\\log(y) = \\beta_0 + \\beta_1 \\times 0 = \\beta_0 \\\\[10pt] \\Rightarrow &amp;y = \\exp\\{\\beta_0\\} \\end{aligned} \\end{equation}\\] Interpretation: When \\(x = 1\\) \\((\\log(x) = 0)\\), the median of \\(y\\) is expected to be \\(\\exp\\{\\beta_0\\}\\). Slope: The slope is interpreted in terms of the change in the median \\(y\\) when \\(x\\) is mutliplied by a factor of \\(C\\), since \\(\\log(Cx) = \\log(x) + \\log(C)\\). Thus, when \\(x\\) is multiplied by a factor of \\(C\\), the change in the median of \\(y\\) is \\[\\begin{equation} \\begin{aligned} \\exp\\{[\\beta_0 + \\beta_1 \\log(Cx)] - [\\beta_0 + \\beta_1 \\log(x)]\\} &amp;= \\exp\\{\\beta_1 [\\log(Cx) - \\log(x)]\\} \\\\[10pt] &amp; = \\exp\\{\\beta_1[\\log(C) + \\log(x) - \\log(x)]\\} \\\\[10pt] &amp; = \\exp\\{\\beta_1 \\log(C)\\} = C^{\\beta_1} \\end{aligned} \\end{equation}\\] Thus, the median of \\(y\\) for \\(Cx\\) is \\(C^{\\beta_1}\\) times the median of \\(y\\) for \\(x\\). Interpretation: When \\(x\\) is multiplied by a factor of \\(C\\), the median of \\(y\\) is expected to multiple by a factor of \\(C^{\\beta_1}\\). For example, if \\(x\\) is doubled, then the median of \\(y\\) is expected to multiply by \\(2^{\\beta_1}\\). 5.6 NOTES: Model Diagnostics This document discusses some of the mathematical details of the model diagnostics - leverage, standardized residuals, and Cook’s distance. We assume the reader knowledge of the matrix form for multiple linear regression.Please see Matrix Form of Linear Regression for a review. 5.6.1 Introduction Suppose we have \\(n\\) observations. Let the \\(i^{th}\\) be \\((x_{i1}, \\ldots, x_{ip}, y_i)\\), such that \\(x_{i1}, \\ldots, x_{ip}\\) are the explanatory variables (predictors) and \\(y_i\\) is the response variable. We assume the data can be modeled using the least-squares regression model, such that the mean response for a given combination of explanatory variables follows the form in (). \\[\\begin{equation} \\label{basic_model} y = \\beta_0 + \\beta_1 x_1 + \\dots + \\beta_p x_p \\end{equation}\\] We can write the response for the \\(i^{th}\\) observation as shown in () \\[\\begin{equation} \\label{ind_response} y_i = \\beta_0 + \\beta_1 x_{i1} + \\dots + \\beta_p x_{ip} + \\epsilon_i \\end{equation}\\] such that \\(\\epsilon_i\\) is the amount \\(y_i\\) deviates from \\(\\mu\\{y|x_{i1}, \\ldots, x_{ip}\\}\\), the mean response for a given combination of explanatory variables. We assume each \\(\\epsilon_i \\sim N(0,\\sigma^2)\\), where \\(\\sigma^2\\) is a constant variance for the distribution of the response \\(y\\) for any combination of explanatory variables \\(x_1, \\ldots, x_p\\). 5.6.2 Matrix Form for the Regression Model We can represent the () and () using matrix notation. Let \\[\\begin{equation} \\label{matrix notation} \\mathbf{Y} = \\begin{bmatrix}y_1 \\\\ y_2 \\\\ \\vdots \\\\y_n\\end{bmatrix} \\hspace{15mm} \\mathbf{X} = \\begin{bmatrix}x_{11} &amp; x_{12} &amp; \\dots &amp; x_{1p} \\\\ x_{21} &amp; x_{22} &amp; \\dots &amp; x_{2p} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots\\\\ x_{n1} &amp; x_{n2} &amp; \\dots &amp; x_{np} \\end{bmatrix} \\hspace{15mm} \\boldsymbol{\\beta}= \\begin{bmatrix}\\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_p \\end{bmatrix} \\hspace{15mm} \\boldsymbol{\\epsilon}= \\begin{bmatrix}\\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{bmatrix} \\end{equation}\\] Thus, \\[\\mathbf{Y} = \\mathbf{X}\\boldsymbol{\\beta} + \\mathbf{\\epsilon}\\] Therefore the estimated response for a given combination of explanatory variables and the associated residuals can be written as \\[\\begin{equation} \\label{matrix_mean} \\hat{\\mathbf{Y}} = \\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\hspace{10mm} \\mathbf{e} = \\mathbf{Y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\end{equation}\\] 5.6.3 Hat Matrix &amp; Leverage Recall from the notes Matrix Form of Linear Regression that \\(\\hat{\\boldsymbol{\\beta}}\\) can be written as the following: \\[\\begin{equation} \\label{beta-hat} \\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y} \\end{equation}\\] Combining () and (), we can write \\(\\hat{\\mathbf{Y}}\\) as the following: \\[\\begin{equation} \\label{y-hat} \\begin{aligned} \\hat{\\mathbf{Y}} &amp;= \\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\\\[10pt] &amp;= \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}\\\\ \\end{aligned} \\end{equation}\\] We define the hat matrix as an \\(n \\times n\\) matrix of the form \\(\\mathbf{H} = \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\). Thus () becomes \\[\\begin{equation} \\label{y-hat-matrix} \\hat{\\mathbf{Y}} = \\mathbf{H}\\mathbf{Y} \\end{equation}\\] The diagonal elements of the hat matrix are a measure of how far the predictor variables of each observation are from the means of the predictor variables. For example, \\(h_{ii}\\) is a measure of how far the values of the predictor variables for the \\(i^{th}\\) observation, \\(x_{i1}, x_{i2}, \\ldots, x_{ip}\\), are from the mean values of the predictor variables, \\(\\bar{x}_1, \\bar{x}_2, \\ldots, \\bar{x}_p\\). In the case of simple linear regression, the \\(i^{th}\\) diagonal, \\(h_{ii}\\), can be written as \\[h_{ii} = \\frac{1}{n} + \\frac{(x_i - \\bar{x})^2}{\\sum_{j=1}^{n}(x_j-\\bar{x})^2}\\] We call these diagonal elements, the leverage of each observation. The diagonal elements of the hat matrix have the following properties: \\(0 \\leq h_ii \\leq 1\\) \\(\\sum\\limits_{i=1}^{n} h_{ii} = p+1\\), where \\(p\\) is the number of predictor variables in the model. The mean hat value is \\(\\bar{h} = \\frac{\\sum\\limits_{i=1}^{n} h_{ii}}{n} = \\frac{p+1}{n}\\). Using these properties, we consider a point to have high leverage if it has a leverage value that is more than 2 times the average. In other words, observations with leverage greater than \\(\\frac{2(p+1)}{n}\\) are considered to be high leverage points, i.e. outliers in the predictor variables. We are interested in flagging high leverage points, because they may have an influence on the regression coefficients. When there are high leverage points in the data, the regression line will tend towards those points; therefore, one property of high leverage points is that they tend to have small residuals. We will show this by rewriting the residuals from () using (). \\[\\begin{equation} \\label{resid-hat} \\begin{aligned} \\mathbf{e} &amp;= \\mathbf{Y} - \\hat{\\mathbf{Y}} \\\\[10pt] &amp; = \\mathbf{Y} - \\mathbf{H}\\mathbf{Y} \\\\[10pt] &amp;= (1-\\mathbf{H})\\mathbf{Y} \\end{aligned} \\end{equation}\\] Note that the identity matrix and hat matrix are idempotent, i.e. \\(\\mathbf{I}\\mathbf{I} = \\mathbf{I}\\), \\(\\mathbf{H}\\mathbf{H} = \\mathbf{H}\\). Thus, \\((\\mathbf{I} - \\mathbf{H}\\) is also idempotent. These matrices are also symmetric. Using these properties and (), we have that the variance-covariance matrix of the residuals \\(\\boldsymbol{e}\\), is \\[\\begin{equation} \\label{resid-var} \\begin{aligned} Var(\\mathbf{e}) &amp;= \\mathbf{e}\\mathbf{e}^T \\\\[10pt] &amp;= (1-\\mathbf{H})Var(\\mathbf{Y})^T(1-\\mathbf{H})^T \\\\[10pt] &amp;= (1-\\mathbf{H})\\hat{\\sigma}^2(1-\\mathbf{H})^T \\\\[10pt] &amp;= \\hat{\\sigma}^2(1-\\mathbf{H})(1-\\mathbf{H}) \\\\[10pt] &amp;= \\hat{\\sigma}^2(1-\\mathbf{H}) \\end{aligned} \\end{equation}\\] where \\(\\hat{\\sigma}^2 = \\frac{\\sum_{i=1}^{n}e_i^2}{n-p-1}\\) is the estimated regression variance. Thus, the variance of the \\(i^{th}\\) residual is \\(Var(e_i) = \\hat{\\sigma}^2(1-h_{ii})\\). Therefore, the higher the leverage, the smaller the variance of the residual. Because the expected value of the residuals is 0, we conclude that points with high leverage tend to have smaller residuals than points with lower leverage. 5.6.4 Standardized Residuals In general, we standardize a value by shifting by the expected value and rescaling by the standard deviation (or standard error). Thus, the \\(i^{th}\\) standardized residual takes the form \\[std.res_i = \\frac{e_i - E(e_i)}{SE(e_i)}\\] The expected value of the residuals is 0, i.e. \\(E(e_i) = 0\\). From (), the standard error of the residual is \\(SE(e_i) = \\hat{\\sigma}\\sqrt{1-h_{ii}}\\). Therefore, \\[\\begin{equation} \\label{std.resid.} std.res_i = \\frac{e_i}{\\hat{\\sigma}\\sqrt{1-h_{ii}}} \\end{equation}\\] 5.6.5 Cook’s Distance Cook’s distance is a measure of how much each observation influences the model coefficients, and thus the predicted values. The Cook’s distance for the \\(i^{th}\\) observation can be written as \\[\\begin{equation} \\label{cooksd} D_i = \\frac{(\\hat{\\mathbf{Y}} -\\hat{\\mathbf{Y}}_{(i)})^T(\\hat{\\mathbf{Y}} -\\hat{\\mathbf{Y}}_{(i)})}{(p+1)\\hat{\\sigma}} \\end{equation}\\] where \\(\\hat{\\mathbf{Y}}_{(i)}\\) is the vector of predicted values from the model fitted when the \\(i^{th}\\) observation is deleted. Cook’s Distance can be calculated without deleting observations one at a time, since () below is mathematically equivalent to (). \\[\\begin{equation} \\label{cooksd-v2} D_i = \\frac{1}{p+1}std.res_i^2\\Bigg[\\frac{h_{ii}}{(1-h_{ii})}\\Bigg] = \\frac{e_i^2}{(p+1)\\hat{\\sigma}^2(1-h_{ii})}\\Bigg[\\frac{h_{ii}}{(1-h_{ii})}\\Bigg] \\end{equation}\\] "],
["6-select.html", " 6 Model Selection 6.1 Model Selection 6.2 Model Selection 6.3 NOTES: Model Selection Criteria", " 6 Model Selection 6.1 Model Selection The goal of today’s lab is to practice forward and backward model selection. In addition to practice with model selection functions in R, you will manually conduct a backward selection procedure to better understand what occurs when you use model selection functions. 6.1.1 Packages You will need the following packages for today’s lab: library(tidyverse) library(knitr) library(broom) library(leaps) library(Sleuth3) #case1201 data library(ISLR) #Hitters data 6.1.2 Data There are two datasets for this lab. 6.1.2.1 Part I The dataset for Part I contains the SAT score (out of 1600) and other variables that may be associated with SAT performance for each of the 50 states in the U.S. The data is based on test takers for the 1982 exam. The following variables are in the dataset: SAT: average total SAT score State: U.S. State Takers: percentage of high school seniors who took exam Income: median income of families of test-takers ($ hundreds) Years: average number of years test-takers had formal education in social sciences, natural sciences, and humanities Public: percentage of test-takers who attended public high schools Expend: total state expenditure on high schools ($ hundreds per student) Rank: median percentile rank of test-takers within their high school classes 6.1.2.2 Part II The dataset for Part II contains the performance statistics and salaries of Major League Baseball players in the 1986 and 1987 seasons. The data is in the Hitters dataset in the ISLR package. Type ?Hitters in the console to see the variables names and their definitions. 6.1.3 Exercises 6.1.3.1 Part I For the first part of the lab, you will return to the model selection activity you started in class using the SAT data. The data is in the case1201 data frame in the Sleuth3 package. sat_scores &lt;- case1201 %&gt;% select(-State) Manually perform backward selection using \\(Adj. R^2\\) as the selection criterion. To help you get started, the full model and the code for the first set of models to test are below. Show each step of the selection process. Display the coefficients and \\(Adj. R^2\\) of your final model. full_model &lt;- lm(SAT ~ ., data = sat_scores) m1 &lt;- lm(SAT ~ Income + Years + Public + Expend + Rank, data = sat_scores) m2 &lt;- lm(SAT ~ Takers + Years + Public + Expend + Rank, data = sat_scores) m3 &lt;- lm(SAT ~ Takers + Income + Public + Expend + Rank, data = sat_scores) m4 &lt;- lm(SAT ~ Takers + Income + Years + Expend + Rank, data = sat_scores) m5 &lt;- lm(SAT ~ Takers + Income + Years + Public + Rank, data = sat_scores) m6 &lt;- lm(SAT ~ Takers + Income + Years + Public + Expend, data = sat_scores) What is the best 5-variable model? Display the model output. Use the regsubsets function to perform backward selection. What is the final model when \\(Adj. R^2\\) is the selection criterion? Display the coefficients and the \\(Adj. R^2\\) of the final model. This should be the same result you got in Exercise 1. What is the final model when \\(BIC\\) is the selection criterion? Display the coefficients and the \\(BIC\\) of the final model. Compare the final models selected by \\(Adj. R^2\\) and \\(BIC\\). Do the models have the same number of predictors? Briefly explain. Are the same predictor variables in each model? Briefly explain. Consider the comparisons made in the previous exercise. Are these differences what you would expect given the selection criteria used? Briefly explain. 6.1.3.2 Part II The data for this part of the lab is the Hitters dataset in the ISLR package. Your goal is to fit a regression model that uses the performance statistics of baseball players to predictor their salary. There are 19 potential predictor variables, so you will use the regsubsets function to conduct forward selection to choose a final model. Read through the data dictionary for the Hitters dataset. You can access it by typing ?Hitters in the console. What is the difference between the variables HmRun and CHmRun? Some observations have missing values for Salary. Filter the data, so only observations that have values for Salary are included. You will use this filtered data for the remainder of the lab. Fill in the code below to conduct forward selection and save the results in an object called sel_summary (selection summary). # The `nvmax` option indicates the maximum-sized variable subsets to consider in the model selection. regfit_forward &lt;- regsubsets(_______, ________, method=&quot;forward&quot;, nvmax = 19) sel_summary &lt;- summary(_______) The object sel_summary contains the summary statistics for the best fit model containing \\(k\\) predictors, where \\(k = 1, \\ldots, 19\\). The object sel_summary is a list object, so it is cumbersome to extract the relevant summary statistics. Therefore, you can create a data frame called summary_stats such that each row represents the best fit model with \\(k\\) predictors and each column is a summary statistic. For example, the second row contains the summary statistics of the best fit model that contains 2 variables. Fill in the code below to create the data frame summary_stats that includes the \\(BIC\\), \\(R^2\\), \\(Adj. R^2\\), and residual sum of squares (RSS) for each model in sel_summary. The data frame summary_stats will also include the column np, the number of predictors in the model represented on each row. summary_stats &lt;- data.frame(bic = sel_summary$bic, adjr2 = _______, rsq = _______, rss = _______) %&gt;% mutate(np = row_number()) #number of variables # See the [ggplot2 documentation](https://ggplot2.tidyverse.org/reference/geom_abline.html#arguments) for code to add a vertical line. Use the data in the summary_stats data frame to plot \\(BIC\\) versus the number of predictors. Include a vertical line on your plot that shows the number of predictors for the overall final model you would select based on \\(BIC\\). Be sure your plot has clear and informative title and axes labels. How does \\(BIC\\) change as the number of predictors increases? How many predictors are in the final model selected based on \\(BIC\\)? You can fill in the code below with either max or min to find the number of predictors in the final model selected based on \\(BIC\\). np_bic &lt;- summary_stats %&gt;% filter(bic == _____(bic)) %&gt;% select(np) %&gt;% pull() Use the data in the summary_stats data frame to plot \\(Adj. R^2\\) versus the number of predictors. Include a vertical line on your plot that shows the number of predictors for the final model you would select based on \\(Adj. R^2\\). Be sure your plot has clear and informative title and axes labels. How does \\(Adj. R^2\\) change as the number of predictors increases? How many predictors are in the final model selected based on \\(Adj. R^2\\)? Use the data in the summary_stats data frame to plot \\(R^2\\) versus the number of predictors. Include a vertical line on your plot that shows the number of predictors for the final model selected based on \\(R^2\\). Be sure your plot has clear and informative title and axes labels. How does \\(R^2\\) change as the number of predictors increases? How many predictors are in the final model selected based on \\(R^2\\)? Should \\(R^2\\) be used as a model selection criterion? Briefly explain why or why not using your answers to Exercises 11 - 13. Choose a final model to predict a baseball player’s Salary from his performance statistics. Display the variables, their coefficients, and the summary statistics from the summary_stats data frame for this model. Briefly explain why you chose the model in the previous exercise. Which model selection criteria did you use (\\(BIC\\), \\(Adj. R^2\\), \\(R^2\\))? Why? What other factors did you consider besides the value of the model selection criteria? 6.1.4 Acknowledgements Part II of this lab was inspired by Lab 6.5 in An Introduction to Statistical Learning and Variable Selection in Regression. 6.2 Model Selection library(tidyverse) library(knitr) library(broom) library(Sleuth3) library(leaps) sat_scores &lt;- case1201 %&gt;% select(-State) #remove the state variable 6.2.1 Backward selection “manually” Manually perform backward selection using Adj. \\(R^2\\) as the selection criteria. Show each step of the selection process. To help you get started, the full model and the code for the first set of models to test are below. You will need to find Adj. \\(R^2\\) for each model. full_model &lt;- lm(SAT ~ ., data = sat_scores) m1 &lt;- lm(SAT ~ Income + Years + Public + Expend + Rank, data = sat_scores) m2 &lt;- lm(SAT ~ Takers + Years + Public + Expend + Rank, data = sat_scores) m3 &lt;- lm(SAT ~ Takers + Income + Public + Expend + Rank, data = sat_scores) m4 &lt;- lm(SAT ~ Takers + Years + Income + Expend + Rank, data = sat_scores) m5 &lt;- lm(SAT ~ Takers + Years + Public + Income + Expend, data = sat_scores) m6 &lt;- lm(SAT ~ Takers + Years + Public + Income + Rank, data = sat_scores) Continue the model selection until you have a final model. Show each step of the model selection process. 6.2.2 Backward selection using regsubsets Use the regsubsets function to perform backward selection using Adj. \\(R^2\\) as the selection criteria. Are the variables the same as the ones at you chose? Is the Adj. \\(R^2\\) the same? 6.2.3 Changing selection criteria Use the regsubsets function to perform backward selection using BIC as the selection criteria. What variables were chosen for the follow model? How does this model compare to the one selected using Adj. \\(R^2\\)? Use the step function to perform backward selection using AIC as the selection criteria. What variables were chosen for the follow model? How does this model compare to the models chosen from the other selection criteria? 6.2.4 Different selection procedure Use forward or stepwise selection to choose a model. Choose the criteria you will use to select the model. How does this model compare to the previous selected models? 6.2.5 Choosing a final model You likely have at least 2 different models chosen by the various model selection procedures. Which variables will you include in your final model? Why did you choose this to be your final model? 6.3 NOTES: Model Selection Criteria This document discusses some of the mathematical details of Akaike’s Information Criterion (AIC) and Schwarz’s Bayesian Information Criterion (BIC). We assume the reader knowledge of the matrix form for multiple linear regression.Please see Matrix Form of Linear Regression for a review. 6.3.1 Maximum Likelihood Estimation of \\(\\boldsymbol{\\beta}\\) and \\(\\sigma\\) To understand the formulas for AIC and BIC, we will first briefly explain the likelihood function and maximum likelihood estimates for regression. Let \\(\\mathbf{Y}\\) be \\(n \\times 1\\) matrix of responses, \\(\\mathbf{X}\\), the \\(n \\times (p+1)\\) matrix of predictors, and \\(\\boldsymbol{\\beta}\\), \\((p+1) \\times 1\\) matrix of coefficients. If the multiple linear regression model is correct then, \\[\\begin{equation} \\label{norm-assumption} \\mathbf{Y} \\sim N(\\mathbf{X}\\boldsymbol{\\beta}, \\sigma^2) \\end{equation}\\] When we do linear regression, our goal is to estimate the unknown parameters \\(\\boldsymbol{\\beta}\\) and \\(\\sigma^2\\) from (). In Matrix Form of Linear Regression, we showed a way to estimate these parameters using matrix alegbra. Another approach for estimating \\(\\boldsymbol{\\beta}\\) and \\(\\sigma^2\\) is using maximum likelihood estimation. A likelihood function is used to summarise the evidence from the data in support of each possible value of a model parameter. Using (), we will write the likelihood function for linear regression as \\[\\begin{equation} \\label{lr} L(\\mathbf{X}, \\mathbf{Y}|\\boldsymbol{\\beta}, \\sigma^2) = \\prod\\limits_{i=1}^n (2\\pi \\sigma^2)^{-\\frac{1}{2}} \\exp\\bigg\\{-\\frac{1}{2\\sigma^2}\\sum\\limits_{i=1}^n(Y_i - \\mathbf{X}_i \\boldsymbol{\\beta})^T(Y_i - \\mathbf{X}_i \\boldsymbol{\\beta})\\bigg\\} \\end{equation}\\] where \\(Y_i\\) is the \\(i^{th}\\) response and \\(\\mathbf{X}_i\\) is the vector of predictors for the \\(i^{th}\\) observation. One approach estimating \\(\\boldsymbol{\\beta}\\) and \\(\\sigma^2\\) is to find the values of those parameters that maximize the likelihood in (), i.e. maximum likelhood estimation. To make the calculations more manageable, instead of maximizing the likelihood function, we will instead maximize its logarithm, i.e. the log-likelihood function. The values of the parameters that maximize the log-likelihood function are those that maximize the likelihood function. The log-likelihood function we will maximize is \\[\\begin{equation} \\label{logL} \\begin{aligned} \\log L(\\mathbf{X}, \\mathbf{Y}|\\boldsymbol{\\beta}, \\sigma^2) &amp;= \\sum\\limits_{i=1}^n -\\frac{1}{2}\\log(2\\pi\\sigma^2) -\\frac{1}{2\\sigma^2}\\sum\\limits_{i=1}^n(Y_i - \\mathbf{X}_i \\boldsymbol{\\beta})^T(Y_i - \\mathbf{X}_i \\boldsymbol{\\beta}) \\\\[10pt] &amp;= -\\frac{n}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}(\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta})^T(\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta})\\\\ \\end{aligned} \\end{equation}\\] [–insert details MLES–] The maximum likelihood estimate of \\(\\boldsymbol{\\beta}\\) and \\(\\sigma^2\\) are \\[\\begin{equation} \\label{mle} \\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y} \\hspace{10mm} \\hat{\\sigma}^2 = \\frac{1}{n}(\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta})^T(\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta}) = \\frac{1}{n}RSS \\end{equation}\\] where \\(RSS\\) is the residual sum of squares. Note that the maximum likelihood estimate is not exactly equal to the estimate of \\(\\sigma^2\\) we typically use \\(\\frac{RSS}{n-p-1}\\). This is because the maximum likelihood estimate of \\(\\sigma^2\\) in () is a biased estimator of \\(\\sigma^2\\). When \\(n\\) is much larger than the number of predictors \\(p\\), then the differences in these two estimates are trivial. 6.3.2 AIC Akaike’s Information Criterion (AIC) is \\[\\begin{equation} \\label{aic} AIC = -2 \\log L + 2(p+1) \\end{equation}\\] where \\(\\log L\\) is the log-likelihood. This is the general form of AIC that can be applied to a variety of models, but for now, let’s focus on AIC for mutliple linear regression. \\[\\begin{equation} \\label{aic-reg} \\begin{aligned} AIC &amp;= -2 \\log L + 2(p+1) \\\\[10pt] &amp;= -2\\bigg[-\\frac{n}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}(\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta})^T(\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta})\\bigg] + 2(p+1) \\\\[10pt] &amp;= n\\log\\big(2\\pi\\frac{RSS}{n}\\big) + \\frac{1}{RSS/n}RSS \\\\[10pt] &amp;= n\\log(2\\pi) + n\\log(RSS) - n\\log(n) + 2(p+1) \\end{aligned} \\end{equation}\\] 6.3.3 BIC [—] "],
["7-logistic.html", " 7 Logistic Regression 7.1 Logistic Regression 7.2 Logistic Regression", " 7 Logistic Regression 7.1 Logistic Regression Over the past ten years, recommendation systems have become increasingly popular as more companies strive to offer customized user experiences. Amazon recommends products you may like based on your browse and purchase history, Netflix recommends movies and TV shows based on your viewing history, and music platforms like Spotify recommend songs you may like based on your listening history. While these recommendation systems are built using a variety of algorithms, they are all trying to achieve the same goal: use the characteristics of the products/movies/music a user is known to like to figure out the products/movies/music the user may like but hasn’t discovered yet. # See [&quot;How Does Spotify Know You So Well?&quot;](https://medium.com/s/story/spotifys-discover-weekly-how-machine-learning-finds-your-new-music-19a41ab76efe) for more information about Spotify&#39;s recommendation algorithms. In today’s lab, we will focus on using the characteristics of songs a user previously played to determine whether or not a user will like a new song. We will use logistic regression to build a model that predicts the probability a user likes a song using the relevant characteristics of that song. 7.1.1 Packages You will need the following packages for today’s lab: library(tidyverse) library(broom) ## Fill in other packages as needed 7.1.2 Data The data in this lab is from the Spotify Song Attributes data set in Kaggle. This data set contains song characteristics of 2017 songs played by a single user and whether or not he liked the song. Since this dataset contains the song preferences of a single user, the scope of the analysis is limited to this particular user. You will use data spotify.csv to build the logistic regression model and test the performance of the model using the songs in test_songs.csv. Click here to download the dataset spotify.csv, and click here to download the dataset test_songs.csv. Upload both files to the to the data folder in your lab-07 project. The Spotify documentation page contains a description of the variables included in this dataset. 7.1.3 Exercises 7.1.3.1 Exploratory Data Analysis Read through the Spotify documentation page to learn more about the variables in the dataset. The response variable for this analysis is like, such that 1 indicates that the user likes the song and 0 otherwise. The remaining will be considered as predictor variables in the model. # Part of the code to make `x` a factor. # mutate(x = factor(x)) - Which potential predictor variables are categorical? You only need to include the variables that are in the dataset. - Recode the each of the categorical predictors so they are a `factor` variable type. Choose a quantitative predictor variable. Make the appropriate plot of the response versus this predictor variable. Describe the relationship between the two variables. Choose a categorical predictor variable. Make the appropriate plot of the response versus this predictor variable. Describe the relationship between the two variables. Let’s consider a potential interaction effect between the variables you choose in Exercises 2 and 3. Make the appropriate plots to examine the potential interaction effect. Do these plots suggest there is a significant interaction effect? Briefly explain. In practice, you should do exploratory data analysis for all potential explanatory variables. We did an abbreviated exploratory data analysis to make the assignment more manageable. 7.1.3.2 Part II: Logistic Regression Model Fit the full model and display the model output. The main objective for the model is to predict whether the user will like a song. Should we use this model for this objective? Briefly explain. Use the step function to perform backward selection. Display the output for the selected model. Briefly describe the criteria used by step to select the final model. For the remainder of this lab, you will use the model chosen by model selection . In practice; however, you would not just stop with the results from the automated model selection procedure and would examine the model further to see if there are any significant interactions, higher-order terms, or if it could even be simplified. Consider the variable duration_ms. Interpret the coefficient of duration_ms and its 95% confidence interval in terms of the odds of the user liking a song. Suppose instead of duration_ms, we use the variable duration_s, the duration of a song in seconds. What would be the effect of duration_s on the odds of the user liking a song? Include the updated coefficient and corresponding 95% confidence interval for duration_s. Assume all other variables in the model are unchanged. Interpret mode and its 95% confidence interval in terms of the odds of the user liking a song. Based on this model, is there evidence of a significant difference in the user’s preference between songs in a major key versus those in a minor key? 7.1.3.3 Part III: Model Assessment In the next few questions, we will do an abbreviated analysis of the residuals. Create a binned plot of the residuals versus the predicted probabilities. You will first need to use the augment function with the options type.predict = &quot;response&quot; and type.residuals = &quot;response&quot; to get the predicted probabilities and corresponding residuals. Choose a quantitative predictor in the final model. Make the appropriate table or plot to examine the residuals versus this predictor variable. Choose a categorical predictor in the final model. Make the appropriate table or plot to examine the residuals versus this predictor variable. In practice, you should examine plots of residuals versus every predictor variable to make a complete assessment of the model fit. For the sake of time on the lab, you will use these three plots to help make the assessment in Exercise 14. Plot the ROC curve and find the area under the curve. Based on the residual plots and the ROC curve, is this logistic model a good fit for the data? Briefly explain. 7.1.3.4 Part IV: Prediction You are part of the data science team at Spotify, and your model will be used to make song recommendations to users. The goal is to recommend songs the user has a high probability of liking. As a group, choose a threshold value to distinguish between songs the user will like and those the user won’t like. What is your threshold value? Use the ROC curve to help justify your choice. Now let’s put your model and decision threshold to the test! Use your model to calculate the predicted probability that the user will like the following two songs: “Sign of the Times” by Harry Styles “Hotline Bling” by Drake The data for the songs can be found in test_songs.csv. Using your decision threshold from Question 15, would you recommend “Sign of the Times” to this user? Would your recommend “Hotline Bling” to this user? Briefly explain your decision. The user likes “Hotline Bling” but doesn’t like “Sign of the Times”. How good were your recommendations based on these two songs? If they were good recommendations, explain how the model and threshold helped you distinguish between songs the user would like and those he wouldn’t. If they were not good recommendations, explain the limitations in your model and/or threshold. 7.2 Logistic Regression The goal of this exercise is to walk through a logistic regression analysis. It will give you a basic idea of the analysis steps and thought-process; however, due to class time constraints, this analysis is not exhaustive. library(tidyverse) library(broom) library(rms) ## add any other packages as needed This data is from an ongoing cardiovascular study on residents of the town of Framingham, Massachusetts. The goal is to predict whether a patient has a 10-year risk of future coronary heart disease. The dataset includes the following: male: 0 = Female; 1 = Male age: Age at exam time. education: 1 = Some High School; 2 = High School or GED; 3 = Some College or Vocational School; 4 = College currentSmoker: 0 = nonsmoker; 1 = smoker cigsPerDay: number of cigarettes smoked per day (estimated average) BPMeds: 0 = Not on Blood Pressure medications; 1 = Is on Blood Pressure medications prevalentStroke prevalentHyp diabetes: 0 = No; 1 = Yes totChol: total cholesterol (mg/dL) sysBP: systolic blood pressure (mmHg) diaBP: diastolic blood pressure (mmHg) BMI: BodyMass Index calculated as: Weight (kg) / Height(meter-squared) heartRate Beats/Min (Ventricular) glucose: total glucose mg/dL TenYearCHD: 0 = Patient doesn’t have 10-year risk of future coronary heart disease; 1 = Patient has 10-year risk of future coronary heart disease; fram_data &lt;- read_csv(&quot;data/framingham.csv&quot;) %&gt;% drop_na() %&gt;% mutate(education = case_when( education == 1 ~ &quot;Some HS&quot;, education == 2 ~ &quot;HS or GED&quot;, education == 3 ~ &quot;Some College&quot;, education == 4 ~ &quot;College&quot; ), currentSmoker = if_else(currentSmoker == 0, &quot;nonsmoker&quot;, &quot;smoker&quot;), diabetes = if_else(diabetes == 0,&quot;No&quot;, &quot;Yes&quot;), male = factor(male) ) Fit a full model (main effects only) with TenYearCHD as the response. Display the model output. Based on the goal of the analysis, should the full model be the final model? Why or why not? Use the step function to conduct backward model selection. What is selection criteria used by the step function? Display the final model. There is reason to believe that the factors related to coronary heart disease may have different effects for men and women. We would like to include this information in the model. Use the drop-in-deviance test to test at least three interactions with male. Which interactions did you choose? Why? Include the output from the tests. Use the results from model selection and the drop-in-deviance test to select a final model. Display the model below. Plot and analyze the binned residuals for the final model. Include all appropriate plots. What is your assessment on the model fit based on these plots? Plot and analyze the ROC curve. Based on the ROC curve, does the model fit the data well? A doctor plans to use the results from your model to help select patients for a new heart disease prevention program. She asks you which threshold would be best to select patients for this program. What threshold would you recommend to the doctor? Why? 7.2.1 References Data obtained from https://www.kaggle.com/neisha/heart-disease-prediction-using-logistic-regression/data "],
["8-multinom-logistic.html", " 8 Multinomial Logistic Regression 8.1 Multinomial Logistic Regression 8.2 Multinomial Logistic Regression", " 8 Multinomial Logistic Regression 8.1 Multinomial Logistic Regression The General Social Survey (GSS) has been used to measure trends in attitudes and behaviors in American society since 1972. In addition to collecting demographic information, the survey includes questions used to gauge attitudes about government spending priorities, confidence in institutions, lifestyle, and many other topics. A full description of the survey may be found here. In today’s lab, we will use multinomial logistic regression to understand the relationship between a person’s political views and their attitudes towards government spending on mass transportation projects. To do so, we will use data from the 2010 GSS survey. Refer to the Multinomial Logistic Regression notes for help with concepts and code. 8.1.1 Packages You will need the following packages for today’s lab: library(tidyverse) library(nnet) library(knitr) library(broom) ## Fill in other packages as needed 8.1.2 Data The data for this lab is from the 2016 General Social Survey. The original data set contains 2867 observations and 935 variables. Given the size of the dataset, we will handle it differently in our workflow than we’ve handled data in previous assignments. # [Working with large files](https://help.github.com/en/articles/working-with-large-files) The size of this dataset is 34.3 MB. Compare that to the Spotify dataset from last weeks’ lab which was 149 KB (0.149 MB)! GitHub will not allow you to push files larger than 100 MB and will give you a warning when you push files as large as 50 MB. Though we could push the file we’re working with today to GitHub, it’s large enough that we’d still prefer not to. You have may noticed that each repo contains a file called .gitignore. It contains a list of the files you don’t want commit or push to GitHub. If you look at the .gitignore file for today’s lab, you will notice that gss2016.csv is listed at the bottom. Click here to download gss2016.csv. Upload gss2016.csv into the data folder of your project. Notice that gss2016.csv does not appear in your Git pane. This is because it is being ignored by git, since it is listed in the .gitignore file. You will use the following variables in the lab: natmass: Respondent’s answer to the following prompt: “We are faced with many problems in this country, none of which can be solved easily or inexpensively. I’m going to name some of these problems, and for each one I’d like you to tell me whether you think we’re spending too much money on it, too little money, or about the right amount…are we spending too much, too little, or about the right amount on mass transportation?” age: Age in years. sex: Sex recorded as male or female sei10: Socioeconomic index from 0 to 100 region: Region where interview took place polviews: Respondent’s answer to the following prompt: “We hear a lot of talk these days about liberals and conservatives. I’m going to show you a seven-point scale on which the political views that people might hold are arranged from extremely liberal - point 1 - to extremely conservative - point 7. Where would you place yourself on this scale?” Use the code below to read in the data. gss &lt;- read_csv(&quot;data/gss2016.csv&quot;, na = c(&quot;&quot;, &quot;Don&#39;t know&quot;, &quot;No answer&quot;, &quot;Not applicable&quot;), guess_max = 2867) %&gt;% select(natmass, age, sex, sei10, region, polviews) %&gt;% drop_na() The argument guess_max = 2867 tells the read_csv function to use all of the observations in a column to determine its data type. Without this argument, only the first 1,000 observations would be used to make this determination. This becomes important for a variable like age; though age is coded as numeric data for most of the observations, there are some in which age is coded as &quot;89 or older&quot;. Without the guess_max argument, you will get warnings when loading the data. Note also that only the variables of interest will be loaded, not the entire dataset. This will make for faster computation and knitting as you work on the lab. 8.1.3 Exercises 8.1.3.1 Part I: Exploratory Data Analysis # See [Reorder factor levels by hand](https://forcats.tidyverse.org/reference/fct_relevel.html) for documentation about `fct_relevel`. The variable natmass will be the response variable in the model, and you want to compare more opinionated views to the moderate position. Recode natmass so it is a factor variable with &quot;About right&quot; as the baseline. Recode polviews so it is a factor variable type with levels that are in an order that is consistent with question on the survey. Note how the categories are spelled in the data. Make a plot of the distribution of polviews. Which political view occurs most frequently in this data set? Make a plot displaying the relationship between natmass and polviews. Use the plot to describe the relationship between a person’s political views and their views on mass transportation spending. You want to use age as a quantitative variable in your model; however, it is currently a character data type because some observations are coded as &quot;89 or older&quot;. Recode age so that is a numeric variable. Note: Before making the variable numeric, you will need to replace the values &quot;89 or older&quot; with a single value. 8.1.3.2 Part II: Multinomial Logistic Regression Model You plan to fit a model using age, sex, sei10, and region to understand variation in opinions about spending on mass transportation. Briefly explain why you should fit a multinomial logistic model. Fit the model described in the previous exercise and display the model output. Make any necessary adjustments to the variables so the intercept will have a meaningful interpretation. Be sure About Right is the baseline level. Be sure the full model displays in the knitted document. Interpret the intercept associated with odds of having an opinion of “Too much” versus “About right”. Consider the relationship between age and one’s opinion about spending on mass transportation. Interpret the coefficient of age in terms of the log odds of having an opinion of “Too little” versus “About right”. Interpret the coefficient of age in terms of the odds of having an opinion of “Too little” versus “About right”. In general, what is the relationship between an person’s age and their opinions on mass transportation spending? Now that you have adjusted for some demographic factors, let’s examine whether a person’s political views has a significant impact on their attitude towards spending on mass transportation. Conduct the appropriate test to determine if polviews is a significant predictor of attitude towards spending on mass transportation. State the null and alternative hypothesis, display all relevant code and output, and state your conclusion in the context of the problem. Choose the appropriate model based on the results from the test. Use this model for the next part of the lab. 8.1.3.3 Part III: Model Fit Calculate the predicted probabilities and residuals from your model. Plot the binned residuals versus the predicted probabilities for each category of natmass. You will have three plots. # You can change the size of your plots, so you can fit multiple plots on a single page. Include the arguments `fig.height = ` and `fig.width = ` in the header of the code chunk to change the plot size. # See [Using R Markdown](https://rstudio.github.io/dygraphs/r-markdown.html) for an example. Use binned residual plots to examine the residuals versus each of the quantitative variables. Create binned plots of the residuals for each category of natmass versus age. You will have three plots. Create binned plots of the residuals for each category of natmass versus sei10. You will have three plots. To examine the residuals versus each categorical predictor, you will look at the average residuals for each each category of the categorical variables. For each level of natmass, calculate the average residuals across categories of sex. For each category of natmass, calculate the average residuals across categories of region. For each category of natmass, calculate the average residuals across categories of polviews. Based on the analysis of the residuals in Exercises 12 - 14, is the model an appropriate fit for the data? Explain. Regardless of your asssesment of the residuals, use your model for the remainder of the lab. 8.1.3.4 Part IV: Using the Model Use your model to describe the relationship between one’s political views and their attitude towards spending on mass transportation. Use your model to predict the category of natmass for each observation in your dataset. Display a table of the actual versus the predicted natmass. What is the misclassification rate? 8.1.4 Acknowledgements The “Data” section is largely inspired by datasciencebox.org. 8.2 Multinomial Logistic Regression The main objective of this analysis is to understand how encouragement affects the frequency that children watch Sesame Street. We will use the following variables: Response: viewcat 1: rarely watched show 2: once or twice a week 3: three to five times a week 4: watched show on average more than five times a week Predictors: age: child’s age in months prenumb: score on numbers pretest (0 to 54) prelet: score on letters pretest (0 to 58) viewenc: 1: encouraged to watch, 2: not encouraged site: 1: three to five year old from disadvantaged inner city area 2: four year old from advantaged suburban area 3: from advantaged rural area 4: from disadvantaged rural area 5: from Spanish speaking home # read in dataset sesame &lt;- read_csv(&quot;data/sesame.csv&quot;) # mean-center relevant continuous variables, make categorical variables factors sesame &lt;- sesame %&gt;% mutate(viewcat = as.factor(viewcat), site = as.factor(site), prenumbCent = prenumb - mean(prenumb), preletCent = prelet - mean(prelet), ageCent = age - mean(age), viewenc = ifelse(viewenc == 1, &quot;Encouraged&quot;, &quot;Not Encouraged&quot;)) 8.2.1 Questions We will build a model to predict how often a child in this study watched Sesame Street. What type of model should we build? Why? Describe how you would conduct exploratory data analysis. What plots and/or summary statistics would you include? What information would you learn from the exploratory data analysis? model1 &lt;- multinom(viewcat ~ site + viewenc + prenumbCent + preletCent + ageCent, data = sesame) kable(tidy(model1, conf.int=TRUE, exponentiate = FALSE), format = &quot;markdown&quot;) Interpret the intercept associated with the odds of viewcat == 2 versus viewcat == 1. Interpret the effect of the numbers pretest score on the odds of viewership. The primary objective of the experiment was to understand the effect of encouragement viewenc on viewership. Does encouragement have a significant effect on viewership? If so, describe the effect. Otherwise, explain why not. We want to test if there are any significant interactions with viewenc and the pretests. We create a model that includes the variables from model1 along with viewenc*preletCent and viewenc*prenumbCent. model2 &lt;- multinom(viewcat ~ site + viewenc + prenumbCent + preletCent + ageCent + viewenc*preletCent + viewenc*prenumbCent, data = sesame) The results from the drop-in-deviance test are shown below. Is there evidence of a significant interaction effect? Explain. anova(model1, model2, test = &quot;Chisq&quot;) How would you assess the appropriateness of the model flit? Describe the plots, tables, and/or calculations you would create to assess model fit. 8.2.2 References Data from http://www2.stat.duke.edu/~jerry/sta210/sesamelab.html "],
["9-special.html", " 9 Special Topics 9.1 Putting It All Together 9.2 Dealing with Missing Data", " 9 Special Topics 9.1 Putting It All Together In this lab, you will put together everything you’ve learned thus far. Unlike previous lab assignments, your lab write up will be in the form of a small report (rather than numbered exercises). Though this analysis will not be as in-depth as your analysis in the final project, this assignment will give your group practice organizing the results of a statistical analysis to tell a complete narrative. You will also practice imputing missing data and using k-fold cross validation to assess your model’s performance on test data. 9.1.1 Packages You will need the following packages for today’s lab: library(tidyverse) library(dslabs) ## Fill in other packages as needed 9.1.2 Data The data for this lab is the gapminder dataset in the dslabs package. This dataset contains health and income data for 184 countries during the years 1960 to 2016. After loading the dslabs package, you can type ?gapminder in the console to to see the variables in the dataset. You will only use data from 2011 in this lab. 9.1.3 Exercises The goal of this analysis is to build a regression model that could be used to predict a country’s gross domestic product (gdp) using the other characteristics included in the data. Introduction Brief introduction of the data and the research question Exploratory Data Analysis At a minimum, your exploratory data analysis should include the following: Analysis of each variable Dealing with missing values using imputation methods Analysis of the relationships between variables Discussion of any potential transformations, if needed Regression Model At a minimum, the discussion for the final regression model should include the following: Brief discussion about the type of model you used (multiple linear regression, logistic, multinomial logistic regression) and why Discussion of any transformations on the response and/or explanatory variables, if applicable Display of the final model Test of interesting interactions Conclusions drawn from the model, including any interesting insights based on the model coefficients Assumptions At a minimum, the discussion of model assumptions should include the following: Appropriate residual plots Check for influential points Check for multicollinearity Discussion of whether or not assumptions are met and how any issues may affect conclusions drawn from the model Model Validation At a minimum, the discussion of the model validation should include the following: Results and discussion from a 5-fold cross validation Conclusion Brief summary of the conclusions drawn from the analysis. 9.2 Dealing with Missing Data library(tidyverse) library(knitr) library(broom) library(skimr) library(nnet) nhanes &lt;- mice::nhanes2 nhanes Explore missingness library(nnet) m &lt;- multinom(age ~ bmi + hyp + chl, data = mice::nhanes2) knitr::kable(tidy(m, conf.int = T),format = &quot;markdown&quot;) Complete-case analyis complete &lt;- nhanes %&gt;% drop_na() m &lt;- multinom(age ~ bmi + hyp + chl, data = complete) knitr::kable(tidy(m, conf.int = T),format = &quot;markdown&quot;) Single imputation Indicator variables "],
["10-datasets.html", " 10 Data Sets", " 10 Data Sets Data Set Description Chapter Original Source advertising.csv test test test airbnb_basic.csv test test test airbnb_details.csv test test test beer.csv test test test evals_mod.csv test test test fivethirtyeight-recent-grads.R test test test framingham.csv test test test gss2016.csv test test test KingCountyHouses.csv test test test recent-grads.csv test test test sesame.csv test test test sis.csv test test test spotify.csv test test test test_songs.csv test test test "],
["references-3.html", "References", " References "]
]
