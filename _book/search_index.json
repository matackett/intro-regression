[
["index.html", "Intro to Regression Analysis 1 Introduction", " Intro to Regression Analysis Maria Tackett 2019-05-13 1 Introduction This is the introduction to the book. This work is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. "],
["2-intro.html", " 2 Introduction", " 2 Introduction You can label chapter and section titles using {#label} after them, e.g., we can reference Chapter 2. If you do not manually label them, there will be automatic labels anyway, e.g., Chapter ??. Figures and tables with captions will be placed in figure and table environments, respectively. par(mar = c(4, 4, .1, .1)) plot(pressure, type = &#39;b&#39;, pch = 19) Figure 2.1: Here is a nice figure! Reference a figure by its code chunk label with the fig: prefix, e.g., see Figure 2.1. Similarly, you can reference tables generated from knitr::kable(), e.g., see Table 2.1. knitr::kable( head(iris, 20), caption = &#39;Here is a nice table!&#39;, booktabs = TRUE ) Table 2.1: Here is a nice table! Sepal.Length Sepal.Width Petal.Length Petal.Width Species 5.1 3.5 1.4 0.2 setosa 4.9 3.0 1.4 0.2 setosa 4.7 3.2 1.3 0.2 setosa 4.6 3.1 1.5 0.2 setosa 5.0 3.6 1.4 0.2 setosa 5.4 3.9 1.7 0.4 setosa 4.6 3.4 1.4 0.3 setosa 5.0 3.4 1.5 0.2 setosa 4.4 2.9 1.4 0.2 setosa 4.9 3.1 1.5 0.1 setosa 5.4 3.7 1.5 0.2 setosa 4.8 3.4 1.6 0.2 setosa 4.8 3.0 1.4 0.1 setosa 4.3 3.0 1.1 0.1 setosa 5.8 4.0 1.2 0.2 setosa 5.7 4.4 1.5 0.4 setosa 5.4 3.9 1.3 0.4 setosa 5.1 3.5 1.4 0.3 setosa 5.7 3.8 1.7 0.3 setosa 5.1 3.8 1.5 0.3 setosa You can write citations, too. For example, we are using the bookdown package (Xie 2018) in this sample book, which was built on top of R Markdown and knitr (Xie 2015). References "],
["3-getstarted.html", " 3 Getting Started", " 3 Getting Started I am updating this to test it out. "],
["4-intro-to-r.html", " 4 Intro to R 4.1 Introduction 4.2 Packages 4.3 Warm up 4.4 Project name: 4.5 Exercises 4.6 Simple Linear Regression", " 4 Intro to R 4.1 Introduction # R is the name of the programming language itself and RStudio is a convenient interface. The main goal of this lab is to introduce you to R and RStudio, which we will be using throughout the course both to learn the statistical concepts discussed in the course and to analyze real data and come to informed conclusions. # git is a version control system (like &quot;Track Changes&quot; features from Microsoft Word but more powerful) and GitHub is the home for your Git-based projects on the internet (like DropBox but much better). An additional goal is to introduce you to git and GitHub, which is the collaboration and version control system that we will be using throughout the course. As the labs progress, you are encouraged to explore beyond what the labs dictate; a willingness to experiment will make you a much better programmer and statistician. If you’re new to R, you should begin by building some basic fluency in R. Today’s lab will focus on fundamental building blocks of R and RStudio: the interface, reading in data, and basic commands. Starting next week, the labs will focus on concepts more specific to regression analysis. To make versioning simpler, today’s lab is individual. This will give you a chance to become more familiar with git and GitHub. Next week you’ll learn about collaborating on GitHub and will produce a single lab report as a team. 4.1.1 Topics covered in this lab: Exploratory Data Analysis (data visualizations and numerical summaries) Simple linear regression Writing a lab report using R Markdown Tracking changes and submitting work using git and GitHub 4.2 Packages We will use the following packages in today’s lab. library(tidyverse) library(readr) library(skimr) library(broom) If you need to install any of the packages, you can run the code below in the console. install.packages(&quot;tidyverse&quot;) install.packages(&quot;readr&quot;) install.packages(&quot;skimr&quot;) install.packages(&quot;broom&quot;) 4.3 Warm up Before we introduce the data, let’s warm up with some simple exercises. 4.4 Project name: Currently your project is called Untitled Project. Update the name of your project to be “Lab 01 - Intro R”. # The top portion of your R Markdown file (between the three dashed lines) is called YAML. It stands for &quot;YAML Ain&#39;t Markup Language&quot;. It is a human friendly data serialization standard for all programming languages. All you need to know is that this area is called the YAML (we will refer to it as such) and that it contains meta information about your document. 4.4.1 YAML: Open the R Markdown (Rmd) file in your project, change the author name to your name, and knit the document. 4.4.2 Commiting changes: Then go to the Git pane in your RStudio. If you have made changes to your Rmd file, you should see it listed here. Click on it to select it in this list and then click on Diff. This shows you the difference between the last committed state of the document and its current state that includes your changes. If you’re happy with these changes, write “Update author name” in the Commit message box and click Commit. You don’t have to commit after every change, this would get quite cumbersome. You should consider committing states that are meaningful to you for inspection, comparison, or restoration. In the first few assignments we will tell you exactly when to commit and in some cases, what commit message to use. As the semester progresses we will let you make these decisions. 4.4.3 Pushing changes: Now that you have made an update and committed this change, it’s time to push these changes to the web! Or more specifically, to your repo on GitHub. Why? So that others can see your changes. And by others, we mean the course teaching team (your repos in this course are private to you and us, only). In order to push your changes to GitHub, click on Push. This will prompt a dialogue box where you first need to enter your user name, and then your password. This might feel cumbersome. Bear with me… We will teach you how to save your password so you don’t have to enter it every time. But for this one assignment you’ll have to manually enter each time you push in order to gain some experience with it. 4.4.4 Data Today’s data comes from the Capital Bikeshare in Washington D.C. The Capital Bikeshare is a system in which customers can rent a bike for little cost, ride it around the city, and return it to a station near their destination. You can get more information about the bikeshare on their website, https://www.capitalbikeshare.com/. We will read in the data from the file bikeshare.csv located in the data folder. bikeshare &lt;- read_csv(&quot;data/bikeshare.csv&quot;) This dataset contains information about the number of bike rentals, environmental conditions, and other information about the each day in 2011 and 2012. 4.5 Exercises Before doing any analysis, we want to understand the basic structure of the data. One way to do this, is to look at the actual dataset. Type the code below in the console to view the entire dataset. View(bikeshare) It is sometimes more useful to view a summary of the data structure rather than view the entire dataset. This is especially true for very large datasets, i.e. those with a very large number of observations and/or rows. We can use the glimpse() function to get a general idea about the structure of our dataset. This function can be very useful when importing data from a file such as a .csv file (like in this lab) to ensure that data imported correctly and that we have the number of observations (rows) and variables (columns) we expect. We can also use this function to see each variable’s type (e.g. integer, character). # You can type `??glimpse` in the console to learn more about the function and its syntax. Type glimpse(bikeshare) in the console an overview of the bikeshare dataset. How many observations are in the bikeshare dataset? How many variables? In this lab, we will focus the analysis on the following variables: season 1: Winter, 2: Spring, 3: Summer, 4: Fall temp Temperature (in \\(^{\\circ}C\\)) ÷ 41 count total number of bike rentals Before fitting any regression models, we want to do an exploratory data analysis (EDA) to summarize the main characteristics of the data. Much of the EDA is visual, which we’ll get to in the next exercise. The EDA also consists of calculating summary statistics for the variables in our dataset. It is good practice to examine any variable that may be relevant to the analysis in the EDA, since there may be variables that aren’t directly included in the regression model but are still affecting the results. To keep today’s lab manageable, we will only examine the three variables season, temp, and count. There are many ways to calculate summary statistics for each variable, and we will use a few of them throughout the semester. For now, let’s use the skim() function to calculate basic measures of center and spread along with get a sketch of the distribution. bikeshare %&gt;% select(season,temp,count) %&gt;% skim() What is the mean number of bike rentals? About 25% of the days in the data have a count above what value? Does it make sense to calculate measures of center and spread for the variable season? If so, explain why it makes sense. Otherwise, explain why the skim() function calculated these summary statistics for the variable season even if they don’t make sense. This is a good place to pause and commit changes with the commit message “Added summary statistics (Ex 1 - 3)”, and push. 4.5.1 Visualizing Your Data One important part of EDA is visualizing the data to get a better idea of the shape of the distribution of each variable along with the relationship between variables. There are a lot of ways to make plots in R; we will use the functions available in the ggplot2 package. The code below is used to create a histogram to visualize the distribution of count. Modify the code by writing an informative title and label for the x-axis. ggplot(data=bikeshare, mapping=aes(x=count)) + geom_histogram() + labs(title=&quot;______&quot;, x=&quot;______&quot;) Sometimes you may want to customize a plot by changing different features such as the color, marker types, etc. When plotting a histogram, one easy way to customize it is by changing the color the bars. We’ll look at two different ways to do this. First, using a color of your choice, include the option color=&quot;_____&quot; inside of geom_histogram() function. Your code will look similar this. Be sure to also include an informative title and label for the x-axis. ggplot(data=bikeshare, mapping=aes(x=count)) + geom_histogram(color=&quot;_______&quot;) + labs(title=&quot;______&quot;, x=&quot;______&quot;) # This [ggplot2 quick reference](http://sape.inf.usi.ch/quick-reference/ggplot2/colour) has a long list of color options. You can also use [HTML color codes](https://htmlcolorcodes.com/). Next, instead of color=&quot;_____&quot;, use fill=&quot;______&quot; inside of the geom_histogram() function and put the color of your choice inside the blank. You can use the same color as before or use a new one. What is the difference in the two plots? In other words, what is the difference in the way color is implemented when using color versus fill? Describe the distribution of count. Your description should include comments about the shape, center, spread, and any potential outliers. You should use the histogram and the summary statistics from Exercise 2 in your description. This is a another good place to pause and commit changes with the commit message “Added data visualization of count (Ex 3 - 6)”, and push. Now that we’ve examined the variables individually, we want to look at the relationship between the variables. To make interpretation easier, we will use the mutate() function to create a new variable called temp_c that is calculated as temp * 41. We will use temp_c for the remainder of the analysis, so the temperature can be discussed in terms of degrees Celsius. bikeshare &lt;- bikeshare %&gt;% mutate(temp_c = temp * 41) Complete the code below to make a scatterplot of the number of bike rentals versus the temperature. ggplot(data=bikeshare, mapping=aes(x=temp_c,y=count)) + ___________________ # [https://ggplot2.tidyverse.org/](https://ggplot2.tidyverse.org/) is a great resource as you learn `ggplot()`. Click **Reference** in the top right corner to see a list of the various plot types available in the ggplot2 package. Describe the relationship between the temperature and the number of bike rentals. The temperature and number of bike rentals varies greatly depending on the season. Therefore, we would like to create a separate scatterplot of count versus temp_c for each season. To do so, we will use the facet_wrap() function, faceting by season. Recall from Exercise 2 that season is currently stored as an integer. We need to change it to a factor variable type before using it in the facet_wrap() function (you could also change it to a character variable). bikeshare &lt;- bikeshare %&gt;% mutate(season = as.factor(season)) ggplot(data=bikeshare, mapping=aes(x=temp_c,y=count)) + geom_point() + labs(title = &quot;Number of Bike Rentals vs. Temperature&quot;, subtitle=&quot;Faceted by Season&quot;, x = &quot;Temperature (Celsius)&quot;, y = &quot;Number of Bike Rentals&quot;) + facet_wrap(~season) For which season does the linear relationship between the temperature and the number of bike rentals appear to be the strongest? This is a another good place to pause and commit changes with the commit message “Added visualization of count vs. temperature (Ex 7 - 8)”, and push. 4.6 Simple Linear Regression We want to fit a least-squares regression using the temperature (temp_c) to explain variation in the number of bike rentals (count) in the winter season. We can use the filter() function to create a subset from the data that only includes days during the winter. The &lt;- assigns the name winter_data to our subset. winter_data &lt;- bikeshare %&gt;% filter(season==&quot;1&quot;) We will use winter_data for the remainder of the lab. Fit a simple linear regression model with using the lm() function; assign it the name winter_model. Replace X, Y, and my.data in the code below with the appropriate values. winter_model &lt;- lm(Y ~ X, data=my.data) tidy(model) #output model Interpret the slope. Does it make sense to interpret the intercept? If so, write the interpretation of the intercept. Otherwise, explain why not. We conclude by checking the assumptions for regression. We use the mutate() function to add a new variable called resid that is the residual for each observation in winter_data data. winter_data &lt;- winter_data %&gt;% mutate(resid = residuals(winter_model)) The code for the residuals vs. the predictor variable and the Normal QQ plot are below. In addition to these plots, write the code to make a histogram of the residuals. You can reuse code from a previous exercise to plot the histogram. ggplot(data=winter_data, mapping=aes(x=temp_c,y=resid)) + geom_point() + geom_hline(yintercept=0,color=&quot;red&quot;)+ labs(title=&quot;Residuals vs. Temperature&quot;, x=&quot;Temperature&quot;, y=&quot;Residuals&quot;) ggplot(data=winter_data, mapping=aes(sample=resid)) + stat_qq() + stat_qq_line()+ labs(title=&quot;Normal QQ Plot of Residuals&quot;) Based on the plots of the residuals and the scatterplot, are linearity, normality, and constant variance assumptions met? Briefly explain. Is the independence assumption met based on the description of the data? Briefly explain. Optional: Create a plot that could be used to help you assess the independence assumption. Throughout the semester, we will learn various methods to deal with any violations in regression assumptions. For now, we will just note them. You’re done! Commit all remaining changes, use the commit message “Done with Lab 1!”, and push. Before you wrap up the assignment, make sure all documents are updated on your GitHub repo. "],
["5-slr.html", " 5 Simple Linear Regression 5.1 Warm up 5.2 Exercises", " 5 Simple Linear Regression The primary goal of today’s lab is to give you practice with some of the tools you will need to conduct regression analysis using R. An additional goal for today is for you to be introduced to your teams and practice collaborating using GitHub and RStudio. 5.0.1 Packages We will use the following packages in today’s lab. library(tidyverse) library(skimr) library(broom) library(rcfss) 5.0.2 Project name: Currently your project is called Untitled Project. Update the name of your project to be “Lab 02 - Simple Linear Regression” 5.1 Warm up Pick one team member to complete the steps in this section while the others contribute to the discussion but do not actually touch the files on their computer. Before we introduce the data, let’s warm up with a simple exercise. 5.1.1 YAML: Open the R Markdown (Rmd) file in your project, change the author name to your team name, and knit the document. 5.1.2 Commiting and pushing changes: Go to the Git pane in your RStudio. View the Diff and confirm that you are happy with the changes. Add a commit message like “Update team name” in the Commit message box and hit Commit. Click on Push. This will prompt a dialogue box where you first need to enter your user name, and then your password. 5.1.3 Pulling changes: Now, the remaining team members who have not been concurrently making these changes on their projects should click on the Pull button in their Git pane and observe that the changes are now reflected on their projects as well. 5.1.4 Data In today’s lab, we will analyze the scorecard dataset from the rcfss package. This dataset contains information about 1849 colleges obtained from the Department of Education’s College Scorecard. Load the rcfss library into the global R environment and type ?scorecard in the console to learn more about the dataset and variable definitions. Today’s analysis will focus on the following variables: type Type of college (Public, Private - nonprofit, Private - for profit) cost The average annual cost of attendance, including tuition and feeds, books and supplies, and living expenses, minus the average grant/scholarship aid admrate Undergraduate admissions rate (from 0 - 100%) 5.2 Exercises 5.2.1 Exploratory Data Analysis Plot a histogram to examine the distribution of admrate. What is the shape of the distribution? To better understand the distribution of admrate, we would like calculate measures of center and spread of the distribution. Fill in the code below to use the skim function to calculate summary statistics for admrate. Report the appropriate measures of center (mean or median) and spread (standard deviation or IQR) based on the shape of the distribution from Exercise 1. scorecard %&gt;% select(admrate) %&gt;% skim() Plot the distribution of cost and calculate the appropriate summary statistics. Describe the distribution of cost (shape, center, and spread) using the plot and appropriate summary statistics. One nice feature of the skim function is that it provides information about the number of observations that are missing values of the variable. How many observations have missing values of admrate? How many observations have missing values of cost? Later in the semester, we will techniques to deal with missing values in the data. For now, however, we will only include complete observations for the remainder of this analysis. We can use the filter function to select only the rows that values for both cost and admrate. Fill in the code below to create a new dataset called scorecard_new that only includes observations with values for both admrate and cost. __________ &lt;- scorecard %&gt;% filter(!is.na(admrate),________) # Learn more about the `filter` function in [Section 5.2 of R for Data Science] (https://r4ds.had.co.nz/transform.html#filter-rows-with-filter) You will use scorecard_new for the rest of the lab. Create a scatterplot to display the relationship between cost (response variable) and admrate (explanatory variable). Use the scatterplot to describe the relationship between the two variables. The data contains information about the type of college, and we would like to incorporate this information into the scatterplot. One way to do this is to use a different color marker for each type of college. Fill in the code below the scatterplot from the previous exercise with the marker colors based on the variable type. Describe two new observations from this scatterplot that you didn’t see in the previous plot. ggplot(data=scorecard_new, mapping=aes(x=admrate, y=cost, color=type)) + _____________________ 5.2.2 Simple Linear Regression Fit a regression model to describe the relationship between a college’s admission rate and cost. Use the tidy function to display the model. Interpret the slope in the context of the problem. Does the intercept have a meaningful interpretation? If so, write the interpretation in the context of the problem. Otherwise, explain why the interpretation is not meaningful. While the tidy function is used to display the model, we can obtain a one-row summary of the model using the glance function. Use the glance function to get a summary of the model fit in the previous exercise. See the documentation for glance for the syntax and a list of values output from the function. What is the value of \\(R^2\\)? Interpret this value in the context of the problem. Do you think this is a “good” value of \\(R^2\\)? Explain. What is the value of \\(\\hat{\\sigma}\\), the residual standard error. What is the 95% confidence interval for the coefficient of admrate, i.e. the slope? Interpret the interval in the context of the data. We want to test the following hypotheses about the population slope \\(\\beta_1\\): \\[H_0: \\beta_1 = 0 \\hspace{5mm} \\text{versus} \\hspace{5mm} H_a: \\beta_1 \\neq 0\\] State what the null and alternative hypotheses mean in terms of the linear relationship between admrate and cost. Consider the confidence interval from Exercise 13 and the hypotheses in Exercise 14. Is the confidence interval consistent with the null or alternative hypothesis? Briefly explain. You’re done! Commit all remaining changes, use the commit message “Done with Lab 2!”, and push. Before you wrap up the assignment, make sure the .Rmd, .html, and .md documents are all updated on your GitHub repo. "],
["6-anova.html", " 6 ANOVA 6.1 Exercises", " 6 ANOVA The goal of this lab is to use Analysis of Variance (ANOVA) to compare means in multiple groups. Additionally, you will be introduced to new R functions used for wrangling and summarizing data. 6.0.1 Packages We will use the following packages in today’s lab. library(tidyverse) library(knitr) library(broom) 6.0.2 Data In today’s lab, we will analyze the diamonds dataset from the ggplot2 package. Type ?diamonds in the console to see a dictionary of the variables in the data set. This analysis will focus on the relationship between a diamond’s carat weight and its color. Before starting the exercises, take a moment to read more about the diamond attributes on the Gemological Institute of America webpage: https://www.gia.edu/diamond-quality-factor. 6.1 Exercises The diamonds dataset contains the price and other characteristics for over 50,000 diamonds price from $326 to $18823. In this lab, we will analyze the subset of diamonds that are priced $1200 or less. Create a dataframe called diamonds_low that is the subset of diamonds priced $1200 or less. How many observations are in diamonds_low? When using Analysis of Variance (ANOVA) to compare group means, it is ideal to have approximately the same number of observations in each group. Therefore, we will combine the worst two color groups, I and J, and create a new color category called “I/J”. Since color is an ordinal (&lt;ord&gt;) variable, we need to use the recode_factor function in the dplyr package to create the new category. Use the count function before and after making the new color category to ensure the recoding worked as expected. ## number of observations at each color level diamonds_low %&gt;% count(color) #create a new vector of the recoded values color_recoded &lt;- recode_factor(diamonds_low$color, `I` = &quot;I/J&quot;, `J` = &quot;I/J&quot;, .default = levels(diamonds_low$color)) #replace the color variable with the recoded data diamonds_low &lt;- diamonds_low %&gt;% mutate(color = color_recoded) # Refer to the [ggplot2 Cheat Sheet](https://www.rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdf) and [ggplot2 reference](https://ggplot2.tidyverse.org/reference/index.html) for plot ideas and help with code. We begin by plotting the relationship between color and carat. As a group, brainstorm ways to plot the relationship between the two variables, then make one of the plots. Be sure to include informative axes labels and an informative title. Fill in the code below to calculate the mean and variance of carat at each level of color. # The `group_by` function is used to do calculation in groups. The `summarise` function is used to reduce variables to values. diamonds_low %&gt;% group_by(_______) %&gt;% summarise(n = n(), avg_carat = mean(carat), var_carat = _______) Based on the plots and summary statistics, does there appear to be a relationship between carat weight the color of diamonds? In other words, does there appear to be a significant difference in the mean carat weight across colors? When using ANOVA to compare means across groups, we make the following assumptions (note how similar they are to the assumptions for regression): Normality: The distribution of \\(y\\) is approximately normal within each category of \\(x\\) - in the \\(k^{th}\\) category, \\(y \\sim (\\mu_k, \\sigma^2)\\) . If the sample size is large, ANOVA is robust to some departures from Normality. Independence: All observations are independent from one another, i.e. one observation does not affect another. Constant Variance: The distribution of \\(y\\) within each category of \\(x\\) has a common variance, \\(\\sigma^2\\). One way to assess if variances are sufficiently equal is to look at the ratio of the maximum group variance to the minimum group variance. If this ratio is less than 2, then we can conclude the variances are approximately equal. This isn’t an exact threshold, but rather a commonly used guideline. Note: There are formal tests for equal variance that are outside the scope of this class. Are the assumptions for ANOVA met? Comment on each assumption using the summary statistics and/or plots from previous exercises to support your conclusion. You may also calculate any additional summary statistics or make additional plots as needed. Regardless of your answer to Excerise 4, We will proceed with the analysis in the remainder of this lab as if the assumptions are met. Use the code below to calculate the ANOVA table. The tidy function from the broom package is used to put the ANOVA output in a dataframe, and with the kable function from the knitr package, you can display the results in an easy-to-read table. anova &lt;- aov(carat ~ color, data=diamonds_low) anova %&gt;% tidy() %&gt;% kable() Use the ANOVA table to calculate the total mean square, i.e. the sample variance of carat. Show your calculations. You can put the calculations in a code chunk to use R like a calculator. What is \\(\\hat{\\sigma}^2\\), the estimated variance of carat within each level of color. We can use ANOVA to test if the true mean value of carat is equal for all levels of color, i.e. \\[ H_0: \\mu_1 = \\mu_2 = \\dots = \\mu_6\\] State the alternative hypothesis is the context of the data. Based on the ANOVA table, what is your conclusion from the test of the hypotheses in the previous question? State the conclusion in the context of the data. Use the code below to plot a 95% confidence interval for the mean carat weight at each level of color. Calculate the value of sigma by filling in the estimated variance from Exercise 7. The formula for the confidence interval for the mean of group \\(k\\) is # The critical value $t^*$ is calculated using the *t* distribution with $n-K$ degrees of freedom. # The standard error of the mean is calculated using $\\hat{\\sigma}$, the square root of the variance within each group calculated from the ANOVA table. \\[\\bar{y}_k \\pm t^* \\frac{\\hat{\\sigma}}{\\sqrt{n_k}}\\] n.groups &lt;- diamonds_low %&gt;% distinct(color) %&gt;% count() crit.val &lt;- qt(0.975, (nrow(diamonds_low)-n.groups$n)) sigma &lt;- sqrt(_________) conf.intervals &lt;- diamonds_low %&gt;% group_by(color) %&gt;% summarise(mean_carat = mean(carat), n = n(), lower = mean_carat - crit.val * sigma/sqrt(n), upper = mean_carat + crit.val * sigma/sqrt(n)) ggplot(data=conf.intervals,aes(x=color,y=mean_carat)) + geom_point() + geom_errorbar(aes(ymin = lower, ymax = upper), width = 0.1) + labs(title=&quot;95% confidence interval for the mean value of carat&quot;, subtitle=&quot;by Color&quot;) + coord_flip() For what color level is the mean carat weight the most different from all the others? Based on this analysis, describe the relationship between the color and the mean carat weight in diamonds that cost $1200 or less. Refer to the diamond documentation to recall what the color scale means. You’re done! Commit all remaining changes, use the commit message “Done with Lab 3!”, and push. Before you wrap up the assignment, make sure the .Rmd, .html, and .md documents are all updated on your GitHub repo. "],
["7-mlr.html", " 7 Multiple Linear Regression 7.1 Exercises", " 7 Multiple Linear Regression The goal of this lab is to use multiple linear regression to understand the variation in the selling price of houses in King County, Washington. You will also gain practice using special predictors, such as categorical predictors and interaction effects, in the model, and you will be introduced to variable transformations. 7.0.1 Packages We will use the following packages in today’s lab. library(tidyverse) library(knitr) library(broom) 7.0.2 Data The for today’s lab contains the price and other characteristics of over 20,000 houses sold in King County, Washington (the county that includes Seattle). The dataset includes the following variables: price: selling price of the house date: date house was sold, measured in days since January 1, 2014 bedrooms: number of bedrooms bathrooms: number of bathrooms sqft: interior square footage floors: number of floors waterfront: 1 if the house has a view of the waterfront, 0 otherwise yr_built: year the house was built yr_renovated: 0 if the house was never renovated, the year the house was renovated if else houses &lt;- read_csv(&quot;data/KingCountyHouses.csv&quot;) 7.1 Exercises Use data visualization and summary statistics to examine the distribution of bedrooms. What is the maximum value? Does this value make sense? If not, what is this an indication of, i.e. how did this value get recorded in the data? Briefly explain. # See the [documentation](https://dplyr.tidyverse.org/reference/summarise.html) for more information about the `summarise` function. We want to remove observations that have extreme values for bedrooms, i.e. those with values for bedrooms above the 95th percentile in the data. What is the 95th percentile for bedrooms? Use the summarise function to help you calculate this value. Fill in the code below to filter the data so that the extreme observations are removed. How many observations are in the updated dataset? houses &lt;- houses %&gt;% filter(bedrooms &lt;= ____) We will use this dataset for the remainder of the analysis. Fit a regression model using square feet to explain variation in the price. Plot the residuals versus the predicted values. Based on this plot, what regression assumption appears to be violated? Briefly explain. Plot the histogram and Normal QQ-plot of the residuals. Based on these plots, what regression assumption appears to be violated? Briefly explain. One way to deal with violations in regression assumptions is to transform the response variable and use that transformed variable when fitting the regression model. (We will talk about this in class next week). Some common transformations used in regression are the natural log (log(y)), the square root (sqrt(y)), and the reciprocal (1/y). Each transformation is applied to the response variable price, and the distributions of the transformed data are shown below. Which transformation should we use to fix the violations of the model assumptions observed in the previous exercise? Briefly explain your choice. Add the variable logprice, the log-transformed version of price, to the data frame. Fit a regression model with logprice as the response and sqft as the predictor variable. Create the residuals plots (residuals vs. predicted, histogram of residuals, Normal QQ-plot). Briefly comment on whether or not using the transformed variable improved on the model assumptions. Though we can explain about 48% of the variation in a house prices by the square footage, we would like to incorporate some of the other available house characteristics in the model. Before fitting the model, use the code below to add the variablefloorsCat that is the categorical version of the variable floors. Discuss with your group why it may make sense to treat floors as categorical, even though it represents a count. houses &lt;- houses %&gt;% mutate(floorsCat = as.factor(floors)) # See the [documentation](https://dplyr.tidyverse.org/reference/tally.html) for more information about the `count` function. Use the count function to see the number of observations at each level of floorsCat. What is the most common number of floors? Use the code below to calculate the mean-centered versions of the variables sqft, bedrooms, and bathrooms and add them to the data frame. houses &lt;- houses %&gt;% mutate(sqftCent = sqft - mean(sqft), bedroomsCent = bedrooms - mean(bedrooms), bathroomsCent = bathrooms-mean(bathrooms)) It is not appropriate to calculate the mean-centered version of the variable waterfront. Briefly explain why it isn’t. Fit a regression model with logprice as the response variable, and the mean-centered variables from the previous exercise along with waterfront and floorsCat as the predictor variables. Display the model output. What is the baseline level for the variable floorsCat? Interpret the intercept of the model in the context of the data. Write the interpretation in terms of the price. What is the intercept of the model for the subset of houses with 3 floors that are not on the waterfront? Write the intercept in terms of the log(price). We would like to consider potential interactions for the model. A significant interaction occurs when the relationship of a predictor variable with the response depends on the value of another predictor variable. Fill in the code below to plot the relationship between logprice and bedrooms by waterfront. Based on this plot, do you think there is a significant interaction effect between bedrooms and waterfront? In other words, do you think the relationship between the logprice and the number of bedrooms differs based on whether or not a house is on the waterfront? Briefly explain. ggplot(data=houses,mapping=aes(x=_____,y=_____,color=as.factor(waterfront))) + geom_smooth(method=&quot;lm&quot;, se=FALSE) + labs(title=&quot;__________________&quot;, x=&quot;Number of bedrooms&quot;, y=&quot;Log Price&quot;, color=&quot;Waterfront&quot;) We will talk more about interaction effects in Monday’s lecture. In HW 03, you explore potential interaction effects using this housing data. You’re done! Commit all remaining changes, use the commit message “Done with Lab 4!”, and push. Before you wrap up the assignment, make sure the .Rmd and .md documents are updated in your GitHub repo. There is a 10% penalty if the .Rmd file has to be knitted to display graphs, i.e. the graphs are not showing in the .md file on GitHub. 7.1.1 Acknowledgement The data used in this lab was obtained from https://github.com/proback/BYSH. "],
["8-mlr2.html", " 8 Data Wrangling &amp; Multiple Linear Regression 8.1 Exercises", " 8 Data Wrangling &amp; Multiple Linear Regression When doing statistical analyses in practice, there is often a lot of time spent on cleaning and preparing the data. The goal of today’s lab is to practice cleaning messy data, so it can be used in a regression analysis. You will also practice interpreting the results from a regression model that has numeric and categorical predictors and a log-transformed response variable. 8.0.1 Packages We will use the following packages in today’s lab. library(tidyverse) library(knitr) library(skimr) library(broom) 8.0.2 Data Today’s data is about Airbnb listings in Asheville, NC. The data was obtained from http://insideairbnb.com/; it was originally scraped from airbnb.com. You can see a visualization of some of the data used in today’s lab at http://insideairbnb.com/asheville/. basic_info &lt;- read_csv(&quot;data/airbnb_basic.csv&quot;) details &lt;- read_csv(&quot;data/airbnb_details.csv&quot;) We will use the following variables in this lab: price: Cost per night (in U.S. dollars) cleaning_fee: Cleaning fee (in U.S. dollars) property_type: Type of dwelling (House, Apartment, etc.) room_type: Entire home/apt (guests have entire place to themselves) Private room (Guests have private room to sleep, all other rooms shared) Shared room (Guests sleep in room shared with others) number_of_reviews: Total number of reviews for the listing review_scores_rating: Average review score (0 - 100) 8.1 Exercises 8.1.1 Data wrangling We would like to use variables from both the basic_info and details data frames in this analysis. Both dataframes have the variable id that uniquely identifies each Airbnb listing. Because we need data from basic_info and details, we only want to include observations that are in both the basic_info and details datasets. Therefore, we will use an inner_join to combine the two data sets. (Note: Both data frames include a variable called id that uniquely identifies each Airbnb listing. R will use this variable to join the two data frames.) # See [Section 13.4 of *R for Data Science*](https://r4ds.had.co.nz/relational-data.html#mutating-joins) for more information about joins. airbnb &lt;- inner_join(basic_info,details) How many observations are in airbnb? How many variables? Some Airbnb rentals have cleaning fees, and we want to include the cleaning fee when we calculate the total rental cost. Use the code below to see how the data in the column cleaning_fee is currently stored in the airbnb data frame. typeof(airbnb$cleaning_fee) The column cleaning_fee currently contains what type of data? Why do you think the data is stored this way even though cleaning_fee is a quantitative variable? Since cleaning_fee is a quantitative variable, we need to make sure it is stored as numeric data in the dataframe. To do so, we will first use the extract function in tidyr package to create a column of cleaning fees that don’t have the dollar sign. Then, we will use the as.numeric() function to make the extracted data the numeric data type double. # See [https://tidyr.tidyverse.org/reference/extract.html) for more information about the `extract` function. airbnb &lt;- airbnb %&gt;% extract(cleaning_fee, &quot;cleaning_fee&quot;) %&gt;% mutate(cleaning_fee = as.numeric(cleaning_fee)) Use the typeof function to confirm that cleaning_fee is now stored as a double data type. Use the skim function to view a summary of the cleaning_fee data. How many observations have missing values for cleaning_fee? What do you think is the most likely reason for the missing observations of cleaning_fee? In other words, what does a missing value of cleaning_fee indicate? Fill in the code below to impute the missing values of cleaning_fee with an appropriate numeric value. Then use the skim function to confirm that there are no longer missing values of cleaning_fee. # See [https://dplyr.tidyverse.org/reference/case_when.html](https://dplyr.tidyverse.org/reference/case_when.html) for more information about the `case_when` function. airbnb &lt;- airbnb %&gt;% mutate(cleaning_fee = case_when( is.na(cleaning_fee) ~ ______, TRUE ~ cleaning_fee )) This is an example of data that is missing not at random, since there is a specific pattern/explanation to the misisng data. We will talk more about dealing with missing data later in the semester. # See [Section 5.6.3 of *R for Data Science*](https://r4ds.had.co.nz/transform.html#counts) for more information about counting observations. Next, we look at the variable property_type. Use the count function to determine how many categories are in the variable property and the frequency of each category. What are the top 4 most common property types? These make up what proportion of the observations? Since an overwhelming majority of the observations in the data are one of the top 4 property types, we would like to create a simplified version of the proprety_type variable that has 5 categories: House, Apartment, Guest suite, Bungalow, and Other. Fill in the code below to create prop_type_simp. airbnb &lt;- airbnb %&gt;% mutate(prop_type_simp = case_when( property_type %in% c(&quot;House&quot;,&quot;______&quot;, &quot;______&quot;,&quot;______&quot;) ~ property_type, TRUE ~ &quot;Other&quot; )) Use the code below to check that prop_type_simp was correctly made. airbnb %&gt;% count(property_type, prop_type_simp) %&gt;% arrange(desc(n)) Airbnb is most commonly used for travel purposes, i.e. as an alternative to traditional hotels. We only want to include Airbnb listings in our regression analysis that are intended for travel purposes. What are the 5 most common values for the variable minimum_nights? Which value in the top 5 stands out? What is the likely intended purpose for Airbnb listings with this seemingly unusual value for minimum_nights? Filter the airbnb data so that it only includes observations with minimum_nights &lt;= 3. You will use this filtered dataset for the remainder of the lab. 8.1.2 Regression Analysis For the response variable, will use the cost to stay at an Airbnb location for 3 nights. Create a new variable called price_3_nights that uses price and cleaning_fee to calculate the total cost to stay at the Airbnb property for 3 nights. Be sure to add this variable to your dataframe. Use histograms to examine the distributions of price_3_nights and log(price_3_nights). Based on the histograms, which variable should you use for the regression model? Briefly explain. Use this variable as the response for the remainder of the lab. Fit a regression model called model1 with the response variable from the previous question and the following predictor variables: prop_type_simp, number_of_reviews, and review_scores_rating. Display the model output. Interpret the coefficient review_scores_rating in terms of price_3_nights. Interpret the coefficient of prop_type_simpGuest suite in terms of price_3_nights. We want to determine if room_type is a significant predictor of the cost for 3 nights, given everything else in the model. Fit a regression model called model2 that includes all of the predictor variables in model1 and room_type. Display the model output. Use the code below to conduct a Nested F test to determine if room_type is a significant predictor of the minimum cost. What is your conclusion from the Nested F test? anova(model1, model2) Suppose you are planning to visit Asheville over spring break, and you want to stay in an Airbnb. You find an Airbnb that is an apartment with a private room, has 10 reviews, and an average rating of 90. Use model2 to predict the total cost to stay at this Airbnb for 3 nights. Include the appropriate 95% interval with your prediction. Report the prediction and interval in terms of price_3_nights. You’re done! Commit all remaining changes, use the commit message “Done with Lab 5!”, and push. Before you wrap up the assignment, make sure the .Rmd and .md documents are updated in your GitHub repo. There is a 10% penalty if the .Rmd file has to be knitted to display graphs, i.e. the graphs are not showing in the .md file on GitHub. 8.1.3 Acknowledgement The data from this lab is from insideairbnb.com "],
["9-selection.html", " 9 Model Selection 9.1 Exercises", " 9 Model Selection The goal of today’s lab is to practice forward and backward model selection. In addition to practice with model selection functions in R, you will manually conduct a backward selection procedure to better understand what occurs when you use model selection functions. 9.0.1 Packages You will need the following packages for today’s lab: library(tidyverse) library(knitr) library(broom) library(leaps) library(Sleuth3) #case1201 data library(ISLR) #Hitters data 9.0.2 Data There are two datasets for this lab. 9.0.2.1 Part I The dataset for Part I contains the SAT score (out of 1600) and other variables that may be associated with SAT performance for each of the 50 states in the U.S. The data is based on test takers for the 1982 exam. The following variables are in the dataset: SAT: average total SAT score State: U.S. State Takers: percentage of high school seniors who took exam Income: median income of families of test-takers ($ hundreds) Years: average number of years test-takers had formal education in social sciences, natural sciences, and humanities Public: percentage of test-takers who attended public high schools Expend: total state expenditure on high schools ($ hundreds per student) Rank: median percentile rank of test-takers within their high school classes 9.0.2.2 Part II The dataset for Part II contains the performance statistics and salaries of Major League Baseball players in the 1986 and 1987 seasons. The data is in the Hitters dataset in the ISLR package. Type ?Hitters in the console to see the variables names and their definitions. 9.1 Exercises 9.1.1 Part I For the first part of the lab, you will return to the model selection activity you started in class using the SAT data. The data is in the case1201 data frame in the Sleuth3 package. sat_scores &lt;- case1201 %&gt;% select(-State) Manually perform backward selection using \\(Adj. R^2\\) as the selection criterion. To help you get started, the full model and the code for the first set of models to test are below. Show each step of the selection process. Display the coefficients and \\(Adj. R^2\\) of your final model. full_model &lt;- lm(SAT ~ ., data = sat_scores) m1 &lt;- lm(SAT ~ Income + Years + Public + Expend + Rank, data = sat_scores) m2 &lt;- lm(SAT ~ Takers + Years + Public + Expend + Rank, data = sat_scores) m3 &lt;- lm(SAT ~ Takers + Income + Public + Expend + Rank, data = sat_scores) m4 &lt;- lm(SAT ~ Takers + Income + Years + Expend + Rank, data = sat_scores) m5 &lt;- lm(SAT ~ Takers + Income + Years + Public + Rank, data = sat_scores) m6 &lt;- lm(SAT ~ Takers + Income + Years + Public + Expend, data = sat_scores) What is the best 5-variable model? Display the model output. Use the regsubsets function to perform backward selection. What is the final model when \\(Adj. R^2\\) is the selection criterion? Display the coefficients and the \\(Adj. R^2\\) of the final model. This should be the same result you got in Exercise 1. What is the final model when \\(BIC\\) is the selection criterion? Display the coefficients and the \\(BIC\\) of the final model. Compare the final models selected by \\(Adj. R^2\\) and \\(BIC\\). Do the models have the same number of predictors? Briefly explain. Are the same predictor variables in each model? Briefly explain. Consider the comparisons made in the previous exercise. Are these differences what you would expect given the selection criteria used? Briefly explain. 9.1.2 Part II The data for this part of the lab is the Hitters dataset in the ISLR package. Your goal is to fit a regression model that uses the performance statistics of baseball players to predictor their salary. There are 19 potential predictor variables, so you will use the regsubsets function to conduct forward selection to choose a final model. Read through the data dictionary for the Hitters dataset. You can access it by typing ?Hitters in the console. What is the difference between the variables HmRun and CHmRun? Some observations have missing values for Salary. Filter the data, so only observations that have values for Salary are included. You will use this filtered data for the remainder of the lab. Fill in the code below to conduct forward selection and save the results in an object called sel_summary (selection summary). # The `nvmax` option indicates the maximum-sized variable subsets to consider in the model selection. regfit_forward &lt;- regsubsets(_______, ________, method=&quot;forward&quot;, nvmax = 19) sel_summary &lt;- summary(_______) The object sel_summary contains the summary statistics for the best fit model containing \\(k\\) predictors, where \\(k = 1, \\ldots, 19\\). The object sel_summary is a list object, so it is cumbersome to extract the relevant summary statistics. Therefore, you can create a data frame called summary_stats such that each row represents the best fit model with \\(k\\) predictors and each column is a summary statistic. For example, the second row contains the summary statistics of the best fit model that contains 2 variables. Fill in the code below to create the data frame summary_stats that includes the \\(BIC\\), \\(R^2\\), \\(Adj. R^2\\), and residual sum of squares (RSS) for each model in sel_summary. The data frame summary_stats will also include the column np, the number of predictors in the model represented on each row. summary_stats &lt;- data.frame(bic = sel_summary$bic, adjr2 = _______, rsq = _______, rss = _______) %&gt;% mutate(np = row_number()) #number of variables # See the [ggplot2 documentation](https://ggplot2.tidyverse.org/reference/geom_abline.html#arguments) for code to add a vertical line. Use the data in the summary_stats data frame to plot \\(BIC\\) versus the number of predictors. Include a vertical line on your plot that shows the number of predictors for the overall final model you would select based on \\(BIC\\). Be sure your plot has clear and informative title and axes labels. How does \\(BIC\\) change as the number of predictors increases? How many predictors are in the final model selected based on \\(BIC\\)? You can fill in the code below with either max or min to find the number of predictors in the final model selected based on \\(BIC\\). np_bic &lt;- summary_stats %&gt;% filter(bic == _____(bic)) %&gt;% select(np) %&gt;% pull() Use the data in the summary_stats data frame to plot \\(Adj. R^2\\) versus the number of predictors. Include a vertical line on your plot that shows the number of predictors for the final model you would select based on \\(Adj. R^2\\). Be sure your plot has clear and informative title and axes labels. How does \\(Adj. R^2\\) change as the number of predictors increases? How many predictors are in the final model selected based on \\(Adj. R^2\\)? Use the data in the summary_stats data frame to plot \\(R^2\\) versus the number of predictors. Include a vertical line on your plot that shows the number of predictors for the final model selected based on \\(R^2\\). Be sure your plot has clear and informative title and axes labels. How does \\(R^2\\) change as the number of predictors increases? How many predictors are in the final model selected based on \\(R^2\\)? Should \\(R^2\\) be used as a model selection criterion? Briefly explain why or why not using your answers to Exercises 11 - 13. Choose a final model to predict a baseball player’s Salary from his performance statistics. Display the variables, their coefficients, and the summary statistics from the summary_stats data frame for this model. Briefly explain why you chose the model in the previous exercise. Which model selection criteria did you use (\\(BIC\\), \\(Adj. R^2\\), \\(R^2\\))? Why? What other factors did you consider besides the value of the model selection criteria? 9.1.3 Acknowledgements Part II of this lab was inspired by Lab 6.5 in An Introduction to Statistical Learning and Variable Selection in Regression. "],
["10-logistic.html", " 10 Logistic Regression 10.1 Exercises", " 10 Logistic Regression Over the past ten years, recommendation systems have become increasingly popular as more companies strive to offer customized user experiences. Amazon recommends products you may like based on your browse and purchase history, Netflix recommends movies and TV shows based on your viewing history, and music platforms like Spotify recommend songs you may like based on your listening history. While these recommendation systems are built using a variety of algorithms, they are all trying to achieve the same goal: use the characteristics of the products/movies/music a user is known to like to figure out the products/movies/music the user may like but hasn’t discovered yet. # See [&quot;How Does Spotify Know You So Well?&quot;](https://medium.com/s/story/spotifys-discover-weekly-how-machine-learning-finds-your-new-music-19a41ab76efe) for more information about Spotify&#39;s recommendation algorithms. In today’s lab, we will focus on using the characteristics of songs a user previously played to determine whether or not a user will like a new song. We will use logistic regression to build a model that predicts the probability a user likes a song using the relevant characteristics of that song. 10.0.1 Packages You will need the following packages for today’s lab: library(tidyverse) library(broom) ## Fill in other packages as needed 10.0.2 Data The data in this lab is from the Spotify Song Attributes data set in Kaggle. This data set contains song characteristics of 2017 songs played by a single user and whether or not he liked the song. Since this dataset contains the song preferences of a single user, the scope of the analysis is limited to this particular user. You will use data spotify.csv to build the logistic regression model and test the performance of the model using the songs in test_songs.csv. Click here to download the dataset spotify.csv, and click here to download the dataset test_songs.csv. Upload both files to the to the data folder in your lab-07 project. The Spotify documentation page contains a description of the variables included in this dataset. 10.1 Exercises 10.1.1 Exploratory Data Analysis Read through the Spotify documentation page to learn more about the variables in the dataset. The response variable for this analysis is like, such that 1 indicates that the user likes the song and 0 otherwise. The remaining will be considered as predictor variables in the model. # Part of the code to make `x` a factor. # mutate(x = factor(x)) - Which potential predictor variables are categorical? You only need to include the variables that are in the dataset. - Recode the each of the categorical predictors so they are a `factor` variable type. Choose a quantitative predictor variable. Make the appropriate plot of the response versus this predictor variable. Describe the relationship between the two variables. Choose a categorical predictor variable. Make the appropriate plot of the response versus this predictor variable. Describe the relationship between the two variables. Let’s consider a potential interaction effect between the variables you choose in Exercises 2 and 3. Make the appropriate plots to examine the potential interaction effect. Do these plots suggest there is a significant interaction effect? Briefly explain. In practice, you should do exploratory data analysis for all potential explanatory variables. We did an abbreviated exploratory data analysis to make the assignment more manageable. 10.1.2 Part II: Logistic Regression Model Fit the full model and display the model output. The main objective for the model is to predict whether the user will like a song. Should we use this model for this objective? Briefly explain. Use the step function to perform backward selection. Display the output for the selected model. Briefly describe the criteria used by step to select the final model. For the remainder of this lab, you will use the model chosen by model selection . In practice; however, you would not just stop with the results from the automated model selection procedure and would examine the model further to see if there are any significant interactions, higher-order terms, or if it could even be simplified. Consider the variable duration_ms. Interpret the coefficient of duration_ms and its 95% confidence interval in terms of the odds of the user liking a song. Suppose instead of duration_ms, we use the variable duration_s, the duration of a song in seconds. What would be the effect of duration_s on the odds of the user liking a song? Include the updated coefficient and corresponding 95% confidence interval for duration_s. Assume all other variables in the model are unchanged. Interpret mode and its 95% confidence interval in terms of the odds of the user liking a song. Based on this model, is there evidence of a significant difference in the user’s preference between songs in a major key versus those in a minor key? 10.1.3 Part III: Model Assessment In the next few questions, we will do an abbreviated analysis of the residuals. Create a binned plot of the residuals versus the predicted probabilities. You will first need to use the augment function with the options type.predict = &quot;response&quot; and type.residuals = &quot;response&quot; to get the predicted probabilities and corresponding residuals. Choose a quantitative predictor in the final model. Make the appropriate table or plot to examine the residuals versus this predictor variable. Choose a categorical predictor in the final model. Make the appropriate table or plot to examine the residuals versus this predictor variable. In practice, you should examine plots of residuals versus every predictor variable to make a complete assessment of the model fit. For the sake of time on the lab, you will use these three plots to help make the assessment in Exercise 14. Plot the ROC curve and find the area under the curve. Based on the residual plots and the ROC curve, is this logistic model a good fit for the data? Briefly explain. 10.1.4 Part IV: Prediction You are part of the data science team at Spotify, and your model will be used to make song recommendations to users. The goal is to recommend songs the user has a high probability of liking. As a group, choose a threshold value to distinguish between songs the user will like and those the user won’t like. What is your threshold value? Use the ROC curve to help justify your choice. Now let’s put your model and decision threshold to the test! Use your model to calculate the predicted probability that the user will like the following two songs: “Sign of the Times” by Harry Styles “Hotline Bling” by Drake The data for the songs can be found in test_songs.csv. Using your decision threshold from Question 15, would you recommend “Sign of the Times” to this user? Would your recommend “Hotline Bling” to this user? Briefly explain your decision. The user likes “Hotline Bling” but doesn’t like “Sign of the Times”. How good were your recommendations based on these two songs? If they were good recommendations, explain how the model and threshold helped you distinguish between songs the user would like and those he wouldn’t. If they were not good recommendations, explain the limitations in your model and/or threshold. "],
["11-multinom-logistic.html", " 11 Multinomial Logistic Regression 11.1 Exercises", " 11 Multinomial Logistic Regression The General Social Survey (GSS) has been used to measure trends in attitudes and behaviors in American society since 1972. In addition to collecting demographic information, the survey includes questions used to gauge attitudes about government spending priorities, confidence in institutions, lifestyle, and many other topics. A full description of the survey may be found here. In today’s lab, we will use multinomial logistic regression to understand the relationship between a person’s political views and their attitudes towards government spending on mass transportation projects. To do so, we will use data from the 2010 GSS survey. Refer to the Multinomial Logistic Regression notes for help with concepts and code. 11.0.1 Packages You will need the following packages for today’s lab: library(tidyverse) library(nnet) library(knitr) library(broom) ## Fill in other packages as needed 11.0.2 Data The data for this lab is from the 2016 General Social Survey. The original data set contains 2867 observations and 935 variables. Given the size of the dataset, we will handle it differently in our workflow than we’ve handled data in previous assignments. # [Working with large files](https://help.github.com/en/articles/working-with-large-files) The size of this dataset is 34.3 MB. Compare that to the Spotify dataset from last weeks’ lab which was 149 KB (0.149 MB)! GitHub will not allow you to push files larger than 100 MB and will give you a warning when you push files as large as 50 MB. Though we could push the file we’re working with today to GitHub, it’s large enough that we’d still prefer not to. You have may noticed that each repo contains a file called .gitignore. It contains a list of the files you don’t want commit or push to GitHub. If you look at the .gitignore file for today’s lab, you will notice that gss2016.csv is listed at the bottom. Click here to download gss2016.csv. Upload gss2016.csv into the data folder of your project. Notice that gss2016.csv does not appear in your Git pane. This is because it is being ignored by git, since it is listed in the .gitignore file. You will use the following variables in the lab: natmass: Respondent’s answer to the following prompt: “We are faced with many problems in this country, none of which can be solved easily or inexpensively. I’m going to name some of these problems, and for each one I’d like you to tell me whether you think we’re spending too much money on it, too little money, or about the right amount…are we spending too much, too little, or about the right amount on mass transportation?” age: Age in years. sex: Sex recorded as male or female sei10: Socioeconomic index from 0 to 100 region: Region where interview took place polviews: Respondent’s answer to the following prompt: “We hear a lot of talk these days about liberals and conservatives. I’m going to show you a seven-point scale on which the political views that people might hold are arranged from extremely liberal - point 1 - to extremely conservative - point 7. Where would you place yourself on this scale?” Use the code below to read in the data. gss &lt;- read_csv(&quot;data/gss2016.csv&quot;, na = c(&quot;&quot;, &quot;Don&#39;t know&quot;, &quot;No answer&quot;, &quot;Not applicable&quot;), guess_max = 2867) %&gt;% select(natmass, age, sex, sei10, region, polviews) %&gt;% drop_na() The argument guess_max = 2867 tells the read_csv function to use all of the observations in a column to determine its data type. Without this argument, only the first 1,000 observations would be used to make this determination. This becomes important for a variable like age; though age is coded as numeric data for most of the observations, there are some in which age is coded as &quot;89 or older&quot;. Without the guess_max argument, you will get warnings when loading the data. Note also that only the variables of interest will be loaded, not the entire dataset. This will make for faster computation and knitting as you work on the lab. 11.1 Exercises 11.1.1 Part I: Exploratory Data Analysis # See [Reorder factor levels by hand](https://forcats.tidyverse.org/reference/fct_relevel.html) for documentation about `fct_relevel`. The variable natmass will be the response variable in the model, and you want to compare more opinionated views to the moderate position. Recode natmass so it is a factor variable with &quot;About right&quot; as the baseline. Recode polviews so it is a factor variable type with levels that are in an order that is consistent with question on the survey. Note how the categories are spelled in the data. Make a plot of the distribution of polviews. Which political view occurs most frequently in this data set? Make a plot displaying the relationship between natmass and polviews. Use the plot to describe the relationship between a person’s political views and their views on mass transportation spending. You want to use age as a quantitative variable in your model; however, it is currently a character data type because some observations are coded as &quot;89 or older&quot;. Recode age so that is a numeric variable. Note: Before making the variable numeric, you will need to replace the values &quot;89 or older&quot; with a single value. 11.1.2 Part II: Multinomial Logistic Regression Model You plan to fit a model using age, sex, sei10, and region to understand variation in opinions about spending on mass transportation. Briefly explain why you should fit a multinomial logistic model. Fit the model described in the previous exercise and display the model output. Make any necessary adjustments to the variables so the intercept will have a meaningful interpretation. Be sure About Right is the baseline level. Be sure the full model displays in the knitted document. Interpret the intercept associated with odds of having an opinion of “Too much” versus “About right”. Consider the relationship between age and one’s opinion about spending on mass transportation. Interpret the coefficient of age in terms of the log odds of having an opinion of “Too little” versus “About right”. Interpret the coefficient of age in terms of the odds of having an opinion of “Too little” versus “About right”. In general, what is the relationship between an person’s age and their opinions on mass transportation spending? Now that you have adjusted for some demographic factors, let’s examine whether a person’s political views has a significant impact on their attitude towards spending on mass transportation. Conduct the appropriate test to determine if polviews is a significant predictor of attitude towards spending on mass transportation. State the null and alternative hypothesis, display all relevant code and output, and state your conclusion in the context of the problem. Choose the appropriate model based on the results from the test. Use this model for the next part of the lab. 11.1.3 Part III: Model Fit Calculate the predicted probabilities and residuals from your model. Plot the binned residuals versus the predicted probabilities for each category of natmass. You will have three plots. # You can change the size of your plots, so you can fit multiple plots on a single page. Include the arguments `fig.height = ` and `fig.width = ` in the header of the code chunk to change the plot size. # See [Using R Markdown](https://rstudio.github.io/dygraphs/r-markdown.html) for an example. Use binned residual plots to examine the residuals versus each of the quantitative variables. Create binned plots of the residuals for each category of natmass versus age. You will have three plots. Create binned plots of the residuals for each category of natmass versus sei10. You will have three plots. To examine the residuals versus each categorical predictor, you will look at the average residuals for each each category of the categorical variables. For each level of natmass, calculate the average residuals across categories of sex. For each category of natmass, calculate the average residuals across categories of region. For each category of natmass, calculate the average residuals across categories of polviews. Based on the analysis of the residuals in Exercises 12 - 14, is the model an appropriate fit for the data? Explain. Regardless of your asssesment of the residuals, use your model for the remainder of the lab. 11.1.4 Part IV: Using the Model Use your model to describe the relationship between one’s political views and their attitude towards spending on mass transportation. Use your model to predict the category of natmass for each observation in your dataset. Display a table of the actual versus the predicted natmass. What is the misclassification rate? 11.1.5 Acknowledgements The “Data” section is largely inspired by datasciencebox.org. "],
["12-together.html", " 12 Putting It All Together", " 12 Putting It All Together In this lab, you will put together everything you’ve learned thus far. Unlike previous lab assignments, your lab write up will be in the form of a small report (rather than numbered exercises). Though this analysis will not be as in-depth as your analysis in the final project, this assignment will give your group practice organizing the results of a statistical analysis to tell a complete narrative. You will also practice imputing missing data and using k-fold cross validation to assess your model’s performance on test data. 12.0.1 Packages You will need the following packages for today’s lab: library(tidyverse) library(dslabs) ## Fill in other packages as needed 12.0.2 Data The data for this lab is the gapminder dataset in the dslabs package. This dataset contains health and income data for 184 countries during the years 1960 to 2016. After loading the dslabs package, you can type ?gapminder in the console to to see the variables in the dataset. You will only use data from 2011 in this lab. 12.0.3 Exercises The goal of this analysis is to build a regression model that could be used to predict a country’s gross domestic product (gdp) using the other characteristics included in the data. Introduction Brief introduction of the data and the research question Exploratory Data Analysis At a minimum, your exploratory data analysis should include the following: Analysis of each variable Dealing with missing values using imputation methods Analysis of the relationships between variables Discussion of any potential transformations, if needed Regression Model At a minimum, the discussion for the final regression model should include the following: Brief discussion about the type of model you used (multiple linear regression, logistic, multinomial logistic regression) and why Discussion of any transformations on the response and/or explanatory variables, if applicable Display of the final model Test of interesting interactions Conclusions drawn from the model, including any interesting insights based on the model coefficients Assumptions At a minimum, the discussion of model assumptions should include the following: Appropriate residual plots Check for influential points Check for multicollinearity Discussion of whether or not assumptions are met and how any issues may affect conclusions drawn from the model Model Validation At a minimum, the discussion of the model validation should include the following: Results and discussion from a 5-fold cross validation Conclusion Brief summary of the conclusions drawn from the analysis. "],
["13-matrix-form-of-linear-regression.html", " 13 Matrix Form of Linear Regression 13.1 Introduction 13.2 Matrix Form for the Regression Model 13.3 Estimating the Coefficients 13.4 Variance-covariance matrix of the coefficients", " 13 Matrix Form of Linear Regression This document provides the details for the matrix form of multiple linear regression. We assume the reader has familiarity with some matrix alegbra. Please see Chapter 1 of An Introduction to Statistical Learning for a brief review of matrix algebra. 13.1 Introduction Suppose we have \\(n\\) observations. Let the \\(i^{th}\\) be \\((x_{i1}, \\ldots, x_{ip}, y_i)\\), such that \\(x_{i1}, \\ldots, x_{ip}\\) are the explanatory variables (predictors) and \\(y_i\\) is the response variable. We assume the data can be modeled using the least-squares regression model, such that the mean response for a given combination of explanatory variables follows the form in (). \\[\\begin{equation} \\label{basic_model} y = \\beta_0 + \\beta_1 x_1 + \\dots + \\beta_p x_p \\end{equation}\\] We can write the response for the \\(i^{th}\\) observation as shown in () \\[\\begin{equation} \\label{ind_response} y_i = \\beta_0 + \\beta_1 x_{i1} + \\dots + \\beta_p x_{ip} + \\epsilon_i \\end{equation}\\] such that \\(\\epsilon_i\\) is the amount \\(y_i\\) deviates from \\(\\mu\\{y|x_{i1}, \\ldots, x_{ip}\\}\\), the mean response for a given combination of explanatory variables. We assume each \\(\\epsilon_i \\sim N(0,\\sigma^2)\\), where \\(\\sigma^2\\) is a constant variance for the distribution of the response \\(y\\) for any combination of explanatory variables \\(x_1, \\ldots, x_p\\). 13.2 Matrix Form for the Regression Model We can represent the () and () using matrix notation. Let \\[\\begin{equation} \\label{matrix notation} \\mathbf{Y} = \\begin{bmatrix}y_1 \\\\ y_2 \\\\ \\vdots \\\\y_n\\end{bmatrix} \\hspace{15mm} \\mathbf{X} = \\begin{bmatrix}x_{11} &amp; x_{12} &amp; \\dots &amp; x_{1p} \\\\ x_{21} &amp; x_{22} &amp; \\dots &amp; x_{2p} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots\\\\ x_{n1} &amp; x_{n2} &amp; \\dots &amp; x_{np} \\end{bmatrix} \\hspace{15mm} \\boldsymbol{\\beta}= \\begin{bmatrix}\\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_p \\end{bmatrix} \\hspace{15mm} \\boldsymbol{\\epsilon}= \\begin{bmatrix}\\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{bmatrix} \\end{equation}\\] Thus, \\[\\mathbf{Y} = \\mathbf{X}\\boldsymbol{\\beta} + \\mathbf{\\epsilon}\\] Therefore the estimated response for a given combination of explanatory variables and the associated residuals can be written as \\[\\begin{equation} \\label{matrix_mean} \\hat{\\mathbf{Y}} = \\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\hspace{10mm} \\mathbf{e} = \\mathbf{Y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\end{equation}\\] 13.3 Estimating the Coefficients The least-squares model is the one that minimizes the sum of the squared residuals. Therefore, we want to find the coefficients, \\(\\hat{\\boldsymbol{\\beta}}\\) that minimizes \\[\\begin{equation} \\label{sum_sq_resid} \\sum\\limits_{i=1}^{n} e_{i}^2 = \\mathbf{e}^T\\mathbf{e} = (\\mathbf{Y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}})^T(\\mathbf{Y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}}) \\end{equation}\\] where \\(\\mathbf{e}^T\\), the transpose of the matrix \\(\\mathbf{e}\\). \\[\\begin{equation} \\label{model_equation} (\\mathbf{Y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}})^T(\\mathbf{Y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}}) = (\\mathbf{Y}^T\\mathbf{Y} - \\mathbf{Y}^T \\mathbf{X}\\hat{\\boldsymbol{\\beta}} - (\\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{X}^T\\mathbf{Y} + \\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{X}^T\\mathbf{X} \\hat{\\boldsymbol{\\beta}}) \\end{equation}\\] Note that \\((\\mathbf{Y^T}\\mathbf{X}\\hat{\\boldsymbol{\\beta}})^T = \\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{X}^T\\mathbf{Y}\\). Since these are both constants (i.e. \\(1\\times 1\\) vectors), \\(\\mathbf{Y^T}\\mathbf{X}\\hat{\\boldsymbol{\\beta}} = \\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{X}^T\\mathbf{Y}\\). Thus, () becomes \\[\\begin{equation} \\mathbf{Y}^T\\mathbf{Y} - 2 \\mathbf{X}^T\\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{Y} + \\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{X}^T\\mathbf{X} \\hat{\\boldsymbol{\\beta}} \\end{equation}\\] Since we want to find the \\(\\hat{\\boldsymbol{\\beta}}\\) that minimizes (), will find the value of \\(\\hat{\\boldsymbol{\\beta}}\\) such that the derivative with respect to \\(\\hat{\\boldsymbol{\\beta}}\\) is equal to 0. \\[\\begin{equation} \\begin{aligned} \\frac{\\partial \\mathbf{e}^T\\mathbf{e}}{\\partial \\hat{\\boldsymbol{\\beta}}} &amp; = \\frac{\\partial}{\\partial \\hat{\\boldsymbol{\\beta}}}(\\mathbf{Y}^T\\mathbf{Y} - 2 \\mathbf{X}^T\\hat{\\boldsymbol{\\beta}}{}^T\\mathbf{Y} + \\hat{\\boldsymbol{\\beta}}{}^{T}\\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}}) = 0 \\\\[10pt] &amp;\\Rightarrow - 2 \\mathbf{X}^T\\mathbf{Y} + 2 \\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}} = 0 \\\\[10pt] &amp; \\Rightarrow 2 \\mathbf{X}^T\\mathbf{Y} = 2 \\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\\\[10pt] &amp; \\Rightarrow \\mathbf{X}^T\\mathbf{Y} = \\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\\\[10pt] &amp; \\Rightarrow (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\\\[10pt] &amp; \\Rightarrow (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y} = \\mathbf{I}\\hat{\\boldsymbol{\\beta}} \\end{aligned} \\end{equation}\\] Thus, the estimate of the model coefficients is \\(\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}\\). 13.4 Variance-covariance matrix of the coefficients We will use two properties to derive the form of the variance-covarinace matrix of the coefficients: \\(E[\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^T] = \\sigma^2I\\) \\(\\hat{\\boldsymbol{\\beta}} = \\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1}\\epsilon\\) First, we will show that \\(E[\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^T] = \\sigma^2I\\) \\[\\begin{equation} \\label{expected_error} \\begin{aligned} E[\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^T] &amp;= E \\begin{bmatrix}\\epsilon_1 &amp; \\epsilon_2 &amp; \\dots &amp; \\epsilon_n \\end{bmatrix}\\begin{bmatrix}\\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{bmatrix} \\\\[10pt] &amp; = E \\begin{bmatrix} \\epsilon_1^2 &amp; \\epsilon_1 \\epsilon_2 &amp; \\dots &amp; \\epsilon_1 \\epsilon_n \\\\ \\epsilon_2 \\epsilon_1 &amp; \\epsilon_2^2 &amp; \\dots &amp; \\epsilon_2 \\epsilon_n \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\epsilon_n \\epsilon_1 &amp; \\epsilon_n \\epsilon_2 &amp; \\dots &amp; \\epsilon_n^2 \\end{bmatrix} \\\\[10pt] &amp; = \\begin{bmatrix} E[\\epsilon_1^2] &amp; E[\\epsilon_1 \\epsilon_2] &amp; \\dots &amp; E[\\epsilon_1 \\epsilon_n] \\\\ E[\\epsilon_2 \\epsilon_1] &amp; E[\\epsilon_2^2] &amp; \\dots &amp; E[\\epsilon_2 \\epsilon_n] \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ E[\\epsilon_n \\epsilon_1] &amp; E[\\epsilon_n \\epsilon_2] &amp; \\dots &amp; E[\\epsilon_n^2] \\end{bmatrix} \\end{aligned} \\end{equation}\\] Recall, the regression assumption that the errors \\(\\epsilon_i&#39;s\\) are Normally distributed with mean 0 and variance \\(\\sigma^2\\). Thus, \\(E(\\epsilon_i^2) = Var(\\epsilon_i) = \\sigma^2\\) for all \\(i\\). Additionally, recall the regression assumption that the errors are uncorrelated, i.e. \\(E(\\epsilon_i \\epsilon_j) = Cov(\\epsilon_i, \\epsilon_j) = 0\\) for all \\(i,j\\). Using these assumptions, we can write () as \\[\\begin{equation} E[\\mathbf{\\epsilon}\\mathbf{\\epsilon}^T] = \\begin{bmatrix} \\sigma^2 &amp; 0 &amp; \\dots &amp; 0 \\\\ 0 &amp; \\sigma^2 &amp; \\dots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\dots &amp; \\sigma^2 \\end{bmatrix} = \\sigma^2 \\mathbf{I} \\end{equation}\\] where \\(\\mathbf{I}\\) is the \\(n \\times n\\) identity matrix. Next, we show that \\(\\hat{\\boldsymbol{\\beta}} = \\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1}\\epsilon\\). Recall that the \\(\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}\\) and \\(\\mathbf{Y} = \\mathbf{X}\\mathbf{\\beta} + \\mathbf{\\epsilon}\\). Then, \\[\\begin{equation} \\begin{aligned} \\hat{\\boldsymbol{\\beta}} &amp;= (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y} \\\\[10pt] &amp;= (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T(\\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}) \\\\[10pt] &amp;= \\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T \\mathbf{\\epsilon} \\\\ \\end{aligned} \\end{equation}\\] Using these two properties, we derive the form of the variance-covariance matrix for the coefficients. Note that the covariance matrix is \\(E[(\\hat{\\boldsymbol{\\beta}} - \\boldsymbol{\\beta})(\\hat{\\boldsymbol{\\beta}} - \\boldsymbol{\\beta})^T]\\) \\[\\begin{equation} \\begin{aligned} E[(\\hat{\\boldsymbol{\\beta}} - \\boldsymbol{\\beta})(\\hat{\\boldsymbol{\\beta}} - \\boldsymbol{\\beta})^T] &amp;= E[(\\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T \\boldsymbol{\\epsilon} - \\boldsymbol{\\beta})(\\boldsymbol{\\beta} + (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T \\boldsymbol{\\epsilon} - \\boldsymbol{\\beta})^T]\\\\[10pt] &amp; = E[(\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T \\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^T\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}] \\\\[10pt] &amp; = (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T E[\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^T]\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\\\[10pt] &amp; = (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T (\\sigma^2\\mathbf{I})\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\\\ &amp;= \\sigma^2\\mathbf{I}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\\\[10pt] &amp; = \\sigma^2\\mathbf{I}(\\mathbf{X}^T\\mathbf{X})^{-1}\\\\[10pt] &amp; = \\sigma^2(\\mathbf{X}^T\\mathbf{X})^{-1} \\\\ \\end{aligned} \\end{equation}\\] "],
["14-log-transformations-in-linear-regression.html", " 14 “Log Transformations in Linear Regression” 14.1 Log-transformation on the response variable 14.2 Log-transformation on the predictor variable 14.3 Log-transformation on the the response and predictor variable", " 14 “Log Transformations in Linear Regression” This document provides details about the model interpretion when the predictor and/or response variables are log-transformed. For simplicity, we will discuss transformations for the simple linear regression model: \\[\\begin{equation} \\label{orig} y = \\beta_0 + \\beta_1 x \\end{equation}\\] All results and interpretations can be easily extended to transformations in multiple regression models. Note: log refers to the natural logarithm. 14.1 Log-transformation on the response variable Suppose we fit a linear regression model with \\(\\log(y)\\), the log-transformed \\(y\\), as the response variable. Under this model, we assume a linear relationship exists between \\(x\\) and \\(\\log(y)\\), such that \\(\\log(y) \\sim N(\\beta_0 + \\beta_1 x, \\sigma^2)\\) for some \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\sigma^2\\). In other words, we can model the relationship between \\(x\\) and \\(\\log(y)\\) using the model in (). \\[\\begin{equation} \\label{log-y} \\log(y) = \\beta_0 + \\beta_1 x \\end{equation}\\] If we interpret the model in terms of \\(\\log(y)\\), then we can use the usual interpretations for slope and intercept. When reporting results, however, it is best to give all interpretations in terms of the original response variable \\(y\\), since interpretations using log-transformed variables are often more difficult to truly understand. In order to get back on the original scale, we need to use the exponential function (also known as the anti-log), \\(\\exp\\{x\\} = e^x\\). Therefore, we use the model in () for interpretations and predictions, we will use () to state our conclusions in terms of \\(y\\). \\[\\begin{equation} \\label{exp-y} \\begin{aligned} &amp;\\exp\\{\\log(y)\\} = \\exp\\{\\beta_0 + \\beta_1 x\\} \\\\[10pt] \\Rightarrow &amp;y = \\exp\\{\\beta_0 + \\beta_1 x\\} \\\\[10pt] \\Rightarrow &amp;y = \\exp\\{\\beta_0\\}\\exp\\{\\beta_1 x\\} \\end{aligned} \\end{equation}\\] In order to interpret the slope and intercept, we need to first understand the relationship between the mean, median and log transformations. 14.1.1 Mean, Median, and Log Transformations Suppose we have a dataset y that contains the following observations: y &lt;- c(3,5,6,7,8) y If we log-transform the values of y then calculate the mean and median, we have log_y &lt;- tibble(log_y = log(y)) summary &lt;- log_y %&gt;% summarise(mean_log_y = mean(log_y), median_log_y = median(log_y)) kable(summary,digits=5) If we calculate the mean and median of y, then log-transform the mean and median, we have centers &lt;- tibble(y) %&gt;% summarise(mean_y = mean(y), median_y = median(y)) summary2 &lt;- centers %&gt;% summarise(log_mean = log(mean_y), log_median = log(median_y)) kable(summary2,digits=5) This is a simple illustration to show \\(\\text{Mean}[{\\log(y)}] \\neq \\log[\\text{Mean}(y)]\\) - the mean and log are not commutable \\(\\text{Median}[\\log(y)] = \\log[\\text{Median}(y)]\\) - the median and log are commutable 14.1.2 Interpretaton of model coefficients Using (), the mean \\(\\log(y)\\) for any given value of \\(x\\) is \\(\\beta_0 + \\beta_1 x\\); however, this does not indicate that the mean of \\(y = \\exp\\{\\beta_0 + \\beta_1 x\\}\\) (see previous section). From the assumptions of linear regression, we assume that for any given value of \\(x\\), the distribution of \\(\\log(y)\\) is Normal, and therefore symmetric. Thus the median of \\(\\log(y)\\) is equal to the mean of \\(\\log(y)\\), i.e \\(\\text{Median}(\\log(y)) = \\beta_0 + \\beta_1 x\\). Since the log and the median are commutable, \\(\\text{Median}(\\log(y)) = \\beta_0 + \\beta_1 x \\Rightarrow \\text{Median}(y) = \\exp\\{\\beta_0 + \\beta_1 x\\}\\). Thus, when we log-transform the response variable, the interpretation of the intercept and slope are in terms of the effect on the median of \\(y\\). Intercept: The intercept is expected median of \\(y\\) when the predictor variable equals 0. Therefore, when \\(x=0\\), \\[\\begin{equation} \\begin{aligned} &amp;\\log(y) = \\beta_0 + \\beta_1 \\times 0 = \\beta_0 \\\\[10pt] \\Rightarrow &amp;y = \\exp\\{\\beta_0\\} \\end{aligned} \\end{equation}\\] Interpretation: When \\(x=0\\), the median of \\(y\\) is expected to be \\(\\exp\\{\\beta_0\\}\\). Slope: The slope is the expected change in the median of \\(y\\) when \\(x\\) increases by 1 unit. The change in the median of \\(y\\) is \\[\\begin{equation} \\exp\\{[\\beta_0 + \\beta_1 (x+1)] - [\\beta_0 + \\beta_1 x]\\} = \\frac{\\exp\\{\\beta_0 + \\beta_1 (x+1)\\}}{\\exp\\{\\beta_0 + \\beta_1 x\\}} = \\frac{\\exp\\{\\beta_0\\}\\exp\\{\\beta_1 x\\}\\exp\\{\\beta_1\\}}{\\exp\\{\\beta_0\\}\\exp\\{\\beta_1 x\\}} = \\exp\\{\\beta_1\\} \\end{equation}\\] Thus, the median of \\(y\\) for \\(x+1\\) is \\(\\exp\\{\\beta_1\\}\\) times the median of \\(y\\) for \\(x\\). Interpretation: When \\(x\\) increases by one unit, the median of \\(y\\) is expected to multiply by a factor of \\(\\exp\\{\\beta_1\\}\\). 14.2 Log-transformation on the predictor variable Suppose we fit a linear regression model with \\(\\log(x)\\), the log-transformed \\(x\\), as the predictor variable. Under this model, we assume a linear relationship exists between \\(\\log(x)\\) and \\(y\\), such that \\(y \\sim N(\\beta_0 + \\beta_1 \\log(x), \\sigma^2)\\) for some \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\sigma^2\\). In other words, we can model the relationship between \\(\\log(x)\\) and \\(y\\) using the model in (). \\[\\begin{equation} \\label{log-x} y = \\beta_0 + \\beta_1 \\log(x) \\end{equation}\\] Intercept: The intercept is the mean of \\(y\\) when \\(\\log(x) = 0\\), i.e. \\(x = 1\\). Interpretation: When \\(x = 1\\) \\((\\log(x) = 0)\\), the mean of \\(y\\) is expected to be \\(\\beta_0\\). Slope: The slope is interpreted in terms of the change in the mean of \\(y\\) when \\(x\\) is mutliplied by a factor of \\(C\\), since \\(\\log(Cx) = \\log(x) + \\log(C)\\). Thus, when \\(x\\) is multiplied by a factor of \\(C\\), the change in the mean of \\(y\\) is \\[\\begin{equation} \\begin{aligned} [\\beta_0 + \\beta_1 \\log(Cx)] - [\\beta_0 + \\beta_1 \\log(x)] &amp;= \\beta_1 [\\log(Cx) - \\log(x)] \\\\[10pt] &amp; = \\beta_1[\\log(C) + \\log(x) - \\log(x)] \\\\[10pt] &amp; = \\beta_1 \\log(C) \\end{aligned} \\end{equation}\\] Thus the mean of \\(y\\) changes by \\(\\beta_1 \\log(C)\\) units. Interpretation: When \\(x\\) is multiplied by a factor of \\(C\\), the mean of \\(y\\) is expected to change by \\(\\beta_1 \\log(C)\\) units. For example, if \\(x\\) is doubled, then the mean of \\(y\\) is expected to change by \\(\\beta_1 \\log(2)\\) units. 14.3 Log-transformation on the the response and predictor variable Suppose we fit a linear regression model with \\(\\log(x)\\), the log-transformed \\(x\\), as the predictor variable and \\(\\log(y)\\), the log-transformed \\(y\\), as the response variable. Under this model, we assume a linear relationship exists between \\(\\log(x)\\) and \\(\\log(y)\\), such that \\(\\log(y) \\sim N(\\beta_0 + \\beta_1 \\log(x), \\sigma^2)\\) for some \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\sigma^2\\). In other words, we can model the relationship between \\(\\log(x)\\) and \\(\\log(y)\\) using the model in (). \\[\\begin{equation} \\label{log-x-y} \\log(y) = \\beta_0 + \\beta_1 \\log(x) \\end{equation}\\] Because the response variable is log-transformed, the interpretations on the original scale will be in terms of the median of \\(y\\) (see the section on the log-transformed response variable for more detail). Intercept: The intercept is the mean of \\(y\\) when \\(\\log(x) = 0\\), i.e. \\(x = 1\\). Therefore, when \\(\\log(x) = 0\\), \\[\\begin{equation} \\begin{aligned} &amp;\\log(y) = \\beta_0 + \\beta_1 \\times 0 = \\beta_0 \\\\[10pt] \\Rightarrow &amp;y = \\exp\\{\\beta_0\\} \\end{aligned} \\end{equation}\\] Interpretation: When \\(x = 1\\) \\((\\log(x) = 0)\\), the median of \\(y\\) is expected to be \\(\\exp\\{\\beta_0\\}\\). Slope: The slope is interpreted in terms of the change in the median \\(y\\) when \\(x\\) is mutliplied by a factor of \\(C\\), since \\(\\log(Cx) = \\log(x) + \\log(C)\\). Thus, when \\(x\\) is multiplied by a factor of \\(C\\), the change in the median of \\(y\\) is \\[\\begin{equation} \\begin{aligned} \\exp\\{[\\beta_0 + \\beta_1 \\log(Cx)] - [\\beta_0 + \\beta_1 \\log(x)]\\} &amp;= \\exp\\{\\beta_1 [\\log(Cx) - \\log(x)]\\} \\\\[10pt] &amp; = \\exp\\{\\beta_1[\\log(C) + \\log(x) - \\log(x)]\\} \\\\[10pt] &amp; = \\exp\\{\\beta_1 \\log(C)\\} = C^{\\beta_1} \\end{aligned} \\end{equation}\\] Thus, the median of \\(y\\) for \\(Cx\\) is \\(C^{\\beta_1}\\) times the median of \\(y\\) for \\(x\\). Interpretation: When \\(x\\) is multiplied by a factor of \\(C\\), the median of \\(y\\) is expected to multiple by a factor of \\(C^{\\beta_1}\\). For example, if \\(x\\) is doubled, then the median of \\(y\\) is expected to multiply by \\(2^{\\beta_1}\\). "],
["15-details-about-model-diagnostics.html", " 15 Details about Model Diagnostics 15.1 Introduction 15.2 Matrix Form for the Regression Model 15.3 Hat Matrix &amp; Leverage 15.4 Standardized Residuals 15.5 Cook’s Distance", " 15 Details about Model Diagnostics This document discusses some of the mathematical details of the model diagnostics - leverage, standardized residuals, and Cook’s distance. We assume the reader knowledge of the matrix form for multiple linear regression.Please see Matrix Form of Linear Regression for a review. 15.1 Introduction Suppose we have \\(n\\) observations. Let the \\(i^{th}\\) be \\((x_{i1}, \\ldots, x_{ip}, y_i)\\), such that \\(x_{i1}, \\ldots, x_{ip}\\) are the explanatory variables (predictors) and \\(y_i\\) is the response variable. We assume the data can be modeled using the least-squares regression model, such that the mean response for a given combination of explanatory variables follows the form in (). \\[\\begin{equation} \\label{basic_model} y = \\beta_0 + \\beta_1 x_1 + \\dots + \\beta_p x_p \\end{equation}\\] We can write the response for the \\(i^{th}\\) observation as shown in () \\[\\begin{equation} \\label{ind_response} y_i = \\beta_0 + \\beta_1 x_{i1} + \\dots + \\beta_p x_{ip} + \\epsilon_i \\end{equation}\\] such that \\(\\epsilon_i\\) is the amount \\(y_i\\) deviates from \\(\\mu\\{y|x_{i1}, \\ldots, x_{ip}\\}\\), the mean response for a given combination of explanatory variables. We assume each \\(\\epsilon_i \\sim N(0,\\sigma^2)\\), where \\(\\sigma^2\\) is a constant variance for the distribution of the response \\(y\\) for any combination of explanatory variables \\(x_1, \\ldots, x_p\\). 15.2 Matrix Form for the Regression Model We can represent the () and () using matrix notation. Let \\[\\begin{equation} \\label{matrix notation} \\mathbf{Y} = \\begin{bmatrix}y_1 \\\\ y_2 \\\\ \\vdots \\\\y_n\\end{bmatrix} \\hspace{15mm} \\mathbf{X} = \\begin{bmatrix}x_{11} &amp; x_{12} &amp; \\dots &amp; x_{1p} \\\\ x_{21} &amp; x_{22} &amp; \\dots &amp; x_{2p} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots\\\\ x_{n1} &amp; x_{n2} &amp; \\dots &amp; x_{np} \\end{bmatrix} \\hspace{15mm} \\boldsymbol{\\beta}= \\begin{bmatrix}\\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_p \\end{bmatrix} \\hspace{15mm} \\boldsymbol{\\epsilon}= \\begin{bmatrix}\\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{bmatrix} \\end{equation}\\] Thus, \\[\\mathbf{Y} = \\mathbf{X}\\boldsymbol{\\beta} + \\mathbf{\\epsilon}\\] Therefore the estimated response for a given combination of explanatory variables and the associated residuals can be written as \\[\\begin{equation} \\label{matrix_mean} \\hat{\\mathbf{Y}} = \\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\hspace{10mm} \\mathbf{e} = \\mathbf{Y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\end{equation}\\] 15.3 Hat Matrix &amp; Leverage Recall from the notes Matrix Form of Linear Regression that \\(\\hat{\\boldsymbol{\\beta}}\\) can be written as the following: \\[\\begin{equation} \\label{beta-hat} \\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y} \\end{equation}\\] Combining () and (), we can write \\(\\hat{\\mathbf{Y}}\\) as the following: \\[\\begin{equation} \\label{y-hat} \\begin{aligned} \\hat{\\mathbf{Y}} &amp;= \\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\\\[10pt] &amp;= \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y}\\\\ \\end{aligned} \\end{equation}\\] We define the hat matrix as an \\(n \\times n\\) matrix of the form \\(\\mathbf{H} = \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\). Thus () becomes \\[\\begin{equation} \\label{y-hat-matrix} \\hat{\\mathbf{Y}} = \\mathbf{H}\\mathbf{Y} \\end{equation}\\] The diagonal elements of the hat matrix are a measure of how far the predictor variables of each observation are from the means of the predictor variables. For example, \\(h_{ii}\\) is a measure of how far the values of the predictor variables for the \\(i^{th}\\) observation, \\(x_{i1}, x_{i2}, \\ldots, x_{ip}\\), are from the mean values of the predictor variables, \\(\\bar{x}_1, \\bar{x}_2, \\ldots, \\bar{x}_p\\). In the case of simple linear regression, the \\(i^{th}\\) diagonal, \\(h_{ii}\\), can be written as \\[h_{ii} = \\frac{1}{n} + \\frac{(x_i - \\bar{x})^2}{\\sum_{j=1}^{n}(x_j-\\bar{x})^2}\\] We call these diagonal elements, the leverage of each observation. The diagonal elements of the hat matrix have the following properties: \\(0 \\leq h_ii \\leq 1\\) \\(\\sum\\limits_{i=1}^{n} h_{ii} = p+1\\), where \\(p\\) is the number of predictor variables in the model. The mean hat value is \\(\\bar{h} = \\frac{\\sum\\limits_{i=1}^{n} h_{ii}}{n} = \\frac{p+1}{n}\\). Using these properties, we consider a point to have high leverage if it has a leverage value that is more than 2 times the average. In other words, observations with leverage greater than \\(\\frac{2(p+1)}{n}\\) are considered to be high leverage points, i.e. outliers in the predictor variables. We are interested in flagging high leverage points, because they may have an influence on the regression coefficients. When there are high leverage points in the data, the regression line will tend towards those points; therefore, one property of high leverage points is that they tend to have small residuals. We will show this by rewriting the residuals from () using (). \\[\\begin{equation} \\label{resid-hat} \\begin{aligned} \\mathbf{e} &amp;= \\mathbf{Y} - \\hat{\\mathbf{Y}} \\\\[10pt] &amp; = \\mathbf{Y} - \\mathbf{H}\\mathbf{Y} \\\\[10pt] &amp;= (1-\\mathbf{H})\\mathbf{Y} \\end{aligned} \\end{equation}\\] Note that the identity matrix and hat matrix are idempotent, i.e. \\(\\mathbf{I}\\mathbf{I} = \\mathbf{I}\\), \\(\\mathbf{H}\\mathbf{H} = \\mathbf{H}\\). Thus, \\((\\mathbf{I} - \\mathbf{H}\\) is also idempotent. These matrices are also symmetric. Using these properties and (), we have that the variance-covariance matrix of the residuals \\(\\boldsymbol{e}\\), is \\[\\begin{equation} \\label{resid-var} \\begin{aligned} Var(\\mathbf{e}) &amp;= \\mathbf{e}\\mathbf{e}^T \\\\[10pt] &amp;= (1-\\mathbf{H})Var(\\mathbf{Y})^T(1-\\mathbf{H})^T \\\\[10pt] &amp;= (1-\\mathbf{H})\\hat{\\sigma}^2(1-\\mathbf{H})^T \\\\[10pt] &amp;= \\hat{\\sigma}^2(1-\\mathbf{H})(1-\\mathbf{H}) \\\\[10pt] &amp;= \\hat{\\sigma}^2(1-\\mathbf{H}) \\end{aligned} \\end{equation}\\] where \\(\\hat{\\sigma}^2 = \\frac{\\sum_{i=1}^{n}e_i^2}{n-p-1}\\) is the estimated regression variance. Thus, the variance of the \\(i^{th}\\) residual is \\(Var(e_i) = \\hat{\\sigma}^2(1-h_{ii})\\). Therefore, the higher the leverage, the smaller the variance of the residual. Because the expected value of the residuals is 0, we conclude that points with high leverage tend to have smaller residuals than points with lower leverage. 15.4 Standardized Residuals In general, we standardize a value by shifting by the expected value and rescaling by the standard deviation (or standard error). Thus, the \\(i^{th}\\) standardized residual takes the form \\[std.res_i = \\frac{e_i - E(e_i)}{SE(e_i)}\\] The expected value of the residuals is 0, i.e. \\(E(e_i) = 0\\). From (), the standard error of the residual is \\(SE(e_i) = \\hat{\\sigma}\\sqrt{1-h_{ii}}\\). Therefore, \\[\\begin{equation} \\label{std.resid.} std.res_i = \\frac{e_i}{\\hat{\\sigma}\\sqrt{1-h_{ii}}} \\end{equation}\\] 15.5 Cook’s Distance Cook’s distance is a measure of how much each observation influences the model coefficients, and thus the predicted values. The Cook’s distance for the \\(i^{th}\\) observation can be written as \\[\\begin{equation} \\label{cooksd} D_i = \\frac{(\\hat{\\mathbf{Y}} -\\hat{\\mathbf{Y}}_{(i)})^T(\\hat{\\mathbf{Y}} -\\hat{\\mathbf{Y}}_{(i)})}{(p+1)\\hat{\\sigma}} \\end{equation}\\] where \\(\\hat{\\mathbf{Y}}_{(i)}\\) is the vector of predicted values from the model fitted when the \\(i^{th}\\) observation is deleted. Cook’s Distance can be calculated without deleting observations one at a time, since () below is mathematically equivalent to (). \\[\\begin{equation} \\label{cooksd-v2} D_i = \\frac{1}{p+1}std.res_i^2\\Bigg[\\frac{h_{ii}}{(1-h_{ii})}\\Bigg] = \\frac{e_i^2}{(p+1)\\hat{\\sigma}^2(1-h_{ii})}\\Bigg[\\frac{h_{ii}}{(1-h_{ii})}\\Bigg] \\end{equation}\\] "],
["16-model-selection-criteria-aic-bic.html", " 16 Model Selection Criteria: AIC &amp; BIC 16.1 Maximum Likelihood Estimation of \\(\\boldsymbol{\\beta}\\) and \\(\\sigma\\) 16.2 AIC 16.3 BIC", " 16 Model Selection Criteria: AIC &amp; BIC This document discusses some of the mathematical details of Akaike’s Information Criterion (AIC) and Schwarz’s Bayesian Information Criterion (BIC). We assume the reader knowledge of the matrix form for multiple linear regression.Please see Matrix Form of Linear Regression for a review. 16.1 Maximum Likelihood Estimation of \\(\\boldsymbol{\\beta}\\) and \\(\\sigma\\) To understand the formulas for AIC and BIC, we will first briefly explain the likelihood function and maximum likelihood estimates for regression. Let \\(\\mathbf{Y}\\) be \\(n \\times 1\\) matrix of responses, \\(\\mathbf{X}\\), the \\(n \\times (p+1)\\) matrix of predictors, and \\(\\boldsymbol{\\beta}\\), \\((p+1) \\times 1\\) matrix of coefficients. If the multiple linear regression model is correct then, \\[\\begin{equation} \\label{norm-assumption} \\mathbf{Y} \\sim N(\\mathbf{X}\\boldsymbol{\\beta}, \\sigma^2) \\end{equation}\\] When we do linear regression, our goal is to estimate the unknown parameters \\(\\boldsymbol{\\beta}\\) and \\(\\sigma^2\\) from (). In Matrix Form of Linear Regression, we showed a way to estimate these parameters using matrix alegbra. Another approach for estimating \\(\\boldsymbol{\\beta}\\) and \\(\\sigma^2\\) is using maximum likelihood estimation. A likelihood function is used to summarise the evidence from the data in support of each possible value of a model parameter. Using (), we will write the likelihood function for linear regression as \\[\\begin{equation} \\label{lr} L(\\mathbf{X}, \\mathbf{Y}|\\boldsymbol{\\beta}, \\sigma^2) = \\prod\\limits_{i=1}^n (2\\pi \\sigma^2)^{-\\frac{1}{2}} \\exp\\bigg\\{-\\frac{1}{2\\sigma^2}\\sum\\limits_{i=1}^n(Y_i - \\mathbf{X}_i \\boldsymbol{\\beta})^T(Y_i - \\mathbf{X}_i \\boldsymbol{\\beta})\\bigg\\} \\end{equation}\\] where \\(Y_i\\) is the \\(i^{th}\\) response and \\(\\mathbf{X}_i\\) is the vector of predictors for the \\(i^{th}\\) observation. One approach estimating \\(\\boldsymbol{\\beta}\\) and \\(\\sigma^2\\) is to find the values of those parameters that maximize the likelihood in (), i.e. maximum likelhood estimation. To make the calculations more manageable, instead of maximizing the likelihood function, we will instead maximize its logarithm, i.e. the log-likelihood function. The values of the parameters that maximize the log-likelihood function are those that maximize the likelihood function. The log-likelihood function we will maximize is \\[\\begin{equation} \\label{logL} \\begin{aligned} \\log L(\\mathbf{X}, \\mathbf{Y}|\\boldsymbol{\\beta}, \\sigma^2) &amp;= \\sum\\limits_{i=1}^n -\\frac{1}{2}\\log(2\\pi\\sigma^2) -\\frac{1}{2\\sigma^2}\\sum\\limits_{i=1}^n(Y_i - \\mathbf{X}_i \\boldsymbol{\\beta})^T(Y_i - \\mathbf{X}_i \\boldsymbol{\\beta}) \\\\[10pt] &amp;= -\\frac{n}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}(\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta})^T(\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta})\\\\ \\end{aligned} \\end{equation}\\] [–insert details MLES–] The maximum likelihood estimate of \\(\\boldsymbol{\\beta}\\) and \\(\\sigma^2\\) are \\[\\begin{equation} \\label{mle} \\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y} \\hspace{10mm} \\hat{\\sigma}^2 = \\frac{1}{n}(\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta})^T(\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta}) = \\frac{1}{n}RSS \\end{equation}\\] where \\(RSS\\) is the residual sum of squares. Note that the maximum likelihood estimate is not exactly equal to the estimate of \\(\\sigma^2\\) we typically use \\(\\frac{RSS}{n-p-1}\\). This is because the maximum likelihood estimate of \\(\\sigma^2\\) in () is a biased estimator of \\(\\sigma^2\\). When \\(n\\) is much larger than the number of predictors \\(p\\), then the differences in these two estimates are trivial. 16.2 AIC Akaike’s Information Criterion (AIC) is \\[\\begin{equation} \\label{aic} AIC = -2 \\log L + 2(p+1) \\end{equation}\\] where \\(\\log L\\) is the log-likelihood. This is the general form of AIC that can be applied to a variety of models, but for now, let’s focus on AIC for mutliple linear regression. \\[\\begin{equation} \\label{aic-reg} \\begin{aligned} AIC &amp;= -2 \\log L + 2(p+1) \\\\[10pt] &amp;= -2\\bigg[-\\frac{n}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}(\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta})^T(\\mathbf{Y} - \\mathbf{X} \\boldsymbol{\\beta})\\bigg] + 2(p+1) \\\\[10pt] &amp;= n\\log\\big(2\\pi\\frac{RSS}{n}\\big) + \\frac{1}{RSS/n}RSS \\\\[10pt] &amp;= n\\log(2\\pi) + n\\log(RSS) - n\\log(n) + 2(p+1) \\end{aligned} \\end{equation}\\] 16.3 BIC [—] "]
]
