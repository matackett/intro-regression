<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title> 6 Multiple Linear Regression | Intro Regression</title>
  <meta name="description" content="An electronic book with assignments and in-class activities to help students apply concepts in an intermediate-level regression analysis course. The primary focus of this text is application and computing; there are also supplemental math notes for some topics.">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content=" 6 Multiple Linear Regression | Intro Regression" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://www.introregression.org/" />
  <meta property="og:image" content="https://www.introregression.org/img/introregression-sticker.png" />
  <meta property="og:description" content="An electronic book with assignments and in-class activities to help students apply concepts in an intermediate-level regression analysis course. The primary focus of this text is application and computing; there are also supplemental math notes for some topics." />
  <meta name="github-repo" content="matackett/intro-regression" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content=" 6 Multiple Linear Regression | Intro Regression" />
  
  <meta name="twitter:description" content="An electronic book with assignments and in-class activities to help students apply concepts in an intermediate-level regression analysis course. The primary focus of this text is application and computing; there are also supplemental math notes for some topics." />
  <meta name="twitter:image" content="https://www.introregression.org/img/introregression-sticker.png" />

<meta name="author" content="Maria Tackett">



  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  <link rel="apple-touch-icon-precomposed" sizes="152x152" href="img/introregression-sticker.png">
  <link rel="shortcut icon" href="img/introregression-sticker.png" type="image/x-icon">
<link rel="prev" href="5-anova.html">
<link rel="next" href="7-select.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Intro Regression</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Welcome to Intro Regression!</a></li>
<li class="chapter" data-level="1" data-path="1-getstarted.html"><a href="1-getstarted.html"><i class="fa fa-check"></i><b>1</b> Getting Started</a><ul>
<li class="chapter" data-level="1.1" data-path="1-getstarted.html"><a href="1-getstarted.html#how-to-use-this-book"><i class="fa fa-check"></i><b>1.1</b> How to use this book</a></li>
<li class="chapter" data-level="1.2" data-path="1-getstarted.html"><a href="1-getstarted.html#review"><i class="fa fa-check"></i><b>1.2</b> Review: Intro Statistics and R</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="2-instr.html"><a href="2-instr.html"><i class="fa fa-check"></i><b>2</b> Notes for Instructors</a><ul>
<li class="chapter" data-level="2.1" data-path="2-instr.html"><a href="2-instr.html#setting-up-your-course"><i class="fa fa-check"></i><b>2.1</b> Setting up your course</a><ul>
<li class="chapter" data-level="2.1.1" data-path="2-instr.html"><a href="2-instr.html#github"><i class="fa fa-check"></i><b>2.1.1</b> GitHub</a></li>
<li class="chapter" data-level="2.1.2" data-path="2-instr.html"><a href="2-instr.html#rstudio-cloud"><i class="fa fa-check"></i><b>2.1.2</b> RStudio Cloud</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="2-instr.html"><a href="2-instr.html#making-assignments"><i class="fa fa-check"></i><b>2.2</b> Making Assignments</a><ul>
<li class="chapter" data-level="2.2.1" data-path="2-instr.html"><a href="2-instr.html#github-1"><i class="fa fa-check"></i><b>2.2.1</b> GitHub</a></li>
<li class="chapter" data-level="2.2.2" data-path="2-instr.html"><a href="2-instr.html#rstudio-cloud-1"><i class="fa fa-check"></i><b>2.2.2</b> RStudio Cloud</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="2-instr.html"><a href="2-instr.html#additional-resources"><i class="fa fa-check"></i><b>2.3</b> Additional Resources</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="3-intro-r.html"><a href="3-intro-r.html"><i class="fa fa-check"></i><b>3</b> Intro to R</a><ul>
<li class="chapter" data-level="3.1" data-path="3-intro-r.html"><a href="3-intro-r.html#comp-intro-to-r"><i class="fa fa-check"></i><b>3.1</b> COMP: Intro to R</a><ul>
<li class="chapter" data-level="" data-path="3-intro-r.html"><a href="3-intro-r.html#introduction"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="" data-path="3-intro-r.html"><a href="3-intro-r.html#packages"><i class="fa fa-check"></i>Packages</a></li>
<li class="chapter" data-level="" data-path="3-intro-r.html"><a href="3-intro-r.html#warm-up"><i class="fa fa-check"></i>Warm up</a></li>
<li class="chapter" data-level="" data-path="3-intro-r.html"><a href="3-intro-r.html#data"><i class="fa fa-check"></i>Data</a></li>
<li class="chapter" data-level="" data-path="3-intro-r.html"><a href="3-intro-r.html#exercises"><i class="fa fa-check"></i>Exercises</a></li>
<li class="chapter" data-level="" data-path="3-intro-r.html"><a href="3-intro-r.html#simple-linear-regression"><i class="fa fa-check"></i>Simple Linear Regression</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="3-intro-r.html"><a href="3-intro-r.html#in-class-movies-on-imdb"><i class="fa fa-check"></i><b>3.2</b> IN-CLASS: Movies on IMDB</a><ul>
<li class="chapter" data-level="" data-path="3-intro-r.html"><a href="3-intro-r.html#data-1"><i class="fa fa-check"></i>Data</a></li>
<li class="chapter" data-level="" data-path="3-intro-r.html"><a href="3-intro-r.html#analysis"><i class="fa fa-check"></i>Analysis</a></li>
<li class="chapter" data-level="" data-path="3-intro-r.html"><a href="3-intro-r.html#next-steps"><i class="fa fa-check"></i>Next Steps</a></li>
<li class="chapter" data-level="" data-path="3-intro-r.html"><a href="3-intro-r.html#discussion-questions"><i class="fa fa-check"></i>Discussion Questions</a></li>
<li class="chapter" data-level="" data-path="3-intro-r.html"><a href="3-intro-r.html#appendix"><i class="fa fa-check"></i>Appendix</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="4-slr.html"><a href="4-slr.html"><i class="fa fa-check"></i><b>4</b> Simple Linear Regression</a><ul>
<li class="chapter" data-level="4.1" data-path="4-slr.html"><a href="4-slr.html#comp-college-admissions"><i class="fa fa-check"></i><b>4.1</b> COMP: College Admissions</a><ul>
<li class="chapter" data-level="4.1.1" data-path="4-slr.html"><a href="4-slr.html#packages-1"><i class="fa fa-check"></i><b>4.1.1</b> Packages</a></li>
<li class="chapter" data-level="4.1.2" data-path="4-slr.html"><a href="4-slr.html#data-2"><i class="fa fa-check"></i><b>4.1.2</b> Data</a></li>
<li class="chapter" data-level="4.1.3" data-path="4-slr.html"><a href="4-slr.html#exercises-1"><i class="fa fa-check"></i><b>4.1.3</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="4-slr.html"><a href="4-slr.html#in-class-advertising-sales"><i class="fa fa-check"></i><b>4.2</b> IN-CLASS: Advertising &amp; Sales</a><ul>
<li class="chapter" data-level="4.2.1" data-path="4-slr.html"><a href="4-slr.html#data-and-packages"><i class="fa fa-check"></i><b>4.2.1</b> Data and packages</a></li>
<li class="chapter" data-level="4.2.2" data-path="4-slr.html"><a href="4-slr.html#analysis-1"><i class="fa fa-check"></i><b>4.2.2</b> Analysis</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="4-slr.html"><a href="4-slr.html#in-class-carbohydrates-in-beer"><i class="fa fa-check"></i><b>4.3</b> IN-CLASS: Carbohydrates in Beer</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="5-anova.html"><a href="5-anova.html"><i class="fa fa-check"></i><b>5</b> Analysis of Variance</a><ul>
<li class="chapter" data-level="5.0.1" data-path="5-anova.html"><a href="5-anova.html#comp-diamonds"><i class="fa fa-check"></i><b>5.0.1</b> COMP: Diamonds</a></li>
<li class="chapter" data-level="" data-path="5-anova.html"><a href="5-anova.html#introduction-1"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="" data-path="5-anova.html"><a href="5-anova.html#data-3"><i class="fa fa-check"></i>Data</a></li>
<li class="chapter" data-level="" data-path="5-anova.html"><a href="5-anova.html#exercises-2"><i class="fa fa-check"></i>Exercises</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="6-mlr.html"><a href="6-mlr.html"><i class="fa fa-check"></i><b>6</b> Multiple Linear Regression</a><ul>
<li class="chapter" data-level="6.1" data-path="6-mlr.html"><a href="6-mlr.html#comp-houses-in-king-county"><i class="fa fa-check"></i><b>6.1</b> COMP: Houses in King County</a><ul>
<li class="chapter" data-level="6.1.1" data-path="6-mlr.html"><a href="6-mlr.html#packages-3"><i class="fa fa-check"></i><b>6.1.1</b> Packages</a></li>
<li class="chapter" data-level="6.1.2" data-path="6-mlr.html"><a href="6-mlr.html#data-4"><i class="fa fa-check"></i><b>6.1.2</b> Data</a></li>
<li class="chapter" data-level="6.1.3" data-path="6-mlr.html"><a href="6-mlr.html#exercises-3"><i class="fa fa-check"></i><b>6.1.3</b> Exercises</a></li>
<li class="chapter" data-level="6.1.4" data-path="6-mlr.html"><a href="6-mlr.html#acknowledgement"><i class="fa fa-check"></i><b>6.1.4</b> Acknowledgement</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="6-mlr.html"><a href="6-mlr.html#comp-airbnb-in-asheville-nc"><i class="fa fa-check"></i><b>6.2</b> COMP: Airbnb in Asheville, NC</a><ul>
<li class="chapter" data-level="6.2.1" data-path="6-mlr.html"><a href="6-mlr.html#packages-4"><i class="fa fa-check"></i><b>6.2.1</b> Packages</a></li>
<li class="chapter" data-level="6.2.2" data-path="6-mlr.html"><a href="6-mlr.html#data-5"><i class="fa fa-check"></i><b>6.2.2</b> Data</a></li>
<li class="chapter" data-level="6.2.3" data-path="6-mlr.html"><a href="6-mlr.html#exercises-4"><i class="fa fa-check"></i><b>6.2.3</b> Exercises</a></li>
<li class="chapter" data-level="6.2.4" data-path="6-mlr.html"><a href="6-mlr.html#acknowledgement-1"><i class="fa fa-check"></i><b>6.2.4</b> Acknowledgement</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="6-mlr.html"><a href="6-mlr.html#in-class-wages"><i class="fa fa-check"></i><b>6.3</b> IN-CLASS: Wages</a><ul>
<li class="chapter" data-level="6.3.1" data-path="6-mlr.html"><a href="6-mlr.html#initial-model"><i class="fa fa-check"></i><b>6.3.1</b> Initial model</a></li>
<li class="chapter" data-level="6.3.2" data-path="6-mlr.html"><a href="6-mlr.html#model-with-mean-centered-variables"><i class="fa fa-check"></i><b>6.3.2</b> Model with mean-centered variables</a></li>
<li class="chapter" data-level="6.3.3" data-path="6-mlr.html"><a href="6-mlr.html#model-with-indicator-variables"><i class="fa fa-check"></i><b>6.3.3</b> Model with indicator variables</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="6-mlr.html"><a href="6-mlr.html#notes-matrix-form-of-linear-regression"><i class="fa fa-check"></i><b>6.4</b> NOTES: Matrix Form of Linear Regression</a><ul>
<li class="chapter" data-level="6.4.1" data-path="6-mlr.html"><a href="6-mlr.html#introduction-2"><i class="fa fa-check"></i><b>6.4.1</b> Introduction</a></li>
<li class="chapter" data-level="6.4.2" data-path="6-mlr.html"><a href="6-mlr.html#matrix-form-for-the-regression-model"><i class="fa fa-check"></i><b>6.4.2</b> Matrix Form for the Regression Model</a></li>
<li class="chapter" data-level="6.4.3" data-path="6-mlr.html"><a href="6-mlr.html#estimating-the-coefficients"><i class="fa fa-check"></i><b>6.4.3</b> Estimating the Coefficients</a></li>
<li class="chapter" data-level="6.4.4" data-path="6-mlr.html"><a href="6-mlr.html#variance-covariance-matrix-of-the-coefficients"><i class="fa fa-check"></i><b>6.4.4</b> Variance-covariance matrix of the coefficients</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="6-mlr.html"><a href="6-mlr.html#notes-log-transformations"><i class="fa fa-check"></i><b>6.5</b> NOTES: Log Transformations</a><ul>
<li class="chapter" data-level="6.5.1" data-path="6-mlr.html"><a href="6-mlr.html#log-transformation-on-the-response-variable"><i class="fa fa-check"></i><b>6.5.1</b> Log-transformation on the response variable</a></li>
<li class="chapter" data-level="6.5.2" data-path="6-mlr.html"><a href="6-mlr.html#log-transformation-on-the-predictor-variable"><i class="fa fa-check"></i><b>6.5.2</b> Log-transformation on the predictor variable</a></li>
<li class="chapter" data-level="6.5.3" data-path="6-mlr.html"><a href="6-mlr.html#log-transformation-on-the-the-response-and-predictor-variable"><i class="fa fa-check"></i><b>6.5.3</b> Log-transformation on the the response and predictor variable</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="6-mlr.html"><a href="6-mlr.html#notes-model-diagnostics"><i class="fa fa-check"></i><b>6.6</b> NOTES: Model Diagnostics</a><ul>
<li class="chapter" data-level="6.6.1" data-path="6-mlr.html"><a href="6-mlr.html#introduction-3"><i class="fa fa-check"></i><b>6.6.1</b> Introduction</a></li>
<li class="chapter" data-level="6.6.2" data-path="6-mlr.html"><a href="6-mlr.html#matrix-form-for-the-regression-model-1"><i class="fa fa-check"></i><b>6.6.2</b> Matrix Form for the Regression Model</a></li>
<li class="chapter" data-level="6.6.3" data-path="6-mlr.html"><a href="6-mlr.html#hat-matrix-leverage"><i class="fa fa-check"></i><b>6.6.3</b> Hat Matrix &amp; Leverage</a></li>
<li class="chapter" data-level="6.6.4" data-path="6-mlr.html"><a href="6-mlr.html#standardized-residuals"><i class="fa fa-check"></i><b>6.6.4</b> Standardized Residuals</a></li>
<li class="chapter" data-level="6.6.5" data-path="6-mlr.html"><a href="6-mlr.html#cooks-distance"><i class="fa fa-check"></i><b>6.6.5</b> Cook’s Distance</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="7-select.html"><a href="7-select.html"><i class="fa fa-check"></i><b>7</b> Model Selection</a><ul>
<li class="chapter" data-level="7.1" data-path="7-select.html"><a href="7-select.html#comp-sats-and-mlb"><i class="fa fa-check"></i><b>7.1</b> COMP: SATs and MLB</a><ul>
<li class="chapter" data-level="" data-path="7-select.html"><a href="7-select.html#introduction-4"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="" data-path="7-select.html"><a href="7-select.html#data-6"><i class="fa fa-check"></i>Data</a></li>
<li class="chapter" data-level="" data-path="7-select.html"><a href="7-select.html#exercises-5"><i class="fa fa-check"></i>Exercises</a></li>
<li class="chapter" data-level="7.1.1" data-path="7-select.html"><a href="7-select.html#acknowledgements"><i class="fa fa-check"></i><b>7.1.1</b> Acknowledgements</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="7-select.html"><a href="7-select.html#in-class-sats-by-state"><i class="fa fa-check"></i><b>7.2</b> IN-CLASS: SATs by State</a><ul>
<li class="chapter" data-level="7.2.1" data-path="7-select.html"><a href="7-select.html#manual-backward-selection"><i class="fa fa-check"></i><b>7.2.1</b> “Manual” backward selection</a></li>
<li class="chapter" data-level="7.2.2" data-path="7-select.html"><a href="7-select.html#backward-selection-using-regsubsets"><i class="fa fa-check"></i><b>7.2.2</b> Backward selection using regsubsets</a></li>
<li class="chapter" data-level="7.2.3" data-path="7-select.html"><a href="7-select.html#changing-selection-criteria"><i class="fa fa-check"></i><b>7.2.3</b> Changing selection criteria</a></li>
<li class="chapter" data-level="7.2.4" data-path="7-select.html"><a href="7-select.html#different-selection-procedure"><i class="fa fa-check"></i><b>7.2.4</b> Different selection procedure</a></li>
<li class="chapter" data-level="7.2.5" data-path="7-select.html"><a href="7-select.html#choosing-a-final-model"><i class="fa fa-check"></i><b>7.2.5</b> Choosing a final model</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="7-select.html"><a href="7-select.html#notes-model-selection-criteria"><i class="fa fa-check"></i><b>7.3</b> NOTES: Model Selection Criteria</a><ul>
<li class="chapter" data-level="7.3.1" data-path="7-select.html"><a href="7-select.html#maximum-likelihood-estimation-of-boldsymbolbeta-and-sigma"><i class="fa fa-check"></i><b>7.3.1</b> Maximum Likelihood Estimation of <span class="math inline">\(\boldsymbol{\beta}\)</span> and <span class="math inline">\(\sigma\)</span></a></li>
<li class="chapter" data-level="7.3.2" data-path="7-select.html"><a href="7-select.html#aic"><i class="fa fa-check"></i><b>7.3.2</b> AIC</a></li>
<li class="chapter" data-level="7.3.3" data-path="7-select.html"><a href="7-select.html#bic"><i class="fa fa-check"></i><b>7.3.3</b> BIC</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="8-logistic.html"><a href="8-logistic.html"><i class="fa fa-check"></i><b>8</b> Logistic Regression</a><ul>
<li class="chapter" data-level="8.1" data-path="8-logistic.html"><a href="8-logistic.html#comp-spotify"><i class="fa fa-check"></i><b>8.1</b> COMP: Spotify</a><ul>
<li class="chapter" data-level="8.1.1" data-path="8-logistic.html"><a href="8-logistic.html#packages-6"><i class="fa fa-check"></i><b>8.1.1</b> Packages</a></li>
<li class="chapter" data-level="8.1.2" data-path="8-logistic.html"><a href="8-logistic.html#data-7"><i class="fa fa-check"></i><b>8.1.2</b> Data</a></li>
<li class="chapter" data-level="8.1.3" data-path="8-logistic.html"><a href="8-logistic.html#exercises-6"><i class="fa fa-check"></i><b>8.1.3</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="8-logistic.html"><a href="8-logistic.html#in-class-risk-of-heart-disease"><i class="fa fa-check"></i><b>8.2</b> IN-CLASS: Risk of Heart Disease</a><ul>
<li class="chapter" data-level="8.2.1" data-path="8-logistic.html"><a href="8-logistic.html#references"><i class="fa fa-check"></i><b>8.2.1</b> References</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="9-multinom-logistic.html"><a href="9-multinom-logistic.html"><i class="fa fa-check"></i><b>9</b> Multinomial Logistic Regression</a><ul>
<li class="chapter" data-level="9.1" data-path="9-multinom-logistic.html"><a href="9-multinom-logistic.html#comp-general-social-survey"><i class="fa fa-check"></i><b>9.1</b> COMP: General Social Survey</a><ul>
<li class="chapter" data-level="9.1.1" data-path="9-multinom-logistic.html"><a href="9-multinom-logistic.html#packages-7"><i class="fa fa-check"></i><b>9.1.1</b> Packages</a></li>
<li class="chapter" data-level="9.1.2" data-path="9-multinom-logistic.html"><a href="9-multinom-logistic.html#data-8"><i class="fa fa-check"></i><b>9.1.2</b> Data</a></li>
<li class="chapter" data-level="9.1.3" data-path="9-multinom-logistic.html"><a href="9-multinom-logistic.html#exercises-7"><i class="fa fa-check"></i><b>9.1.3</b> Exercises</a></li>
<li class="chapter" data-level="9.1.4" data-path="9-multinom-logistic.html"><a href="9-multinom-logistic.html#acknowledgements-1"><i class="fa fa-check"></i><b>9.1.4</b> Acknowledgements</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="9-multinom-logistic.html"><a href="9-multinom-logistic.html#in-class-sesame-street"><i class="fa fa-check"></i><b>9.2</b> IN-CLASS: Sesame Street</a><ul>
<li class="chapter" data-level="9.2.1" data-path="9-multinom-logistic.html"><a href="9-multinom-logistic.html#questions"><i class="fa fa-check"></i><b>9.2.1</b> Questions</a></li>
<li class="chapter" data-level="9.2.2" data-path="9-multinom-logistic.html"><a href="9-multinom-logistic.html#references-1"><i class="fa fa-check"></i><b>9.2.2</b> References</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="10-special.html"><a href="10-special.html"><i class="fa fa-check"></i><b>10</b> Special Topics</a><ul>
<li class="chapter" data-level="10.1" data-path="10-special.html"><a href="10-special.html#comp-regression-start-to-finish"><i class="fa fa-check"></i><b>10.1</b> COMP: Regression Start to Finish</a><ul>
<li class="chapter" data-level="10.1.1" data-path="10-special.html"><a href="10-special.html#packages-8"><i class="fa fa-check"></i><b>10.1.1</b> Packages</a></li>
<li class="chapter" data-level="10.1.2" data-path="10-special.html"><a href="10-special.html#data-9"><i class="fa fa-check"></i><b>10.1.2</b> Data</a></li>
<li class="chapter" data-level="10.1.3" data-path="10-special.html"><a href="10-special.html#exercises-8"><i class="fa fa-check"></i><b>10.1.3</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="10-special.html"><a href="10-special.html#in-class-missing-data-exercise"><i class="fa fa-check"></i><b>10.2</b> IN-CLASS: Missing Data Exercise</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="11-datasets.html"><a href="11-datasets.html"><i class="fa fa-check"></i><b>11</b> Data Sets</a></li>
<li class="chapter" data-level="" data-path="references-2.html"><a href="references-2.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Intro Regression</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="mlr" class="section level1">
<h1><span class="header-section-number"> 6</span> Multiple Linear Regression</h1>
<div id="comp-houses-in-king-county" class="section level2">
<h2><span class="header-section-number">6.1</span> COMP: Houses in King County</h2>
<p>The goal of this lab is to use multiple linear regression to understand the variation in the selling price of houses in King County, Washington. You will also gain practice using special predictors, such as categorical predictors and interaction effects, in the model, and you will be introduced to variable transformations.</p>
<div id="packages-3" class="section level3">
<h3><span class="header-section-number">6.1.1</span> Packages</h3>
<p>We will use the following packages in today’s lab.</p>
</div>
<div id="data-4" class="section level3">
<h3><span class="header-section-number">6.1.2</span> Data</h3>
<p>The for today’s lab contains the price and other characteristics of over 20,000 houses sold in King County, Washington (the county that includes Seattle). The dataset includes the following variables:</p>
<ul>
<li><code>price</code>: selling price of the house</li>
<li><code>date</code>: date house was sold, measured in days since January 1, 2014</li>
<li><code>bedrooms</code>: number of bedrooms</li>
<li><code>bathrooms</code>: number of bathrooms</li>
<li><code>sqft</code>: interior square footage</li>
<li><code>floors</code>: number of floors</li>
<li><code>waterfront</code>: 1 if the house has a view of the waterfront, 0 otherwise</li>
<li><code>yr_built</code>: year the house was built</li>
<li><code>yr_renovated</code>: 0 if the house was never renovated, the year the house was renovated if else</li>
</ul>
</div>
<div id="exercises-3" class="section level3">
<h3><span class="header-section-number">6.1.3</span> Exercises</h3>
<ol style="list-style-type: decimal">
<li><p>Use data visualization and summary statistics to examine the distribution of <code>bedrooms</code>. What is the maximum value? Does this value make sense? If not, what is this an indication of, i.e. how did this value get recorded in the data? Briefly explain.</p></li>
<li><p>We want to remove observations that have extreme values for bedrooms, i.e. those with values for <code>bedrooms</code> above the 95<sup>th</sup> percentile in the data. What is the 95<sup>th</sup> percentile for <code>bedrooms</code>? Use the <code>summarise</code> function to help you calculate this value.</p></li>
<li><p>Fill in the code below to filter the data so that the extreme observations are removed. How many observations are in the updated dataset?</p></li>
</ol>
<p>We will use this dataset for the remainder of the analysis.</p>
<ol start="4" style="list-style-type: decimal">
<li>Fit a regression model using square feet to explain variation in the price. Plot the residuals versus the predicted values. Based on this plot, what regression assumption appears to be violated? Briefly explain.</li>
</ol>
<p>Plot the histogram and Normal QQ-plot of the residuals. Based on these plots, what regression assumption appears to be violated? Briefly explain.</p>
<ol start="5" style="list-style-type: decimal">
<li>One way to deal with violations in regression assumptions is to transform the response variable and use that transformed variable when fitting the regression model. (We will talk about this in class next week). Some common transformations used in regression are the natural log (<code>log(y)</code>), the square root (<code>sqrt(y)</code>), and the reciprocal (<code>1/y</code>).</li>
</ol>
<p>Each transformation is applied to the response variable <code>price</code>, and the distributions of the transformed data are shown below.</p>
<p>Which transformation should we use to fix the violations of the model assumptions observed in the previous exercise? Briefly explain your choice.</p>
<ol start="6" style="list-style-type: decimal">
<li><p>Add the variable <code>logprice</code>, the log-transformed version of <code>price</code>, to the data frame. Fit a regression model with <code>logprice</code> as the response and <code>sqft</code> as the predictor variable. Create the residuals plots (residuals vs. predicted, histogram of residuals, Normal QQ-plot). Briefly comment on whether or not using the transformed variable improved on the model assumptions.</p></li>
<li><p>Though we can explain about 48% of the variation in a house prices by the square footage, we would like to incorporate some of the other available house characteristics in the model.</p></li>
</ol>
<p>Before fitting the model, use the code below to add the variable<code>floorsCat</code> that is the categorical version of the variable <code>floors</code>. Discuss with your group why it may make sense to treat <code>floors</code> as categorical, even though it represents a count.</p>
<p>Use the <code>count</code> function to see the number of observations at each level of <code>floorsCat</code>. What is the most common number of floors?</p>
<ol start="8" style="list-style-type: decimal">
<li>Use the code below to calculate the mean-centered versions of the variables <code>sqft</code>, <code>bedrooms</code>, and <code>bathrooms</code> and add them to the data frame.</li>
</ol>
<p>It is not appropriate to calculate the mean-centered version of the variable <code>waterfront</code>. Briefly explain why it isn’t.</p>
<ol start="9" style="list-style-type: decimal">
<li><p>Fit a regression model with <code>logprice</code> as the response variable, and the mean-centered variables from the previous exercise along with <code>waterfront</code> and <code>floorsCat</code> as the predictor variables. Display the model output.</p></li>
<li><p>What is the baseline level for the variable <code>floorsCat</code>?</p></li>
<li><p>Interpret the intercept of the model in the context of the data. Write the interpretation in terms of the <strong>price</strong>.</p></li>
<li><p>What is the intercept of the model for the subset of houses with 3 floors that are not on the waterfront? Write the intercept in terms of the <strong>log(price)</strong>.</p></li>
<li><p>We would like to consider potential interactions for the model. A significant <strong>interaction</strong> occurs when the relationship of a predictor variable with the response depends on the value of another predictor variable.</p></li>
</ol>
<p>Fill in the code below to plot the relationship between <code>logprice</code> and <code>bedrooms</code> by <code>waterfront</code>. Based on this plot, do you think there is a significant interaction effect between <code>bedrooms</code> and <code>waterfront</code>? In other words, do you think the relationship between the logprice and the number of bedrooms differs based on whether or not a house is on the waterfront? Briefly explain.</p>
<p><strong>We will talk more about interaction effects in Monday’s lecture. In HW 03, you explore potential interaction effects using this housing data.</strong></p>
<p><em>You’re done! Commit all remaining changes, use the commit message “Done with Lab 4!”, and push. Before you wrap up the assignment, make sure the .Rmd and .md documents are updated in your GitHub repo. There is a 10% penalty if the .Rmd file has to be knitted to display graphs, i.e. the graphs are not showing in the .md file on GitHub.</em></p>
</div>
<div id="acknowledgement" class="section level3">
<h3><span class="header-section-number">6.1.4</span> Acknowledgement</h3>
<p>The data used in this lab was obtained from <a href="https://github.com/proback/BYSH" class="uri">https://github.com/proback/BYSH</a>.</p>
<hr />
</div>
</div>
<div id="comp-airbnb-in-asheville-nc" class="section level2">
<h2><span class="header-section-number">6.2</span> COMP: Airbnb in Asheville, NC</h2>
<p>When doing statistical analyses in practice, there is often a lot of time spent on cleaning and preparing the data. The goal of today’s lab is to practice cleaning messy data, so it can be used in a regression analysis. You will also practice interpreting the results from a regression model that has numeric and categorical predictors and a log-transformed response variable.</p>
<div id="packages-4" class="section level3">
<h3><span class="header-section-number">6.2.1</span> Packages</h3>
<p>We will use the following packages in today’s lab.</p>
</div>
<div id="data-5" class="section level3">
<h3><span class="header-section-number">6.2.2</span> Data</h3>
<p>Today’s data is about Airbnb listings in Asheville, NC. The data was obtained from <a href="http://insideairbnb.com/index.html">http://insideairbnb.com/</a>; it was originally scraped from <a href="https://www.airbnb.com/">airbnb.com</a>.</p>
<p>You can see a visualization of some of the data used in today’s lab at <a href="http://insideairbnb.com/asheville/" class="uri">http://insideairbnb.com/asheville/</a>.</p>
<p>We will use the following variables in this lab:</p>
<ul>
<li><code>price</code>: Cost per night (in U.S. dollars)</li>
<li><code>cleaning_fee</code>: Cleaning fee (in U.S. dollars)</li>
<li><code>property_type</code>: Type of dwelling (House, Apartment, etc.)</li>
<li><code>room_type</code>:
<ul>
<li><em>Entire home/apt</em> (guests have entire place to themselves)</li>
<li><em>Private room</em> (Guests have private room to sleep, all other rooms shared)</li>
<li><em>Shared room</em> (Guests sleep in room shared with others)</li>
</ul></li>
<li><code>number_of_reviews</code>: Total number of reviews for the listing</li>
<li><code>review_scores_rating</code>: Average review score (0 - 100)</li>
</ul>
</div>
<div id="exercises-4" class="section level3">
<h3><span class="header-section-number">6.2.3</span> Exercises</h3>
<div id="data-wrangling" class="section level4">
<h4><span class="header-section-number">6.2.3.1</span> Data wrangling</h4>
<ol style="list-style-type: decimal">
<li>We would like to use variables from both the <code>basic_info</code> and <code>details</code> data frames in this analysis. Both dataframes have the variable <code>id</code> that uniquely identifies each Airbnb listing. Because we need data from <code>basic_info</code> and <code>details</code>, we only want to include observations that are in both the <code>basic_info</code> and <code>details</code> datasets. Therefore, we will use an <code>inner_join</code> to combine the two data sets. (Note: Both data frames include a variable called <code>id</code> that uniquely identifies each Airbnb listing. R will use this variable to join the two data frames.)</li>
</ol>
<p>How many observations are in <code>airbnb</code>? How many variables?</p>
<ol start="2" style="list-style-type: decimal">
<li>Some Airbnb rentals have cleaning fees, and we want to include the cleaning fee when we calculate the total rental cost. Use the code below to see how the data in the column <code>cleaning_fee</code> is currently stored in the <code>airbnb</code> data frame.</li>
</ol>
<p>The column <code>cleaning_fee</code> currently contains what type of data? Why do you think the data is stored this way even though <code>cleaning_fee</code> is a quantitative variable?</p>
<ol start="3" style="list-style-type: decimal">
<li>Since <code>cleaning_fee</code> is a quantitative variable, we need to make sure it is stored as numeric data in the dataframe. To do so, we will first use the <code>extract</code> function in <code>tidyr</code> package to create a column of cleaning fees that don’t have the dollar sign. Then, we will use the <code>as.numeric()</code> function to make the extracted data the numeric data type <code>double</code>.</li>
</ol>
<p>Use the <code>typeof</code> function to confirm that <code>cleaning_fee</code> is now stored as a <code>double</code> data type.</p>
<ol start="4" style="list-style-type: decimal">
<li><p>Use the <code>skim</code> function to view a summary of the <code>cleaning_fee</code> data. How many observations have missing values for <code>cleaning_fee</code>? What do you think is the most likely reason for the missing observations of <code>cleaning_fee</code>? In other words, what does a missing value of <code>cleaning_fee</code> indicate?</p></li>
<li><p>Fill in the code below to impute the missing values of <code>cleaning_fee</code> with an appropriate numeric value. Then use the <code>skim</code> function to confirm that there are no longer missing values of <code>cleaning_fee</code>.</p></li>
</ol>
<p><em>This is an example of data that is missing not at random, since there is a specific pattern/explanation to the misisng data. We will talk more about dealing with missing data later in the semester.</em></p>
<ol start="6" style="list-style-type: decimal">
<li>Next, we look at the variable <code>property_type</code>.
<ul>
<li>Use the <code>count</code> function to determine how many categories are in the variable <code>property</code> and the frequency of each category.</li>
<li>What are the top 4 most common property types? These make up what proportion of the observations?</li>
</ul></li>
<li>Since an overwhelming majority of the observations in the data are one of the top 4 property types, we would like to create a simplified version of the <code>proprety_type</code> variable that has 5 categories: <em>House</em>, <em>Apartment</em>, <em>Guest suite</em>, <em>Bungalow</em>, and <em>Other</em>. Fill in the code below to create <code>prop_type_simp</code>.</li>
</ol>
<p>Use the code below to check that <code>prop_type_simp</code> was correctly made.</p>
<ol start="8" style="list-style-type: decimal">
<li>Airbnb is most commonly used for travel purposes, i.e. as an alternative to traditional hotels. We only want to include Airbnb listings in our regression analysis that are intended for travel purposes. What are the 5 most common values for the variable <code>minimum_nights</code>? Which value in the top 5 stands out? What is the likely intended purpose for Airbnb listings with this seemingly unusual value for <code>minimum_nights</code>?</li>
</ol>
<p>Filter the <code>airbnb</code> data so that it only includes observations with <code>minimum_nights &lt;= 3</code>. You will use this filtered dataset for the remainder of the lab.</p>
</div>
<div id="regression-analysis" class="section level4">
<h4><span class="header-section-number">6.2.3.2</span> Regression Analysis</h4>
<ol start="9" style="list-style-type: decimal">
<li><p>For the response variable, will use the cost to stay at an Airbnb location for 3 nights. Create a new variable called <code>price_3_nights</code> that uses <code>price</code> and <code>cleaning_fee</code> to calculate the total cost to stay at the Airbnb property for 3 nights. Be sure to add this variable to your dataframe.</p></li>
<li><p>Use histograms to examine the distributions of <code>price_3_nights</code> and <code>log(price_3_nights)</code>. Based on the histograms, which variable should you use for the regression model? Briefly explain.</p></li>
</ol>
<p>Use this variable as the response for the remainder of the lab.</p>
<ol start="11" style="list-style-type: decimal">
<li><p>Fit a regression model called <code>model1</code> with the response variable from the previous question and the following predictor variables: <code>prop_type_simp</code>, <code>number_of_reviews</code>, and <code>review_scores_rating</code>. Display the model output.</p></li>
<li><p>Interpret the coefficient <code>review_scores_rating</code> in terms of <code>price_3_nights</code>.</p></li>
<li><p>Interpret the coefficient of <code>prop_type_simpGuest suite</code> in terms of <code>price_3_nights</code>.</p></li>
<li><p>We want to determine if <code>room_type</code> is a significant predictor of the cost for 3 nights, given everything else in the model. Fit a regression model called <code>model2</code> that includes all of the predictor variables in <code>model1</code> and <code>room_type</code>. Display the model output.</p></li>
</ol>
<p>Use the code below to conduct a Nested F test to determine if <code>room_type</code> is a significant predictor of the minimum cost. What is your conclusion from the Nested F test?</p>
<ol start="15" style="list-style-type: decimal">
<li>Suppose you are planning to visit Asheville over spring break, and you want to stay in an Airbnb. You find an Airbnb that is an apartment with a private room, has 10 reviews, and an average rating of 90. Use <code>model2</code> to predict the total cost to stay at this Airbnb for 3 nights. Include the appropriate 95% interval with your prediction. Report the prediction and interval in terms of <code>price_3_nights</code>.</li>
</ol>
<p><em>You’re done! Commit all remaining changes, use the commit message “Done with Lab 5!”, and push. Before you wrap up the assignment, make sure the .Rmd and .md documents are updated in your GitHub repo. There is a 10% penalty if the .Rmd file has to be knitted to display graphs, i.e. the graphs are not showing in the .md file on GitHub.</em></p>
</div>
</div>
<div id="acknowledgement-1" class="section level3">
<h3><span class="header-section-number">6.2.4</span> Acknowledgement</h3>
<p>The data from this lab is from <a href="http://insideairbnb.com/index.html">insideairbnb.com</a></p>
<hr />
</div>
</div>
<div id="in-class-wages" class="section level2">
<h2><span class="header-section-number">6.3</span> IN-CLASS: Wages</h2>
<div id="initial-model" class="section level3">
<h3><span class="header-section-number">6.3.1</span> Initial model</h3>
</div>
<div id="model-with-mean-centered-variables" class="section level3">
<h3><span class="header-section-number">6.3.2</span> Model with mean-centered variables</h3>
<ul>
<li><p>Calculate the regression model using the mean-centered variables.</p></li>
<li><p>How did the model change?</p></li>
</ul>
</div>
<div id="model-with-indicator-variables" class="section level3">
<h3><span class="header-section-number">6.3.3</span> Model with indicator variables</h3>
<ul>
<li><p>Use the code below to create a categorical variable for <code>Educ</code>.</p></li>
<li><p>Calculate the regression model using <code>EducCat</code> instead of <code>Educ</code>.</p></li>
</ul>
<hr />
</div>
</div>
<div id="notes-matrix-form-of-linear-regression" class="section level2">
<h2><span class="header-section-number">6.4</span> NOTES: Matrix Form of Linear Regression</h2>
<p>This document provides the details for the matrix form of multiple linear regression. We assume the reader has familiarity with some matrix alegbra. Please see Chapter 1 of <a href="https://www-bcf.usc.edu/~gareth/ISL/"><em>An Introduction to Statistical Learning</em></a> for a brief review of matrix algebra.</p>
<div id="introduction-2" class="section level3">
<h3><span class="header-section-number">6.4.1</span> Introduction</h3>
<p>Suppose we have <span class="math inline">\(n\)</span> observations. Let the <span class="math inline">\(i^{th}\)</span> be <span class="math inline">\((x_{i1}, \ldots, x_{ip}, y_i)\)</span>, such that <span class="math inline">\(x_{i1}, \ldots, x_{ip}\)</span> are the explanatory variables (predictors) and <span class="math inline">\(y_i\)</span> is the response variable. We assume the data can be modeled using the least-squares regression model, such that the mean response for a given combination of explanatory variables follows the form in ().</p>
<span class="math display">\[\begin{equation}
\label{basic_model}
y = \beta_0 + \beta_1 x_1 + \dots + \beta_p x_p 
\end{equation}\]</span>
<p>We can write the response for the <span class="math inline">\(i^{th}\)</span> observation as shown in ()</p>
<span class="math display">\[\begin{equation}
\label{ind_response}
y_i = \beta_0 + \beta_1 x_{i1} + \dots + \beta_p x_{ip} + \epsilon_i 
\end{equation}\]</span>
<p>such that <span class="math inline">\(\epsilon_i\)</span> is the amount <span class="math inline">\(y_i\)</span> deviates from <span class="math inline">\(\mu\{y|x_{i1}, \ldots, x_{ip}\}\)</span>, the mean response for a given combination of explanatory variables. We assume each <span class="math inline">\(\epsilon_i \sim N(0,\sigma^2)\)</span>, where <span class="math inline">\(\sigma^2\)</span> is a constant variance for the distribution of the response <span class="math inline">\(y\)</span> for any combination of explanatory variables <span class="math inline">\(x_1, \ldots, x_p\)</span>.</p>
</div>
<div id="matrix-form-for-the-regression-model" class="section level3">
<h3><span class="header-section-number">6.4.2</span> Matrix Form for the Regression Model</h3>
<p>We can represent the () and () using matrix notation. Let</p>
<span class="math display">\[\begin{equation}
\label{matrix notation}
\mathbf{Y} = \begin{bmatrix}y_1 \\ y_2 \\ \vdots \\y_n\end{bmatrix} 
\hspace{15mm}
\mathbf{X} = \begin{bmatrix}x_{11} &amp; x_{12} &amp; \dots &amp; x_{1p} \\
x_{21} &amp; x_{22} &amp; \dots &amp; x_{2p} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots\\
x_{n1} &amp; x_{n2} &amp; \dots &amp; x_{np} \end{bmatrix}
\hspace{15mm}
\boldsymbol{\beta}= \begin{bmatrix}\beta_0 \\ \beta_1 \\ \vdots \\ \beta_p \end{bmatrix} 
\hspace{15mm}
\boldsymbol{\epsilon}= \begin{bmatrix}\epsilon_1 \\ \epsilon_2 \\ \vdots \\ \epsilon_n \end{bmatrix}
\end{equation}\]</span>
<p>Thus,</p>
<p><span class="math display">\[\mathbf{Y} = \mathbf{X}\boldsymbol{\beta} + \mathbf{\epsilon}\]</span></p>
<p>Therefore the estimated response for a given combination of explanatory variables and the associated residuals can be written as</p>
<span class="math display">\[\begin{equation}
\label{matrix_mean}
\hat{\mathbf{Y}} = \mathbf{X}\hat{\boldsymbol{\beta}} \hspace{10mm} \mathbf{e} = \mathbf{Y} - \mathbf{X}\hat{\boldsymbol{\beta}}
\end{equation}\]</span>
</div>
<div id="estimating-the-coefficients" class="section level3">
<h3><span class="header-section-number">6.4.3</span> Estimating the Coefficients</h3>
<p>The least-squares model is the one that minimizes the sum of the squared residuals. Therefore, we want to find the coefficients, <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> that minimizes</p>
<span class="math display">\[\begin{equation}
\label{sum_sq_resid}
\sum\limits_{i=1}^{n} e_{i}^2 = \mathbf{e}^T\mathbf{e} = (\mathbf{Y} - \mathbf{X}\hat{\boldsymbol{\beta}})^T(\mathbf{Y} - \mathbf{X}\hat{\boldsymbol{\beta}})
\end{equation}\]</span>
<p>where <span class="math inline">\(\mathbf{e}^T\)</span>, the transpose of the matrix <span class="math inline">\(\mathbf{e}\)</span>.</p>
<span class="math display">\[\begin{equation}
\label{model_equation}
(\mathbf{Y} - \mathbf{X}\hat{\boldsymbol{\beta}})^T(\mathbf{Y} - \mathbf{X}\hat{\boldsymbol{\beta}}) = (\mathbf{Y}^T\mathbf{Y} - 
\mathbf{Y}^T \mathbf{X}\hat{\boldsymbol{\beta}} - (\hat{\boldsymbol{\beta}}{}^{T}\mathbf{X}^T\mathbf{Y} +
\hat{\boldsymbol{\beta}}{}^{T}\mathbf{X}^T\mathbf{X}
\hat{\boldsymbol{\beta}})
\end{equation}\]</span>
<p>Note that <span class="math inline">\((\mathbf{Y^T}\mathbf{X}\hat{\boldsymbol{\beta}})^T = \hat{\boldsymbol{\beta}}{}^{T}\mathbf{X}^T\mathbf{Y}\)</span>. Since these are both constants (i.e. <span class="math inline">\(1\times 1\)</span> vectors), <span class="math inline">\(\mathbf{Y^T}\mathbf{X}\hat{\boldsymbol{\beta}} = \hat{\boldsymbol{\beta}}{}^{T}\mathbf{X}^T\mathbf{Y}\)</span>. Thus, () becomes</p>
<span class="math display">\[\begin{equation}
\mathbf{Y}^T\mathbf{Y} - 2 \mathbf{X}^T\hat{\boldsymbol{\beta}}{}^{T}\mathbf{Y} + \hat{\boldsymbol{\beta}}{}^{T}\mathbf{X}^T\mathbf{X}
\hat{\boldsymbol{\beta}}
\end{equation}\]</span>
<p>Since we want to find the <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> that minimizes (), will find the value of <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> such that the derivative with respect to <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> is equal to 0.</p>
<span class="math display">\[\begin{equation}
\begin{aligned}
\frac{\partial \mathbf{e}^T\mathbf{e}}{\partial \hat{\boldsymbol{\beta}}} &amp; = \frac{\partial}{\partial \hat{\boldsymbol{\beta}}}(\mathbf{Y}^T\mathbf{Y} - 2 \mathbf{X}^T\hat{\boldsymbol{\beta}}{}^T\mathbf{Y} + \hat{\boldsymbol{\beta}}{}^{T}\mathbf{X}^T\mathbf{X}\hat{\boldsymbol{\beta}}) = 0 \\[10pt]
&amp;\Rightarrow - 2 \mathbf{X}^T\mathbf{Y} + 2 \mathbf{X}^T\mathbf{X}\hat{\boldsymbol{\beta}} = 0 \\[10pt]
&amp; \Rightarrow 2 \mathbf{X}^T\mathbf{Y} = 2 \mathbf{X}^T\mathbf{X}\hat{\boldsymbol{\beta}} \\[10pt]
&amp; \Rightarrow \mathbf{X}^T\mathbf{Y} = \mathbf{X}^T\mathbf{X}\hat{\boldsymbol{\beta}} \\[10pt]
&amp; \Rightarrow (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{Y} = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{X}\hat{\boldsymbol{\beta}} \\[10pt]
&amp; \Rightarrow (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{Y} = \mathbf{I}\hat{\boldsymbol{\beta}}
\end{aligned}
\end{equation}\]</span>
<p>Thus, the estimate of the model coefficients is <span class="math inline">\(\hat{\boldsymbol{\beta}} = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{Y}\)</span>.</p>
</div>
<div id="variance-covariance-matrix-of-the-coefficients" class="section level3">
<h3><span class="header-section-number">6.4.4</span> Variance-covariance matrix of the coefficients</h3>
<p>We will use two properties to derive the form of the variance-covarinace matrix of the coefficients:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(E[\boldsymbol{\epsilon}\boldsymbol{\epsilon}^T] = \sigma^2I\)</span></li>
<li><span class="math inline">\(\hat{\boldsymbol{\beta}} = \boldsymbol{\beta} + (\mathbf{X}^T\mathbf{X})^{-1}\epsilon\)</span></li>
</ol>
First, we will show that <span class="math inline">\(E[\boldsymbol{\epsilon}\boldsymbol{\epsilon}^T] = \sigma^2I\)</span>
<span class="math display">\[\begin{equation}
\label{expected_error}
\begin{aligned}
E[\boldsymbol{\epsilon}\boldsymbol{\epsilon}^T] &amp;= E \begin{bmatrix}\epsilon_1  &amp; \epsilon_2 &amp; \dots &amp; \epsilon_n \end{bmatrix}\begin{bmatrix}\epsilon_1 \\ \epsilon_2 \\ \vdots \\ \epsilon_n \end{bmatrix}  \\[10pt]
&amp; = E \begin{bmatrix} \epsilon_1^2  &amp; \epsilon_1 \epsilon_2 &amp; \dots &amp; \epsilon_1 \epsilon_n \\
\epsilon_2 \epsilon_1 &amp; \epsilon_2^2 &amp; \dots &amp; \epsilon_2 \epsilon_n \\ 
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 
\epsilon_n \epsilon_1 &amp; \epsilon_n \epsilon_2 &amp; \dots &amp; \epsilon_n^2 
\end{bmatrix} \\[10pt]
&amp; = \begin{bmatrix} E[\epsilon_1^2]  &amp; E[\epsilon_1 \epsilon_2] &amp; \dots &amp; E[\epsilon_1 \epsilon_n] \\
E[\epsilon_2 \epsilon_1] &amp; E[\epsilon_2^2] &amp; \dots &amp; E[\epsilon_2 \epsilon_n] \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 
E[\epsilon_n \epsilon_1] &amp; E[\epsilon_n \epsilon_2] &amp; \dots &amp; E[\epsilon_n^2]
\end{bmatrix}
\end{aligned}
\end{equation}\]</span>
<p>Recall, the regression assumption that the errors <span class="math inline">\(\epsilon_i&#39;s\)</span> are Normally distributed with mean 0 and variance <span class="math inline">\(\sigma^2\)</span>. Thus, <span class="math inline">\(E(\epsilon_i^2) = Var(\epsilon_i) = \sigma^2\)</span> for all <span class="math inline">\(i\)</span>. Additionally, recall the regression assumption that the errors are uncorrelated, i.e. <span class="math inline">\(E(\epsilon_i \epsilon_j) = Cov(\epsilon_i, \epsilon_j) = 0\)</span> for all <span class="math inline">\(i,j\)</span>. Using these assumptions, we can write () as</p>
<span class="math display">\[\begin{equation}
E[\mathbf{\epsilon}\mathbf{\epsilon}^T]  = \begin{bmatrix} \sigma^2  &amp; 0 &amp; \dots &amp; 0 \\
0 &amp; \sigma^2  &amp; \dots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; \dots &amp; \sigma^2
\end{bmatrix} = \sigma^2 \mathbf{I}
\end{equation}\]</span>
<p>where <span class="math inline">\(\mathbf{I}\)</span> is the <span class="math inline">\(n \times n\)</span> identity matrix.</p>
<p>Next, we show that <span class="math inline">\(\hat{\boldsymbol{\beta}} = \boldsymbol{\beta} + (\mathbf{X}^T\mathbf{X})^{-1}\epsilon\)</span>.</p>
<p>Recall that the <span class="math inline">\(\hat{\boldsymbol{\beta}} = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{Y}\)</span> and <span class="math inline">\(\mathbf{Y} = \mathbf{X}\mathbf{\beta} + \mathbf{\epsilon}\)</span>. Then,</p>
<span class="math display">\[\begin{equation}
\begin{aligned}
\hat{\boldsymbol{\beta}} &amp;= (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{Y} \\[10pt]
&amp;= (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T(\mathbf{X}\boldsymbol{\beta} + \boldsymbol{\epsilon}) \\[10pt]
&amp;= \boldsymbol{\beta} + (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T \mathbf{\epsilon} \\
\end{aligned}
\end{equation}\]</span>
<p>Using these two properties, we derive the form of the variance-covariance matrix for the coefficients. Note that the covariance matrix is <span class="math inline">\(E[(\hat{\boldsymbol{\beta}} - \boldsymbol{\beta})(\hat{\boldsymbol{\beta}} - \boldsymbol{\beta})^T]\)</span></p>
<span class="math display">\[\begin{equation}
\begin{aligned}
E[(\hat{\boldsymbol{\beta}} - \boldsymbol{\beta})(\hat{\boldsymbol{\beta}} - \boldsymbol{\beta})^T] &amp;= E[(\boldsymbol{\beta} + (\mathbf{X}^T\mathbf{X})^{-1} \mathbf{X}^T \boldsymbol{\epsilon} - \boldsymbol{\beta})(\boldsymbol{\beta} + (\mathbf{X}^T\mathbf{X})^{-1} \mathbf{X}^T \boldsymbol{\epsilon} - \boldsymbol{\beta})^T]\\[10pt]
&amp; = E[(\mathbf{X}^T\mathbf{X})^{-1} \mathbf{X}^T \boldsymbol{\epsilon}\boldsymbol{\epsilon}^T\mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}] \\[10pt]
&amp; = (\mathbf{X}^T\mathbf{X})^{-1} \mathbf{X}^T E[\boldsymbol{\epsilon}\boldsymbol{\epsilon}^T]\mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\\[10pt]
&amp; = (\mathbf{X}^T\mathbf{X})^{-1} \mathbf{X}^T (\sigma^2\mathbf{I})\mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\\
&amp;= \sigma^2\mathbf{I}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\\[10pt]
&amp; = \sigma^2\mathbf{I}(\mathbf{X}^T\mathbf{X})^{-1}\\[10pt]
&amp;  = \sigma^2(\mathbf{X}^T\mathbf{X})^{-1} \\
\end{aligned}
\end{equation}\]</span>
<hr />
</div>
</div>
<div id="notes-log-transformations" class="section level2">
<h2><span class="header-section-number">6.5</span> NOTES: Log Transformations</h2>
<p>This document provides details about the model interpretion when the predictor and/or response variables are log-transformed. For simplicity, we will discuss transformations for the simple linear regression model:</p>
<span class="math display">\[\begin{equation}
\label{orig}
y = \beta_0 + \beta_1 x
\end{equation}\]</span>
<p>All results and interpretations can be easily extended to transformations in multiple regression models.</p>
<p><em>Note</em>: <em>log</em> refers to the natural logarithm.</p>
<div id="log-transformation-on-the-response-variable" class="section level3">
<h3><span class="header-section-number">6.5.1</span> Log-transformation on the response variable</h3>
<p>Suppose we fit a linear regression model with <span class="math inline">\(\log(y)\)</span>, the log-transformed <span class="math inline">\(y\)</span>, as the response variable. Under this model, we assume a linear relationship exists between <span class="math inline">\(x\)</span> and <span class="math inline">\(\log(y)\)</span>, such that <span class="math inline">\(\log(y) \sim N(\beta_0 + \beta_1 x, \sigma^2)\)</span> for some <span class="math inline">\(\beta_0\)</span>, <span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\sigma^2\)</span>. In other words, we can model the relationship between <span class="math inline">\(x\)</span> and <span class="math inline">\(\log(y)\)</span> using the model in ().</p>
<span class="math display">\[\begin{equation}
\label{log-y}
\log(y) = \beta_0 + \beta_1 x
\end{equation}\]</span>
<p>If we interpret the model in terms of <span class="math inline">\(\log(y)\)</span>, then we can use the usual interpretations for slope and intercept. When reporting results, however, it is best to give all interpretations in terms of the original response variable <span class="math inline">\(y\)</span>, since interpretations using log-transformed variables are often more difficult to truly understand.</p>
<p>In order to get back on the original scale, we need to use the exponential function (also known as the anti-log), <span class="math inline">\(\exp\{x\} = e^x\)</span>. Therefore, we use the model in () for interpretations and predictions, we will use () to state our conclusions in terms of <span class="math inline">\(y\)</span>.</p>
<span class="math display">\[\begin{equation}
\label{exp-y}
\begin{aligned}
&amp;\exp\{\log(y)\} = \exp\{\beta_0 + \beta_1 x\} \\[10pt]
\Rightarrow &amp;y = \exp\{\beta_0 + \beta_1 x\} \\[10pt]
\Rightarrow &amp;y = \exp\{\beta_0\}\exp\{\beta_1 x\}
\end{aligned}
\end{equation}\]</span>
<p>In order to interpret the slope and intercept, we need to first understand the relationship between the mean, median and log transformations.</p>
<div id="mean-median-and-log-transformations" class="section level4">
<h4><span class="header-section-number">6.5.1.1</span> Mean, Median, and Log Transformations</h4>
<p>Suppose we have a dataset <code>y</code> that contains the following observations:</p>
<p>If we log-transform the values of <code>y</code> then calculate the mean and median, we have</p>
<p>If we calculate the mean and median of <code>y</code>, then log-transform the mean and median, we have</p>
<p>This is a simple illustration to show</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(\text{Mean}[{\log(y)}] \neq \log[\text{Mean}(y)]\)</span> - the mean and log are not commutable</p></li>
<li><p><span class="math inline">\(\text{Median}[\log(y)] = \log[\text{Median}(y)]\)</span> - the median and log are commutable</p></li>
</ol>
</div>
<div id="interpretaton-of-model-coefficients" class="section level4">
<h4><span class="header-section-number">6.5.1.2</span> Interpretaton of model coefficients</h4>
<p>Using (), the mean <span class="math inline">\(\log(y)\)</span> for any given value of <span class="math inline">\(x\)</span> is <span class="math inline">\(\beta_0 + \beta_1 x\)</span>; however, this does <strong>not</strong> indicate that the mean of <span class="math inline">\(y = \exp\{\beta_0 + \beta_1 x\}\)</span> (see previous section). From the assumptions of linear regression, we assume that for any given value of <span class="math inline">\(x\)</span>, the distribution of <span class="math inline">\(\log(y)\)</span> is Normal, and therefore symmetric. Thus the median of <span class="math inline">\(\log(y)\)</span> is equal to the mean of <span class="math inline">\(\log(y)\)</span>, i.e <span class="math inline">\(\text{Median}(\log(y)) = \beta_0 + \beta_1 x\)</span>.</p>
<p>Since the log and the median are commutable, <span class="math inline">\(\text{Median}(\log(y)) = \beta_0 + \beta_1 x \Rightarrow \text{Median}(y) = \exp\{\beta_0 + \beta_1 x\}\)</span>. Thus, when we log-transform the response variable, the interpretation of the intercept and slope are in terms of the effect on the <strong>median</strong> of <span class="math inline">\(y\)</span>.</p>
<p><strong>Intercept</strong>: The intercept is expected median of <span class="math inline">\(y\)</span> when the predictor variable equals 0. Therefore, when <span class="math inline">\(x=0\)</span>,</p>
<span class="math display">\[\begin{equation}
\begin{aligned}
&amp;\log(y) = \beta_0 + \beta_1 \times 0 = \beta_0 \\[10pt]
\Rightarrow &amp;y = \exp\{\beta_0\}
\end{aligned}
\end{equation}\]</span>
<p><em>Interpretation: When <span class="math inline">\(x=0\)</span>, the median of <span class="math inline">\(y\)</span> is expected to be <span class="math inline">\(\exp\{\beta_0\}\)</span>.</em></p>
<p><strong>Slope</strong>: The slope is the expected change in the median of <span class="math inline">\(y\)</span> when <span class="math inline">\(x\)</span> increases by 1 unit. The change in the median of <span class="math inline">\(y\)</span> is</p>
<span class="math display">\[\begin{equation}
\exp\{[\beta_0 + \beta_1 (x+1)] - [\beta_0 + \beta_1 x]\} = \frac{\exp\{\beta_0 + \beta_1 (x+1)\}}{\exp\{\beta_0 + \beta_1 x\}} = \frac{\exp\{\beta_0\}\exp\{\beta_1 x\}\exp\{\beta_1\}}{\exp\{\beta_0\}\exp\{\beta_1 x\}} = \exp\{\beta_1\}
\end{equation}\]</span>
<p>Thus, the median of <span class="math inline">\(y\)</span> for <span class="math inline">\(x+1\)</span> is <span class="math inline">\(\exp\{\beta_1\}\)</span> times the median of <span class="math inline">\(y\)</span> for <span class="math inline">\(x\)</span>.</p>
<p><em>Interpretation: When <span class="math inline">\(x\)</span> increases by one unit, the median of <span class="math inline">\(y\)</span> is expected to multiply by a factor of <span class="math inline">\(\exp\{\beta_1\}\)</span>.</em></p>
</div>
</div>
<div id="log-transformation-on-the-predictor-variable" class="section level3">
<h3><span class="header-section-number">6.5.2</span> Log-transformation on the predictor variable</h3>
<p>Suppose we fit a linear regression model with <span class="math inline">\(\log(x)\)</span>, the log-transformed <span class="math inline">\(x\)</span>, as the predictor variable. Under this model, we assume a linear relationship exists between <span class="math inline">\(\log(x)\)</span> and <span class="math inline">\(y\)</span>, such that <span class="math inline">\(y \sim N(\beta_0 + \beta_1 \log(x), \sigma^2)\)</span> for some <span class="math inline">\(\beta_0\)</span>, <span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\sigma^2\)</span>. In other words, we can model the relationship between <span class="math inline">\(\log(x)\)</span> and <span class="math inline">\(y\)</span> using the model in ().</p>
<span class="math display">\[\begin{equation}
\label{log-x}
y = \beta_0 + \beta_1 \log(x)
\end{equation}\]</span>
<p><strong>Intercept</strong>: The intercept is the mean of <span class="math inline">\(y\)</span> when <span class="math inline">\(\log(x) = 0\)</span>, i.e. <span class="math inline">\(x = 1\)</span>.</p>
<p><em>Interpretation: When <span class="math inline">\(x = 1\)</span> <span class="math inline">\((\log(x) = 0)\)</span>, the mean of <span class="math inline">\(y\)</span> is expected to be <span class="math inline">\(\beta_0\)</span>.</em></p>
<p><strong>Slope</strong>: The slope is interpreted in terms of the change in the mean of <span class="math inline">\(y\)</span> when <span class="math inline">\(x\)</span> is mutliplied by a factor of <span class="math inline">\(C\)</span>, since <span class="math inline">\(\log(Cx) = \log(x) + \log(C)\)</span>. Thus, when <span class="math inline">\(x\)</span> is multiplied by a factor of <span class="math inline">\(C\)</span>, the change in the mean of <span class="math inline">\(y\)</span> is</p>
<span class="math display">\[\begin{equation}
\begin{aligned}
[\beta_0 + \beta_1 \log(Cx)] - [\beta_0 + \beta_1 \log(x)] &amp;= \beta_1 [\log(Cx) - \log(x)] \\[10pt] 
&amp; = \beta_1[\log(C) + \log(x) - \log(x)] \\[10pt] 
&amp; = \beta_1 \log(C)
\end{aligned}
\end{equation}\]</span>
<p>Thus the mean of <span class="math inline">\(y\)</span> changes by <span class="math inline">\(\beta_1 \log(C)\)</span> units.</p>
<p><em>Interpretation: When <span class="math inline">\(x\)</span> is multiplied by a factor of <span class="math inline">\(C\)</span>, the mean of <span class="math inline">\(y\)</span> is expected to change by <span class="math inline">\(\beta_1 \log(C)\)</span> units. For example, if <span class="math inline">\(x\)</span> is doubled, then the mean of <span class="math inline">\(y\)</span> is expected to change by <span class="math inline">\(\beta_1 \log(2)\)</span> units.</em></p>
</div>
<div id="log-transformation-on-the-the-response-and-predictor-variable" class="section level3">
<h3><span class="header-section-number">6.5.3</span> Log-transformation on the the response and predictor variable</h3>
<p>Suppose we fit a linear regression model with <span class="math inline">\(\log(x)\)</span>, the log-transformed <span class="math inline">\(x\)</span>, as the predictor variable and <span class="math inline">\(\log(y)\)</span>, the log-transformed <span class="math inline">\(y\)</span>, as the response variable. Under this model, we assume a linear relationship exists between <span class="math inline">\(\log(x)\)</span> and <span class="math inline">\(\log(y)\)</span>, such that <span class="math inline">\(\log(y) \sim N(\beta_0 + \beta_1 \log(x), \sigma^2)\)</span> for some <span class="math inline">\(\beta_0\)</span>, <span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\sigma^2\)</span>. In other words, we can model the relationship between <span class="math inline">\(\log(x)\)</span> and <span class="math inline">\(\log(y)\)</span> using the model in ().</p>
<span class="math display">\[\begin{equation}
\label{log-x-y}
\log(y) = \beta_0 + \beta_1 \log(x)
\end{equation}\]</span>
<p>Because the response variable is log-transformed, the interpretations on the original scale will be in terms of the median of <span class="math inline">\(y\)</span> (see the section on the log-transformed response variable for more detail).</p>
<p><strong>Intercept</strong>: The intercept is the mean of <span class="math inline">\(y\)</span> when <span class="math inline">\(\log(x) = 0\)</span>, i.e. <span class="math inline">\(x = 1\)</span>. Therefore, when <span class="math inline">\(\log(x) = 0\)</span>,</p>
<span class="math display">\[\begin{equation}
\begin{aligned}
&amp;\log(y) = \beta_0 + \beta_1 \times 0 = \beta_0 \\[10pt]
\Rightarrow &amp;y = \exp\{\beta_0\}
\end{aligned}
\end{equation}\]</span>
<p><em>Interpretation: When <span class="math inline">\(x = 1\)</span> <span class="math inline">\((\log(x) = 0)\)</span>, the median of <span class="math inline">\(y\)</span> is expected to be <span class="math inline">\(\exp\{\beta_0\}\)</span>.</em></p>
<p><strong>Slope</strong>: The slope is interpreted in terms of the change in the median <span class="math inline">\(y\)</span> when <span class="math inline">\(x\)</span> is mutliplied by a factor of <span class="math inline">\(C\)</span>, since <span class="math inline">\(\log(Cx) = \log(x) + \log(C)\)</span>. Thus, when <span class="math inline">\(x\)</span> is multiplied by a factor of <span class="math inline">\(C\)</span>, the change in the median of <span class="math inline">\(y\)</span> is</p>
<span class="math display">\[\begin{equation}
\begin{aligned}
\exp\{[\beta_0 + \beta_1 \log(Cx)] - [\beta_0 + \beta_1 \log(x)]\} &amp;= 
\exp\{\beta_1 [\log(Cx) - \log(x)]\} \\[10pt] 
&amp; = \exp\{\beta_1[\log(C) + \log(x) - \log(x)]\} \\[10pt] 
&amp; = \exp\{\beta_1 \log(C)\} = C^{\beta_1}
\end{aligned}
\end{equation}\]</span>
<p>Thus, the median of <span class="math inline">\(y\)</span> for <span class="math inline">\(Cx\)</span> is <span class="math inline">\(C^{\beta_1}\)</span> times the median of <span class="math inline">\(y\)</span> for <span class="math inline">\(x\)</span>.</p>
<p><em>Interpretation: When <span class="math inline">\(x\)</span> is multiplied by a factor of <span class="math inline">\(C\)</span>, the median of <span class="math inline">\(y\)</span> is expected to multiple by a factor of <span class="math inline">\(C^{\beta_1}\)</span>. For example, if <span class="math inline">\(x\)</span> is doubled, then the median of <span class="math inline">\(y\)</span> is expected to multiply by <span class="math inline">\(2^{\beta_1}\)</span>.</em></p>
<hr />
</div>
</div>
<div id="notes-model-diagnostics" class="section level2">
<h2><span class="header-section-number">6.6</span> NOTES: Model Diagnostics</h2>
<p>This document discusses some of the mathematical details of the model diagnostics - leverage, standardized residuals, and Cook’s distance. We assume the reader knowledge of the matrix form for multiple linear regression.Please see <a href="https://github.com/STA210-Sp19/supplemental-notes/blob/master/regression-basics-matrix.pdf">Matrix Form of Linear Regression</a> for a review.</p>
<div id="introduction-3" class="section level3">
<h3><span class="header-section-number">6.6.1</span> Introduction</h3>
<p>Suppose we have <span class="math inline">\(n\)</span> observations. Let the <span class="math inline">\(i^{th}\)</span> be <span class="math inline">\((x_{i1}, \ldots, x_{ip}, y_i)\)</span>, such that <span class="math inline">\(x_{i1}, \ldots, x_{ip}\)</span> are the explanatory variables (predictors) and <span class="math inline">\(y_i\)</span> is the response variable. We assume the data can be modeled using the least-squares regression model, such that the mean response for a given combination of explanatory variables follows the form in ().</p>
<span class="math display">\[\begin{equation}
\label{basic_model}
y = \beta_0 + \beta_1 x_1 + \dots + \beta_p x_p 
\end{equation}\]</span>
<p>We can write the response for the <span class="math inline">\(i^{th}\)</span> observation as shown in ()</p>
<span class="math display">\[\begin{equation}
\label{ind_response}
y_i = \beta_0 + \beta_1 x_{i1} + \dots + \beta_p x_{ip} + \epsilon_i 
\end{equation}\]</span>
<p>such that <span class="math inline">\(\epsilon_i\)</span> is the amount <span class="math inline">\(y_i\)</span> deviates from <span class="math inline">\(\mu\{y|x_{i1}, \ldots, x_{ip}\}\)</span>, the mean response for a given combination of explanatory variables. We assume each <span class="math inline">\(\epsilon_i \sim N(0,\sigma^2)\)</span>, where <span class="math inline">\(\sigma^2\)</span> is a constant variance for the distribution of the response <span class="math inline">\(y\)</span> for any combination of explanatory variables <span class="math inline">\(x_1, \ldots, x_p\)</span>.</p>
</div>
<div id="matrix-form-for-the-regression-model-1" class="section level3">
<h3><span class="header-section-number">6.6.2</span> Matrix Form for the Regression Model</h3>
<p>We can represent the () and () using matrix notation. Let</p>
<span class="math display">\[\begin{equation}
\label{matrix notation}
\mathbf{Y} = \begin{bmatrix}y_1 \\ y_2 \\ \vdots \\y_n\end{bmatrix} 
\hspace{15mm}
\mathbf{X} = \begin{bmatrix}x_{11} &amp; x_{12} &amp; \dots &amp; x_{1p} \\
x_{21} &amp; x_{22} &amp; \dots &amp; x_{2p} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots\\
x_{n1} &amp; x_{n2} &amp; \dots &amp; x_{np} \end{bmatrix}
\hspace{15mm}
\boldsymbol{\beta}= \begin{bmatrix}\beta_0 \\ \beta_1 \\ \vdots \\ \beta_p \end{bmatrix} 
\hspace{15mm}
\boldsymbol{\epsilon}= \begin{bmatrix}\epsilon_1 \\ \epsilon_2 \\ \vdots \\ \epsilon_n \end{bmatrix}
\end{equation}\]</span>
<p>Thus,</p>
<p><span class="math display">\[\mathbf{Y} = \mathbf{X}\boldsymbol{\beta} + \mathbf{\epsilon}\]</span></p>
<p>Therefore the estimated response for a given combination of explanatory variables and the associated residuals can be written as</p>
<span class="math display">\[\begin{equation}
\label{matrix_mean}
\hat{\mathbf{Y}} = \mathbf{X}\hat{\boldsymbol{\beta}} \hspace{10mm} \mathbf{e} = \mathbf{Y} - \mathbf{X}\hat{\boldsymbol{\beta}}
\end{equation}\]</span>
</div>
<div id="hat-matrix-leverage" class="section level3">
<h3><span class="header-section-number">6.6.3</span> Hat Matrix &amp; Leverage</h3>
<p>Recall from the notes <a href="https://github.com/STA210-Sp19/supplemental-notes/blob/master/regression-basics-matrix.pdf"><strong>Matrix Form of Linear Regression</strong></a> that <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> can be written as the following:</p>
<span class="math display">\[\begin{equation}
\label{beta-hat}
\hat{\boldsymbol{\beta}} = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{Y}
\end{equation}\]</span>
<p>Combining () and (), we can write <span class="math inline">\(\hat{\mathbf{Y}}\)</span> as the following:</p>
<span class="math display">\[\begin{equation}
\label{y-hat}
\begin{aligned}
\hat{\mathbf{Y}} &amp;= \mathbf{X}\hat{\boldsymbol{\beta}} \\[10pt]
&amp;= \mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{Y}\\
\end{aligned}
\end{equation}\]</span>
<p>We define the <strong>hat matrix</strong> as an <span class="math inline">\(n \times n\)</span> matrix of the form <span class="math inline">\(\mathbf{H} = \mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\)</span>. Thus () becomes</p>
<span class="math display">\[\begin{equation}
\label{y-hat-matrix}
\hat{\mathbf{Y}} = \mathbf{H}\mathbf{Y}
\end{equation}\]</span>
<p>The diagonal elements of the hat matrix are a measure of how far the predictor variables of each observation are from the means of the predictor variables. For example, <span class="math inline">\(h_{ii}\)</span> is a measure of how far the values of the predictor variables for the <span class="math inline">\(i^{th}\)</span> observation, <span class="math inline">\(x_{i1}, x_{i2}, \ldots, x_{ip}\)</span>, are from the mean values of the predictor variables, <span class="math inline">\(\bar{x}_1, \bar{x}_2, \ldots, \bar{x}_p\)</span>. In the case of simple linear regression, the <span class="math inline">\(i^{th}\)</span> diagonal, <span class="math inline">\(h_{ii}\)</span>, can be written as</p>
<p><span class="math display">\[h_{ii} =  \frac{1}{n} + \frac{(x_i - \bar{x})^2}{\sum_{j=1}^{n}(x_j-\bar{x})^2}\]</span></p>
<p>We call these diagonal elements, the <strong>leverage</strong> of each observation.</p>
<p>The diagonal elements of the hat matrix have the following properties:</p>
<ul>
<li><span class="math inline">\(0 \leq h_ii \leq 1\)</span></li>
<li><span class="math inline">\(\sum\limits_{i=1}^{n} h_{ii} = p+1\)</span>, where <span class="math inline">\(p\)</span> is the number of predictor variables in the model.</li>
<li>The mean hat value is <span class="math inline">\(\bar{h} = \frac{\sum\limits_{i=1}^{n} h_{ii}}{n} = \frac{p+1}{n}\)</span>.</li>
</ul>
<p>Using these properties, we consider a point to have <strong>high leverage</strong> if it has a leverage value that is more than 2 times the average. In other words, observations with leverage greater than <span class="math inline">\(\frac{2(p+1)}{n}\)</span> are considered to be <strong>high leverage</strong> points, i.e. outliers in the predictor variables. We are interested in flagging high leverage points, because they may have an influence on the regression coefficients.</p>
<p>When there are high leverage points in the data, the regression line will tend towards those points; therefore, one property of high leverage points is that they tend to have small residuals. We will show this by rewriting the residuals from () using ().</p>
<span class="math display">\[\begin{equation}
\label{resid-hat}
\begin{aligned}
\mathbf{e} &amp;= \mathbf{Y} - \hat{\mathbf{Y}} \\[10pt]
&amp; = \mathbf{Y} - \mathbf{H}\mathbf{Y} \\[10pt]
&amp;= (1-\mathbf{H})\mathbf{Y}
\end{aligned}
\end{equation}\]</span>
<p>Note that the identity matrix and hat matrix are <strong>idempotent</strong>, i.e. <span class="math inline">\(\mathbf{I}\mathbf{I} = \mathbf{I}\)</span>, <span class="math inline">\(\mathbf{H}\mathbf{H} = \mathbf{H}\)</span>. Thus, <span class="math inline">\((\mathbf{I} - \mathbf{H}\)</span> is also idempotent. These matrices are also symmetric. Using these properties and (), we have that the variance-covariance matrix of the residuals <span class="math inline">\(\boldsymbol{e}\)</span>, is</p>
<span class="math display">\[\begin{equation}
\label{resid-var}
\begin{aligned}
Var(\mathbf{e}) &amp;= \mathbf{e}\mathbf{e}^T \\[10pt]
&amp;=  (1-\mathbf{H})Var(\mathbf{Y})^T(1-\mathbf{H})^T \\[10pt]
&amp;= (1-\mathbf{H})\hat{\sigma}^2(1-\mathbf{H})^T  \\[10pt]
&amp;= \hat{\sigma}^2(1-\mathbf{H})(1-\mathbf{H})  \\[10pt]
&amp;= \hat{\sigma}^2(1-\mathbf{H})
\end{aligned}
\end{equation}\]</span>
<p>where <span class="math inline">\(\hat{\sigma}^2 = \frac{\sum_{i=1}^{n}e_i^2}{n-p-1}\)</span> is the estimated regression variance. Thus, the variance of the <span class="math inline">\(i^{th}\)</span> residual is <span class="math inline">\(Var(e_i) = \hat{\sigma}^2(1-h_{ii})\)</span>. Therefore, the higher the leverage, the smaller the variance of the residual. Because the expected value of the residuals is 0, we conclude that points with high leverage tend to have smaller residuals than points with lower leverage.</p>
</div>
<div id="standardized-residuals" class="section level3">
<h3><span class="header-section-number">6.6.4</span> Standardized Residuals</h3>
<p>In general, we standardize a value by shifting by the expected value and rescaling by the standard deviation (or standard error). Thus, the <span class="math inline">\(i^{th}\)</span> standardized residual takes the form</p>
<p><span class="math display">\[std.res_i = \frac{e_i - E(e_i)}{SE(e_i)}\]</span></p>
<p>The expected value of the residuals is 0, i.e. <span class="math inline">\(E(e_i) = 0\)</span>. From (), the standard error of the residual is <span class="math inline">\(SE(e_i) = \hat{\sigma}\sqrt{1-h_{ii}}\)</span>. Therefore,</p>
<span class="math display">\[\begin{equation}
\label{std.resid.}
std.res_i = \frac{e_i}{\hat{\sigma}\sqrt{1-h_{ii}}}
\end{equation}\]</span>
</div>
<div id="cooks-distance" class="section level3">
<h3><span class="header-section-number">6.6.5</span> Cook’s Distance</h3>
<p>Cook’s distance is a measure of how much each observation influences the model coefficients, and thus the predicted values. The Cook’s distance for the <span class="math inline">\(i^{th}\)</span> observation can be written as</p>
<span class="math display">\[\begin{equation}
\label{cooksd}
D_i = \frac{(\hat{\mathbf{Y}} -\hat{\mathbf{Y}}_{(i)})^T(\hat{\mathbf{Y}} -\hat{\mathbf{Y}}_{(i)})}{(p+1)\hat{\sigma}}
\end{equation}\]</span>
<p>where <span class="math inline">\(\hat{\mathbf{Y}}_{(i)}\)</span> is the vector of predicted values from the model fitted when the <span class="math inline">\(i^{th}\)</span> observation is deleted. Cook’s Distance can be calculated without deleting observations one at a time, since () below is mathematically equivalent to ().</p>
<span class="math display">\[\begin{equation}
\label{cooksd-v2}
D_i = \frac{1}{p+1}std.res_i^2\Bigg[\frac{h_{ii}}{(1-h_{ii})}\Bigg] = \frac{e_i^2}{(p+1)\hat{\sigma}^2(1-h_{ii})}\Bigg[\frac{h_{ii}}{(1-h_{ii})}\Bigg]
\end{equation}\]</span>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="5-anova.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="7-select.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": ["intro-regression.pdf", "intro-regression.epub"],
"toc": {
"collapse": "section",
"toc_depth": 1,
"scroll_highlight": true
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
