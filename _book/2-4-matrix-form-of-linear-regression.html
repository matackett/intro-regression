<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>2.4 Matrix Form of Linear Regression | Intro to Regression Analysis</title>
  <meta name="description" content="This document contains lab assignments and other materials for an intermediate-level regression course.">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="2.4 Matrix Form of Linear Regression | Intro to Regression Analysis" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This document contains lab assignments and other materials for an intermediate-level regression course." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="2.4 Matrix Form of Linear Regression | Intro to Regression Analysis" />
  
  <meta name="twitter:description" content="This document contains lab assignments and other materials for an intermediate-level regression course." />
  

<meta name="author" content="Maria Tackett">


<meta name="date" content="2019-05-14">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="2-3-analyzing-wages.html">
<link rel="next" href="2-5-log-transformations-in-linear-regression.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />










<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Broadening Your Statistical Horizons</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Beginning of the Book</a></li>
<li class="chapter" data-level="2" data-path="2-mlr.html"><a href="2-mlr.html"><i class="fa fa-check"></i><b>2</b> Multiple Linear Regression</a><ul>
<li class="chapter" data-level="2.1" data-path="2-1-multiple-linear-regression.html"><a href="2-1-multiple-linear-regression.html"><i class="fa fa-check"></i><b>2.1</b> Multiple Linear Regression</a><ul>
<li class="chapter" data-level="2.1.1" data-path="2-1-multiple-linear-regression.html"><a href="2-1-multiple-linear-regression.html#packages"><i class="fa fa-check"></i><b>2.1.1</b> Packages</a></li>
<li class="chapter" data-level="2.1.2" data-path="2-1-multiple-linear-regression.html"><a href="2-1-multiple-linear-regression.html#data"><i class="fa fa-check"></i><b>2.1.2</b> Data</a></li>
<li class="chapter" data-level="2.1.3" data-path="2-1-multiple-linear-regression.html"><a href="2-1-multiple-linear-regression.html#exercises"><i class="fa fa-check"></i><b>2.1.3</b> Exercises</a></li>
<li class="chapter" data-level="2.1.4" data-path="2-1-multiple-linear-regression.html"><a href="2-1-multiple-linear-regression.html#acknowledgement"><i class="fa fa-check"></i><b>2.1.4</b> Acknowledgement</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="2-2-data-wrangling-multiple-linear-regression.html"><a href="2-2-data-wrangling-multiple-linear-regression.html"><i class="fa fa-check"></i><b>2.2</b> Data Wrangling &amp; Multiple Linear Regression</a><ul>
<li class="chapter" data-level="2.2.1" data-path="2-2-data-wrangling-multiple-linear-regression.html"><a href="2-2-data-wrangling-multiple-linear-regression.html#packages-1"><i class="fa fa-check"></i><b>2.2.1</b> Packages</a></li>
<li class="chapter" data-level="2.2.2" data-path="2-2-data-wrangling-multiple-linear-regression.html"><a href="2-2-data-wrangling-multiple-linear-regression.html#data-1"><i class="fa fa-check"></i><b>2.2.2</b> Data</a></li>
<li class="chapter" data-level="2.2.3" data-path="2-2-data-wrangling-multiple-linear-regression.html"><a href="2-2-data-wrangling-multiple-linear-regression.html#exercises-1"><i class="fa fa-check"></i><b>2.2.3</b> Exercises</a></li>
<li class="chapter" data-level="2.2.4" data-path="2-2-data-wrangling-multiple-linear-regression.html"><a href="2-2-data-wrangling-multiple-linear-regression.html#acknowledgement-1"><i class="fa fa-check"></i><b>2.2.4</b> Acknowledgement</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="2-3-analyzing-wages.html"><a href="2-3-analyzing-wages.html"><i class="fa fa-check"></i><b>2.3</b> Analyzing Wages</a><ul>
<li class="chapter" data-level="2.3.1" data-path="2-3-analyzing-wages.html"><a href="2-3-analyzing-wages.html#initial-model"><i class="fa fa-check"></i><b>2.3.1</b> Initial model</a></li>
<li class="chapter" data-level="2.3.2" data-path="2-3-analyzing-wages.html"><a href="2-3-analyzing-wages.html#model-with-mean-centered-variables"><i class="fa fa-check"></i><b>2.3.2</b> Model with mean-centered variables</a></li>
<li class="chapter" data-level="2.3.3" data-path="2-3-analyzing-wages.html"><a href="2-3-analyzing-wages.html#model-with-indicator-variables"><i class="fa fa-check"></i><b>2.3.3</b> Model with indicator variables</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="2-4-matrix-form-of-linear-regression.html"><a href="2-4-matrix-form-of-linear-regression.html"><i class="fa fa-check"></i><b>2.4</b> Matrix Form of Linear Regression</a><ul>
<li class="chapter" data-level="2.4.1" data-path="2-4-matrix-form-of-linear-regression.html"><a href="2-4-matrix-form-of-linear-regression.html#introduction"><i class="fa fa-check"></i><b>2.4.1</b> Introduction</a></li>
<li class="chapter" data-level="2.4.2" data-path="2-4-matrix-form-of-linear-regression.html"><a href="2-4-matrix-form-of-linear-regression.html#matrix-form-for-the-regression-model"><i class="fa fa-check"></i><b>2.4.2</b> Matrix Form for the Regression Model</a></li>
<li class="chapter" data-level="2.4.3" data-path="2-4-matrix-form-of-linear-regression.html"><a href="2-4-matrix-form-of-linear-regression.html#estimating-the-coefficients"><i class="fa fa-check"></i><b>2.4.3</b> Estimating the Coefficients</a></li>
<li class="chapter" data-level="2.4.4" data-path="2-4-matrix-form-of-linear-regression.html"><a href="2-4-matrix-form-of-linear-regression.html#variance-covariance-matrix-of-the-coefficients"><i class="fa fa-check"></i><b>2.4.4</b> Variance-covariance matrix of the coefficients</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="2-5-log-transformations-in-linear-regression.html"><a href="2-5-log-transformations-in-linear-regression.html"><i class="fa fa-check"></i><b>2.5</b> Log Transformations in Linear Regression</a><ul>
<li class="chapter" data-level="2.5.1" data-path="2-5-log-transformations-in-linear-regression.html"><a href="2-5-log-transformations-in-linear-regression.html#log-transformation-on-the-response-variable"><i class="fa fa-check"></i><b>2.5.1</b> Log-transformation on the response variable</a></li>
<li class="chapter" data-level="2.5.2" data-path="2-5-log-transformations-in-linear-regression.html"><a href="2-5-log-transformations-in-linear-regression.html#log-transformation-on-the-predictor-variable"><i class="fa fa-check"></i><b>2.5.2</b> Log-transformation on the predictor variable</a></li>
<li class="chapter" data-level="2.5.3" data-path="2-5-log-transformations-in-linear-regression.html"><a href="2-5-log-transformations-in-linear-regression.html#log-transformation-on-the-the-response-and-predictor-variable"><i class="fa fa-check"></i><b>2.5.3</b> Log-transformation on the the response and predictor variable</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="2-6-details-about-model-diagnostics.html"><a href="2-6-details-about-model-diagnostics.html"><i class="fa fa-check"></i><b>2.6</b> Details about Model Diagnostics</a><ul>
<li class="chapter" data-level="2.6.1" data-path="2-6-details-about-model-diagnostics.html"><a href="2-6-details-about-model-diagnostics.html#introduction-1"><i class="fa fa-check"></i><b>2.6.1</b> Introduction</a></li>
<li class="chapter" data-level="2.6.2" data-path="2-6-details-about-model-diagnostics.html"><a href="2-6-details-about-model-diagnostics.html#matrix-form-for-the-regression-model-1"><i class="fa fa-check"></i><b>2.6.2</b> Matrix Form for the Regression Model</a></li>
<li class="chapter" data-level="2.6.3" data-path="2-6-details-about-model-diagnostics.html"><a href="2-6-details-about-model-diagnostics.html#hat-matrix-leverage"><i class="fa fa-check"></i><b>2.6.3</b> Hat Matrix &amp; Leverage</a></li>
<li class="chapter" data-level="2.6.4" data-path="2-6-details-about-model-diagnostics.html"><a href="2-6-details-about-model-diagnostics.html#standardized-residuals"><i class="fa fa-check"></i><b>2.6.4</b> Standardized Residuals</a></li>
<li class="chapter" data-level="2.6.5" data-path="2-6-details-about-model-diagnostics.html"><a href="2-6-details-about-model-diagnostics.html#cooks-distance"><i class="fa fa-check"></i><b>2.6.5</b> Cookâ€™s Distance</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Intro to Regression Analysis</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="matrix-form-of-linear-regression" class="section level2">
<h2><span class="header-section-number">2.4</span> Matrix Form of Linear Regression</h2>
<p>This document provides the details for the matrix form of multiple linear regression. We assume the reader has familiarity with some matrix alegbra. Please see Chapter 1 of <a href="https://www-bcf.usc.edu/~gareth/ISL/"><em>An Introduction to Statistical Learning</em></a> for a brief review of matrix algebra.</p>
<div id="introduction" class="section level3">
<h3><span class="header-section-number">2.4.1</span> Introduction</h3>
<p>Suppose we have <span class="math inline">\(n\)</span> observations. Let the <span class="math inline">\(i^{th}\)</span> be <span class="math inline">\((x_{i1}, \ldots, x_{ip}, y_i)\)</span>, such that <span class="math inline">\(x_{i1}, \ldots, x_{ip}\)</span> are the explanatory variables (predictors) and <span class="math inline">\(y_i\)</span> is the response variable. We assume the data can be modeled using the least-squares regression model, such that the mean response for a given combination of explanatory variables follows the form in ().</p>
<span class="math display">\[\begin{equation}
\label{basic_model}
y = \beta_0 + \beta_1 x_1 + \dots + \beta_p x_p 
\end{equation}\]</span>
<p>We can write the response for the <span class="math inline">\(i^{th}\)</span> observation as shown in ()</p>
<span class="math display">\[\begin{equation}
\label{ind_response}
y_i = \beta_0 + \beta_1 x_{i1} + \dots + \beta_p x_{ip} + \epsilon_i 
\end{equation}\]</span>
<p>such that <span class="math inline">\(\epsilon_i\)</span> is the amount <span class="math inline">\(y_i\)</span> deviates from <span class="math inline">\(\mu\{y|x_{i1}, \ldots, x_{ip}\}\)</span>, the mean response for a given combination of explanatory variables. We assume each <span class="math inline">\(\epsilon_i \sim N(0,\sigma^2)\)</span>, where <span class="math inline">\(\sigma^2\)</span> is a constant variance for the distribution of the response <span class="math inline">\(y\)</span> for any combination of explanatory variables <span class="math inline">\(x_1, \ldots, x_p\)</span>.</p>
</div>
<div id="matrix-form-for-the-regression-model" class="section level3">
<h3><span class="header-section-number">2.4.2</span> Matrix Form for the Regression Model</h3>
<p>We can represent the () and () using matrix notation. Let</p>
<span class="math display">\[\begin{equation}
\label{matrix notation}
\mathbf{Y} = \begin{bmatrix}y_1 \\ y_2 \\ \vdots \\y_n\end{bmatrix} 
\hspace{15mm}
\mathbf{X} = \begin{bmatrix}x_{11} &amp; x_{12} &amp; \dots &amp; x_{1p} \\
x_{21} &amp; x_{22} &amp; \dots &amp; x_{2p} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots\\
x_{n1} &amp; x_{n2} &amp; \dots &amp; x_{np} \end{bmatrix}
\hspace{15mm}
\boldsymbol{\beta}= \begin{bmatrix}\beta_0 \\ \beta_1 \\ \vdots \\ \beta_p \end{bmatrix} 
\hspace{15mm}
\boldsymbol{\epsilon}= \begin{bmatrix}\epsilon_1 \\ \epsilon_2 \\ \vdots \\ \epsilon_n \end{bmatrix}
\end{equation}\]</span>
<p>Thus,</p>
<p><span class="math display">\[\mathbf{Y} = \mathbf{X}\boldsymbol{\beta} + \mathbf{\epsilon}\]</span></p>
<p>Therefore the estimated response for a given combination of explanatory variables and the associated residuals can be written as</p>
<span class="math display">\[\begin{equation}
\label{matrix_mean}
\hat{\mathbf{Y}} = \mathbf{X}\hat{\boldsymbol{\beta}} \hspace{10mm} \mathbf{e} = \mathbf{Y} - \mathbf{X}\hat{\boldsymbol{\beta}}
\end{equation}\]</span>
</div>
<div id="estimating-the-coefficients" class="section level3">
<h3><span class="header-section-number">2.4.3</span> Estimating the Coefficients</h3>
<p>The least-squares model is the one that minimizes the sum of the squared residuals. Therefore, we want to find the coefficients, <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> that minimizes</p>
<span class="math display">\[\begin{equation}
\label{sum_sq_resid}
\sum\limits_{i=1}^{n} e_{i}^2 = \mathbf{e}^T\mathbf{e} = (\mathbf{Y} - \mathbf{X}\hat{\boldsymbol{\beta}})^T(\mathbf{Y} - \mathbf{X}\hat{\boldsymbol{\beta}})
\end{equation}\]</span>
<p>where <span class="math inline">\(\mathbf{e}^T\)</span>, the transpose of the matrix <span class="math inline">\(\mathbf{e}\)</span>.</p>
<span class="math display">\[\begin{equation}
\label{model_equation}
(\mathbf{Y} - \mathbf{X}\hat{\boldsymbol{\beta}})^T(\mathbf{Y} - \mathbf{X}\hat{\boldsymbol{\beta}}) = (\mathbf{Y}^T\mathbf{Y} - 
\mathbf{Y}^T \mathbf{X}\hat{\boldsymbol{\beta}} - (\hat{\boldsymbol{\beta}}{}^{T}\mathbf{X}^T\mathbf{Y} +
\hat{\boldsymbol{\beta}}{}^{T}\mathbf{X}^T\mathbf{X}
\hat{\boldsymbol{\beta}})
\end{equation}\]</span>
<p>Note that <span class="math inline">\((\mathbf{Y^T}\mathbf{X}\hat{\boldsymbol{\beta}})^T = \hat{\boldsymbol{\beta}}{}^{T}\mathbf{X}^T\mathbf{Y}\)</span>. Since these are both constants (i.e. <span class="math inline">\(1\times 1\)</span> vectors), <span class="math inline">\(\mathbf{Y^T}\mathbf{X}\hat{\boldsymbol{\beta}} = \hat{\boldsymbol{\beta}}{}^{T}\mathbf{X}^T\mathbf{Y}\)</span>. Thus, () becomes</p>
<span class="math display">\[\begin{equation}
\mathbf{Y}^T\mathbf{Y} - 2 \mathbf{X}^T\hat{\boldsymbol{\beta}}{}^{T}\mathbf{Y} + \hat{\boldsymbol{\beta}}{}^{T}\mathbf{X}^T\mathbf{X}
\hat{\boldsymbol{\beta}}
\end{equation}\]</span>
<p>Since we want to find the <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> that minimizes (), will find the value of <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> such that the derivative with respect to <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> is equal to 0.</p>
<span class="math display">\[\begin{equation}
\begin{aligned}
\frac{\partial \mathbf{e}^T\mathbf{e}}{\partial \hat{\boldsymbol{\beta}}} &amp; = \frac{\partial}{\partial \hat{\boldsymbol{\beta}}}(\mathbf{Y}^T\mathbf{Y} - 2 \mathbf{X}^T\hat{\boldsymbol{\beta}}{}^T\mathbf{Y} + \hat{\boldsymbol{\beta}}{}^{T}\mathbf{X}^T\mathbf{X}\hat{\boldsymbol{\beta}}) = 0 \\[10pt]
&amp;\Rightarrow - 2 \mathbf{X}^T\mathbf{Y} + 2 \mathbf{X}^T\mathbf{X}\hat{\boldsymbol{\beta}} = 0 \\[10pt]
&amp; \Rightarrow 2 \mathbf{X}^T\mathbf{Y} = 2 \mathbf{X}^T\mathbf{X}\hat{\boldsymbol{\beta}} \\[10pt]
&amp; \Rightarrow \mathbf{X}^T\mathbf{Y} = \mathbf{X}^T\mathbf{X}\hat{\boldsymbol{\beta}} \\[10pt]
&amp; \Rightarrow (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{Y} = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{X}\hat{\boldsymbol{\beta}} \\[10pt]
&amp; \Rightarrow (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{Y} = \mathbf{I}\hat{\boldsymbol{\beta}}
\end{aligned}
\end{equation}\]</span>
<p>Thus, the estimate of the model coefficients is <span class="math inline">\(\hat{\boldsymbol{\beta}} = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{Y}\)</span>.</p>
</div>
<div id="variance-covariance-matrix-of-the-coefficients" class="section level3">
<h3><span class="header-section-number">2.4.4</span> Variance-covariance matrix of the coefficients</h3>
<p>We will use two properties to derive the form of the variance-covarinace matrix of the coefficients:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(E[\boldsymbol{\epsilon}\boldsymbol{\epsilon}^T] = \sigma^2I\)</span></li>
<li><span class="math inline">\(\hat{\boldsymbol{\beta}} = \boldsymbol{\beta} + (\mathbf{X}^T\mathbf{X})^{-1}\epsilon\)</span></li>
</ol>
First, we will show that <span class="math inline">\(E[\boldsymbol{\epsilon}\boldsymbol{\epsilon}^T] = \sigma^2I\)</span>
<span class="math display">\[\begin{equation}
\label{expected_error}
\begin{aligned}
E[\boldsymbol{\epsilon}\boldsymbol{\epsilon}^T] &amp;= E \begin{bmatrix}\epsilon_1  &amp; \epsilon_2 &amp; \dots &amp; \epsilon_n \end{bmatrix}\begin{bmatrix}\epsilon_1 \\ \epsilon_2 \\ \vdots \\ \epsilon_n \end{bmatrix}  \\[10pt]
&amp; = E \begin{bmatrix} \epsilon_1^2  &amp; \epsilon_1 \epsilon_2 &amp; \dots &amp; \epsilon_1 \epsilon_n \\
\epsilon_2 \epsilon_1 &amp; \epsilon_2^2 &amp; \dots &amp; \epsilon_2 \epsilon_n \\ 
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 
\epsilon_n \epsilon_1 &amp; \epsilon_n \epsilon_2 &amp; \dots &amp; \epsilon_n^2 
\end{bmatrix} \\[10pt]
&amp; = \begin{bmatrix} E[\epsilon_1^2]  &amp; E[\epsilon_1 \epsilon_2] &amp; \dots &amp; E[\epsilon_1 \epsilon_n] \\
E[\epsilon_2 \epsilon_1] &amp; E[\epsilon_2^2] &amp; \dots &amp; E[\epsilon_2 \epsilon_n] \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 
E[\epsilon_n \epsilon_1] &amp; E[\epsilon_n \epsilon_2] &amp; \dots &amp; E[\epsilon_n^2]
\end{bmatrix}
\end{aligned}
\end{equation}\]</span>
<p>Recall, the regression assumption that the errors <span class="math inline">\(\epsilon_i&#39;s\)</span> are Normally distributed with mean 0 and variance <span class="math inline">\(\sigma^2\)</span>. Thus, <span class="math inline">\(E(\epsilon_i^2) = Var(\epsilon_i) = \sigma^2\)</span> for all <span class="math inline">\(i\)</span>. Additionally, recall the regression assumption that the errors are uncorrelated, i.e. <span class="math inline">\(E(\epsilon_i \epsilon_j) = Cov(\epsilon_i, \epsilon_j) = 0\)</span> for all <span class="math inline">\(i,j\)</span>. Using these assumptions, we can write () as</p>
<span class="math display">\[\begin{equation}
E[\mathbf{\epsilon}\mathbf{\epsilon}^T]  = \begin{bmatrix} \sigma^2  &amp; 0 &amp; \dots &amp; 0 \\
0 &amp; \sigma^2  &amp; \dots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; \dots &amp; \sigma^2
\end{bmatrix} = \sigma^2 \mathbf{I}
\end{equation}\]</span>
<p>where <span class="math inline">\(\mathbf{I}\)</span> is the <span class="math inline">\(n \times n\)</span> identity matrix.</p>
<p>Next, we show that <span class="math inline">\(\hat{\boldsymbol{\beta}} = \boldsymbol{\beta} + (\mathbf{X}^T\mathbf{X})^{-1}\epsilon\)</span>.</p>
<p>Recall that the <span class="math inline">\(\hat{\boldsymbol{\beta}} = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{Y}\)</span> and <span class="math inline">\(\mathbf{Y} = \mathbf{X}\mathbf{\beta} + \mathbf{\epsilon}\)</span>. Then,</p>
<span class="math display">\[\begin{equation}
\begin{aligned}
\hat{\boldsymbol{\beta}} &amp;= (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{Y} \\[10pt]
&amp;= (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T(\mathbf{X}\boldsymbol{\beta} + \boldsymbol{\epsilon}) \\[10pt]
&amp;= \boldsymbol{\beta} + (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T \mathbf{\epsilon} \\
\end{aligned}
\end{equation}\]</span>
<p>Using these two properties, we derive the form of the variance-covariance matrix for the coefficients. Note that the covariance matrix is <span class="math inline">\(E[(\hat{\boldsymbol{\beta}} - \boldsymbol{\beta})(\hat{\boldsymbol{\beta}} - \boldsymbol{\beta})^T]\)</span></p>
<span class="math display">\[\begin{equation}
\begin{aligned}
E[(\hat{\boldsymbol{\beta}} - \boldsymbol{\beta})(\hat{\boldsymbol{\beta}} - \boldsymbol{\beta})^T] &amp;= E[(\boldsymbol{\beta} + (\mathbf{X}^T\mathbf{X})^{-1} \mathbf{X}^T \boldsymbol{\epsilon} - \boldsymbol{\beta})(\boldsymbol{\beta} + (\mathbf{X}^T\mathbf{X})^{-1} \mathbf{X}^T \boldsymbol{\epsilon} - \boldsymbol{\beta})^T]\\[10pt]
&amp; = E[(\mathbf{X}^T\mathbf{X})^{-1} \mathbf{X}^T \boldsymbol{\epsilon}\boldsymbol{\epsilon}^T\mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}] \\[10pt]
&amp; = (\mathbf{X}^T\mathbf{X})^{-1} \mathbf{X}^T E[\boldsymbol{\epsilon}\boldsymbol{\epsilon}^T]\mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\\[10pt]
&amp; = (\mathbf{X}^T\mathbf{X})^{-1} \mathbf{X}^T (\sigma^2\mathbf{I})\mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\\
&amp;= \sigma^2\mathbf{I}(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{X}(\mathbf{X}^T\mathbf{X})^{-1}\\[10pt]
&amp; = \sigma^2\mathbf{I}(\mathbf{X}^T\mathbf{X})^{-1}\\[10pt]
&amp;  = \sigma^2(\mathbf{X}^T\mathbf{X})^{-1} \\
\end{aligned}
\end{equation}\]</span>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="2-3-analyzing-wages.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="2-5-log-transformations-in-linear-regression.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": ["intro-regression.pdf", "intro-regression.epub"],
"toc": {
"collapse": "section",
"toc_depth": 1,
"scroll_highlight": true
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
